{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from tabnet import TabNet, TabNetClassifier, TabNetRegression\n",
    "from tabnet import StackedTabNetClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpath = os.path.join(\"..\",\"nocode\",\"fin\")\n",
    "trainfile = os.path.join(bpath,\"train.csv\")\n",
    "testfile = os.path.join(bpath,\"example_test.csv\")\n",
    "featfile = os.path.join(bpath,\"features.csv\")\n",
    "resfile = os.path.join(bpath,\"example_sample_submission.csv\")\n",
    "modelfile = os.path.join(bpath,'weights')\n",
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "SEED=33\n",
    "set_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内存优化函数\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" 根据数据的范围，修改数据类型 \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype.name\n",
    "\n",
    "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分数据函数\n",
    "class PurgedGroupTimeSeriesSplitStacking(_BaseKFold):\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 stacking_mode=True,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_val_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 val_group_gap=None,\n",
    "                 test_group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.max_val_group_size = max_val_group_size\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.val_group_gap = val_group_gap\n",
    "        self.test_group_gap = test_group_gap\n",
    "        self.verbose = verbose\n",
    "        self.stacking_mode = stacking_mode\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        if self.stacking_mode:\n",
    "            return self.split_ensemble(X, y, groups)\n",
    "        else:\n",
    "            return self.split_standard(X, y, groups)\n",
    "\n",
    "    def split_standard(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and validation set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/validation set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        val : ndarray\n",
    "            The validation set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.val_group_gap\n",
    "        max_val_group_size = self.max_val_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError((\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        group_val_size = min(n_groups // n_folds, max_val_group_size)\n",
    "        group_val_starts = range(n_groups - n_splits * group_val_size, n_groups, group_val_size)\n",
    "        for group_val_start in group_val_starts:\n",
    "            train_array = []\n",
    "            val_array = []\n",
    "\n",
    "            group_st = max(0, group_val_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_val_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(np.concatenate((train_array, train_array_tmp)), axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    "\n",
    "            for val_group_idx in unique_groups[group_val_start: group_val_start + group_val_size]:\n",
    "                val_array_tmp = group_dict[val_group_idx]\n",
    "                val_array = np.sort(np.unique(np.concatenate((val_array, val_array_tmp)), axis=None), axis=None)\n",
    "\n",
    "            val_array = val_array[group_gap:]\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                pass\n",
    "\n",
    "            yield [int(i) for i in train_array], [int(i) for i in val_array]\n",
    "\n",
    "    def split_ensemble(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training, validation and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        val : ndarray\n",
    "            The validation set indices for that split (testing indices for base classifiers).\n",
    "        test : ndarray\n",
    "            The testing set indices for that split (testing indices for final classifier)\n",
    "        \"\"\"\n",
    "\n",
    "        if groups is None:\n",
    "            raise ValueError(\"The 'groups' parameter should not be None\")\n",
    "\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_splits = self.n_splits\n",
    "        val_group_gap = self.val_group_gap\n",
    "        test_group_gap = self.test_group_gap\n",
    "        if test_group_gap is None:\n",
    "            test_group_gap = val_group_gap\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        max_val_group_size = self.max_val_group_size\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        if max_test_group_size is None:\n",
    "            max_test_group_size = max_val_group_size\n",
    "\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError((\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds, n_groups))\n",
    "\n",
    "        group_val_size = min(n_groups // n_folds, max_val_group_size)\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size, n_groups, group_test_size)\n",
    "        train_indices = []\n",
    "        val_indices = []\n",
    "        test_indices = []\n",
    "\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            val_array = []\n",
    "            test_array = []\n",
    "\n",
    "            val_group_st = max(max_train_group_size + val_group_gap, group_test_start - test_group_gap - max_val_group_size)\n",
    "            train_group_st = max(0, val_group_st - val_group_gap - max_train_group_size)\n",
    "\n",
    "            for train_group_idx in unique_groups[train_group_st:(val_group_st - val_group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(np.concatenate((train_array, train_array_tmp)), axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    "\n",
    "            for val_group_idx in unique_groups[val_group_st:(group_test_start - test_group_gap)]:\n",
    "                val_array_tmp = group_dict[val_group_idx]\n",
    "                val_array = np.sort(np.unique(np.concatenate((val_array, val_array_tmp)), axis=None), axis=None)\n",
    "\n",
    "            val_array = val_array[val_group_gap:]\n",
    "\n",
    "            for test_group_idx in unique_groups[group_test_start:(group_test_start + group_test_size)]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(np.concatenate((test_array, test_array_tmp)), axis=None), axis=None)\n",
    "\n",
    "            test_array = test_array[test_group_gap:]\n",
    "\n",
    "            yield [int(i) for i in train_array], [int(i) for i in val_array], [int(i) for i in test_array]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载文件\n",
    "trainpd = pd.read_csv(trainfile, header=0, encoding=\"utf8\")\n",
    "trainpd = trainpd.reset_index(drop = True)\n",
    "# 2. 空值处理 \n",
    "features = trainpd.columns \n",
    "# features = [c for c in trainpd.columns if 'feature' in c]\n",
    "# f_mean = trainpd.mean()\n",
    "# trainpd = trainpd.fillna(f_mean)\n",
    "trainpd[features] = trainpd[features].fillna(method='bfill').fillna(0)\n",
    "trainpd['action'] = (trainpd['resp'] > 0 and trainpd['weight'] != 0).astype('int8')\n",
    "# 3. 优化内存\n",
    "trainpd = reduce_mem_usage(trainpd)\n",
    "# print(list(trainpd.dtypes))\n",
    "\n",
    "# 4. 数据拆分训练\n",
    "X = trainpd.loc[:, features]\n",
    "y = trainpd.loc[:, 'action']\n",
    "g = trainpd.loc[:, 'date']\n",
    "# del trainpd\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 5. 数据拆分验证\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "N_SPLITS = 5\n",
    "STACKING_MODE = True\n",
    "VAL_GROUP_GAP = 20  # Days between end of training set and start of validation set\n",
    "TEST_GROUP_GAP = 20  # Days between end of validation set and start of testing/stacking set\n",
    "MAX_DAYS_TRAIN = 120\n",
    "# MAX_DAYS_VAL = 60\n",
    "MAX_DAYS_VAL = 0\n",
    "MAX_DAYS_TEST = 60\n",
    "RANDOM_SEED = 28\n",
    "\n",
    "cv = PurgedGroupTimeSeriesSplitStacking(n_splits=N_SPLITS,\n",
    "                                        stacking_mode=STACKING_MODE,\n",
    "                                        max_train_group_size=MAX_DAYS_TRAIN, max_val_group_size=MAX_DAYS_VAL,\n",
    "                                        max_test_group_size=MAX_DAYS_TEST, val_group_gap=VAL_GROUP_GAP,\n",
    "                                        test_group_gap=TEST_GROUP_GAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘图函数\n",
    "def plot_cv_indices_stacking(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "    jet = plt.cm.get_cmap('jet', 256)\n",
    "    seq = np.linspace(0, 1, 256)\n",
    "    _ = np.random.shuffle(seq)  # inplace\n",
    "    cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, indices_split in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[indices_split[0]] = 1\n",
    "        indices[indices_split[1]] = 0\n",
    "        if cv.stacking_mode:\n",
    "            indices[indices_split[2]] = -1\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    if cv.stacking_mode:\n",
    "        ax.scatter(range(len(X)), [ii + 3.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "    ax.set(yticks=np.arange(n_splits + 2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits + 2.2, -.2], xlim=[0, len(y)])\n",
    "\n",
    "    ax.set_title('{}'.format(name_dict[cv.stacking_mode]), fontsize=15)\n",
    "    # ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "plot_cv_indices_stacking(cv, train[just_features], train['action'], train['date'], ax, 5, lw=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testpd = pd.read_csv(testfile, header=0, encoding=\"utf8\")\n",
    "# testpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featpd = pd.read_csv(featfile, header=0, encoding=\"utf8\")\n",
    "# featpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# respd = pd.read_csv(resfile, header=0, encoding=\"utf8\")\n",
    "# respd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabNetClassifier(feature_list, num_classes, ...,dynamic=True)\n",
    "model = StackedTabNetClassifier(feature_list, num_classes, num_layers, ...,dynamic=True)\n",
    "print(TabNet.feature_selection_masks)\n",
    "print(model.tabnet.*)\n",
    "# 训练\n",
    "# # Mask Generation must be in Eager Execution Mode\n",
    "# x, _ = next(iter(tf_dataset))  # Assuming it generates an (x, y) tuple.\n",
    "# _ = model(x)  # This forces eager execution.\n",
    "for fold, (train_idx, val_idx, test_idx) in enumerate(cv.split(X, y, g)):\n",
    "    print(\"FOLD: {}\\n\".format(fold))\n",
    "    print(\"First train_day: {}\\t Last train_day: {} \\n\".format(trainpd.loc[min(train_idx), 'date'],\n",
    "                                                               trainpd.loc[max(train_idx), 'date']))\n",
    "    print(\"First val_day: {}\\t Last val_day: {} \\n\".format(trainpd.loc[min(val_idx), 'date'],\n",
    "                                                           trainpd.loc[max(val_idx), 'date']))\n",
    "    print(\"First test_day: {}\\t Last test_day: {} \\n\\n\\n\".format(trainpd.loc[min(test_idx), 'date'],\n",
    "                                                                 trainpd.loc[max(test_idx), 'date']))\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(X, y, g)):\n",
    "#     print(\"FOLD: {}\\n\".format(fold))\n",
    "#     print(\"First train_day: {}\\t Last train_day: {} \\n\".format(trainpd.loc[min(train_idx), 'date'],\n",
    "#                                                                trainpd.loc[max(train_idx), 'date']))\n",
    "#     print(\"First test_day: {}\\t Last test_day: {} \\n\\n\\n\".format(trainpd.loc[min(test_idx), 'date'],\n",
    "#                                                                  trainpd.loc[max(test_idx), 'date']))\n",
    "    model.fit(X[train_idx],y[train_idx],validation_split=0.2)\n",
    "# 加载模型\n",
    "model.load_weights(modelfile)\n",
    "\n",
    "# 模型预测\n",
    "model.predict(x)\n",
    "# clf = TabNetClassifier()\n",
    "# clf.fit(\n",
    "#     X_train, y_train,\n",
    "#     eval_set=[(X_test, y_test)], max_epochs=2  # Change this to increase the accuracy\n",
    "# )\n",
    "\n",
    "# 可视化\n",
    "writer = tf.summary.create_file_writer(\"logs/\")\n",
    "with writer.as_default():\n",
    "    for i, mask in enumerate(model.tabnet.feature_selection_masks):\n",
    "        print(\"Saving mask {} of shape {}\".format(i + 1, mask.shape))\n",
    "        tf.summary.image('mask_at_iter_{}'.format(i + 1), step=0, data=mask, max_outputs=1)\n",
    "        writer.flush()\n",
    "\n",
    "    agg_mask = model.tabnet.aggregate_feature_selection_mask\n",
    "    print(\"Saving aggregate mask of shape\", agg_mask.shape)\n",
    "    tf.summary.image(\"Aggregate Mask\", step=0, data=agg_mask, max_outputs=1)\n",
    "    writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成学习 voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [['Neural Network :', MLPClassifier(max_iter=1000)],\n",
    "               ['LogisticRegression :', LogisticRegression(max_iter=1000)],\n",
    "               ['ExtraTreesClassifier :', ExtraTreesClassifier()],\n",
    "               ['DecisionTree :', DecisionTreeClassifier()],\n",
    "               ['RandomForest :', RandomForestClassifier()],\n",
    "               ['Naive Bayes :', GaussianNB()],\n",
    "               ['KNeighbours :', KNeighborsClassifier()],\n",
    "               ['SVM :', SVC()],\n",
    "               ['AdaBoostClassifier :', AdaBoostClassifier()],\n",
    "               ['GradientBoostingClassifier: ', GradientBoostingClassifier()],\n",
    "               ['XGB :', XGBClassifier()],\n",
    "               ['CatBoost :', CatBoostClassifier(logging_level='Silent')]]\n",
    "\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['action'] = y_test\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    classifier = classifier\n",
    "    classifier.fit(X_train, y_train.ravel())\n",
    "    predictions = classifier.predict(X_test)\n",
    "    predictions_df[name.strip(\" :\")] = predictions\n",
    "    print(name, accuracy_score(y_test, predictions))\n",
    "\n",
    "clf1 = ExtraTreesClassifier()\n",
    "clf2 = CatBoostClassifier(logging_level='Silent')\n",
    "clf3 = RandomForestClassifier()\n",
    "eclf1 = VotingClassifier(estimators=[('ExTrees', clf1), ('CatBoost', clf2), ('RF', clf3)], voting='soft')\n",
    "eclf1.fit(X_train, y_train)\n",
    "predictions = eclf1.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n",
    "c = []\n",
    "c.append(cross_val_score(clf1, X_train, y_train, scoring='accuracy', cv=10).mean())\n",
    "c.append(cross_val_score(clf2, X_train, y_train, scoring='accuracy', cv=10).mean())\n",
    "c.append(cross_val_score(clf3, X_train, y_train, scoring='accuracy', cv=10).mean())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 预测评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线上预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janestreet\n",
    "env = janestreet.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "I_WANT_TO_SUBMIT = False\n",
    "I_WANT_TO_SUBMIT = True\n",
    "rcount = 0\n",
    "score_u = 0\n",
    "if I_WANT_TO_SUBMIT:\n",
    "    for (test_df, prediction_df) in env.iter_test():\n",
    "#         X_test = test_df.loc[:, just_features].fillna(-999)\n",
    "        print(test_df, prediction_df)\n",
    "#         y_preds = model_trained.predict(X_test.values)\n",
    "#         prediction_df.action = y_preds.item()\n",
    "        prediction_df.action = 0\n",
    "#         t=y_preds.item()*test_df['weight'][0]*test_df['resp'][0]\n",
    "        env.predict(prediction_df)\n",
    "        rcount += len(test_df.index)\n",
    "        print(rcount)\n",
    "    print(f'Finished processing {rcount} rows.')\n",
    "#     score_u = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
