{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from scipy.stats import entropy, kurtosis\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x      y  z        t   terror        q  flag  event_id  hit_id\n",
      "0 -142.5 -147.5  0  767.879  2.02966  1.05052     0         7       1\n",
      "(9473201, 9)\n",
      "   event_id  nhit  nhitreal  energymc  thetamc    phimc   xcmc    ycmc\n",
      "0         7   426        70   48348.9  63.1686  11.0982 -40.83  114.03\n",
      "(13315, 8)\n",
      "       x      y  z        t  terror        q  event_id  hit_id\n",
      "0 -142.5 -127.5  0  848.061  1.9984  1.15067         9       1\n",
      "(4086511, 8)\n"
     ]
    }
   ],
   "source": [
    "pathf = os.path.join(\"..\", \"data\", \"particles\")\n",
    "model_path = os.path.join(pathf, \"model\")\n",
    "log_path = os.path.join(pathf, \"model\")\n",
    "trainpd = pd.read_csv(os.path.join(pathf, \"train.csv\"))\n",
    "print(trainpd.head(1))\n",
    "trainshape = trainpd.shape\n",
    "print(trainshape)\n",
    "eventpd = pd.read_csv(os.path.join(pathf, \"event.csv\"))\n",
    "print(eventpd.head(1))\n",
    "print(eventpd.shape)\n",
    "testpd = pd.read_csv(os.path.join(pathf, \"test.csv\"))\n",
    "testshape = testpd.shape\n",
    "print(testpd.head(1))\n",
    "print(testpd.shape)\n",
    "\n",
    "data = pd.concat([trainpd, testpd], ignore_index=True)\n",
    "data = pd.merge(data, eventpd, on='event_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (k(q,mc)*(t0+l))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+l) = (t+l)^2\n",
    "data['fx'] = data['x'] - data['xcmc']\n",
    "data['fy'] = data['y'] - data['ycmc']\n",
    "data['phimc'] = data['phimc'] * np.pi / 180.\n",
    "data['fphi'] = np.arctan2(data['fy'], data['fx']) - data['phimc']\n",
    "data['fdis'] = np.sqrt(data['fx'] ** 2 + data['fy'] ** 2)\n",
    "data['thetamc'] = data['thetamc'] * np.pi / 180.\n",
    "data['fsinthmc'] = np.sin(data['thetamc'])\n",
    "data['fcosphi'] = np.cos(data['fphi'])\n",
    "\n",
    "data['ft2'] = data['t'] ** 2\n",
    "data['fdis2'] = data['fdis'] ** 2\n",
    "data['fsencond'] = data['fdis'] * data['fcosphi'] * data['fsinthmc']\n",
    "\n",
    "data['fttrue'] = data['t'] / data['terror']\n",
    "data['nhitratio'] = data['nhit'] / data['nhitreal']\n",
    "\n",
    "data['fenergymc2'] = data['energymc'] ** 2\n",
    "\n",
    "del data['fx']\n",
    "del data['fy']\n",
    "del data['x']\n",
    "del data['y']\n",
    "del data['z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_new = pd.DataFrame()\n",
    "info_new[\"event_id\"] = data.groupby([\"event_id\"])[\"event_id\"].mean()\n",
    "info_new[\"fdis_mean\"] = data.groupby([\"event_id\"])[\"fdis\"].mean()\n",
    "info_new[\"fdis_std\"] = data.groupby([\"event_id\"])[\"fdis\"].std()\n",
    "info_new[\"fdis_stdmean\"] = info_new[\"fdis_std\"] / info_new[\"fdis_mean\"]\n",
    "info_new[\"ft_mean\"] = data.groupby([\"event_id\"])[\"t\"].mean()\n",
    "info_new[\"ft_std\"] = data.groupby([\"event_id\"])[\"t\"].std()\n",
    "info_new[\"ft_stdmean\"] = info_new[\"ft_std\"] / info_new[\"ft_mean\"]\n",
    "info_new[\"ft_mean2\"] = info_new[\"ft_mean\"] ** 2\n",
    "info_new.reset_index(drop=True, inplace=True)\n",
    "data = pd.merge(data, info_new, on='event_id', how='left')\n",
    "\n",
    "data['fsencond2'] = data['fsencond'] * data['ft_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpd = data[:trainshape[0]].reset_index()\n",
    "testpd = data[trainshape[0]:].reset_index()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'event_id', 'flag', 'hit_id', 'q', 't', 'terror', 'nhit',\n",
      "       'nhitreal', 'energymc', 'thetamc', 'phimc', 'xcmc', 'ycmc', 'fx', 'fy',\n",
      "       'fdis', 'fsinth', 'fcosth', 'fphi', 'fsinphi', 'fcosphi', 'fttrue',\n",
      "       'nhitratio', 'vfsinphi', 'vfcosphi', 'vfdis', 'vfsinth', 'vfcosth',\n",
      "       'fdis_stdmean'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(trainpd.columns)\n",
    "feature = [x for x in trainpd.columns if x not in ['flag', 'index', 'hit_id', 'event_id']]\n",
    "labels = trainpd['flag']\n",
    "del trainpd['flag']\n",
    "del testpd['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-a4ccbd76abe9>:70: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From d:\\programdata\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-7-a4ccbd76abe9>:76: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\programdata\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From d:\\programdata\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:809: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "def batch_iter_list(data_list, batch_size, num_epochs, shuffle=True):\n",
    "    data_size = len(data_list[0])\n",
    "    num_batches_per_epoch = data_size // batch_size  # 每个epoch中包含的batch数量\n",
    "    for epoch in range(num_epochs):\n",
    "        # 每个epoch是否进行shuflle\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data_list = [data[shuffle_indices] for data in data_list]\n",
    "        else:\n",
    "            shuffled_data_list = data_list\n",
    "\n",
    "        for batch_num in range(num_batches_per_epoch + 1):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield [shuffled_data[start_index:end_index] for shuffled_data in shuffled_data_list]\n",
    "\n",
    "class AbstractModeltensor(object):\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config\n",
    "\n",
    "    # You need to override this method.\n",
    "    def buildModel(self):\n",
    "        raise NotImplementedError(\"You need to implement your own model.\")\n",
    "\n",
    "\n",
    "class NeurousNet(AbstractModeltensor):\n",
    "    def __init__(self, xlenth, config=None):\n",
    "        super(NeurousNet, self).__init__(config)\n",
    "        self.graph = tf.Graph()  # 为每个类(实例)单独创建一个graph\n",
    "        self.modeldic = {\n",
    "            \"cnn_dense_less\": self._cnn_dense_less_model,\n",
    "            \"nomul_model\": self._nomul_model,\n",
    "        }\n",
    "        self.ydim = 1\n",
    "        self.keep_prob_ph = config[\"dropout\"]\n",
    "        self.input_dim = xlenth\n",
    "        self.out_dim = 1\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('Inputs'):\n",
    "                self.input_p = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "                self.learn_rate_p = tf.placeholder(dtype=tf.float32, shape=[], name=\"lr\")\n",
    "                self.lr_decay = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "            with tf.name_scope('Outputs'):\n",
    "                self.target_y = tf.placeholder(dtype=tf.float32, shape=[None, self.out_dim])\n",
    "\n",
    "    def buildModel(self):\n",
    "        tf.reset_default_graph()\n",
    "        with self.graph.as_default():\n",
    "            # 不同选择加载\n",
    "            self.modeldic[self.config[\"modelname\"]]()\n",
    "            # 打印打包\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            # 损失目标\n",
    "            tvars = tf.trainable_variables()  # 返回需要训练的variable\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.train_list, tvars), 2)\n",
    "            grads_and_vars = tuple(zip(grads, tvars))\n",
    "            self.train_op = tf.train.AdamOptimizer(self.learn_rate_p).apply_gradients(grads_and_vars)\n",
    "            #             self.train_op = []\n",
    "            #             for i2 in self.train_list:\n",
    "            #                 self.train_op.append(tf.train.AdamOptimizer(self.learn_rate_p).minimize(i2))\n",
    "            # 同一保存加载\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            # [print(n.name) for n in tf.get_default_graph().as_graph_def().node]\n",
    "            # return self.saver\n",
    "\n",
    "    def _cnn_dense_less_model(self):\n",
    "        with self.graph.as_default():\n",
    "            # 部分1，预测值\n",
    "            dense1 = tf.layers.dense(inputs=self.input_p, units=self.input_dim, activation=tf.nn.softmax,\n",
    "                                     name=\"layer_dense1\")\n",
    "            tf.summary.histogram('dense1', dense1)  # 记录标量的变化\n",
    "            mult_layer1 = tf.nn.softmax(dense1 * self.input_p, name='mult_layer1')\n",
    "            mult_layer2 = tf.nn.softmax(mult_layer1 * self.input_p, name='mult_layer2')\n",
    "            concat1 = tf.concat([self.input_p, dense1, mult_layer1, mult_layer2], 1, name='concat1')\n",
    "            tf.summary.histogram('concat1', concat1)  # 记录标量的变化\n",
    "            denseo1 = tf.nn.dropout(concat1, keep_prob=self.keep_prob_ph)\n",
    "            denseo2 = tf.layers.dense(inputs=denseo1, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense2\")\n",
    "            denseo3 = tf.layers.dense(inputs=denseo2, units=self.input_dim // 4, activation=tf.nn.elu,\n",
    "                                      name=\"layer_dense3\")\n",
    "            y_res_t = tf.layers.dense(inputs=denseo3, units=self.out_dim, activation=None)\n",
    "            y_res_v = tf.nn.sigmoid(y_res_t, name=\"y_res_v\")\n",
    "            tf.summary.histogram('y_res_v', y_res_v)  # 记录标量的变化\n",
    "            # 损失返回值\n",
    "            y_los = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_res_t, labels=self.target_y, name=\"y_los\")\n",
    "            y_loss_t = tf.reduce_mean(y_los, name=\"y_loss_t\")\n",
    "            y_loss_v = tf.add(y_loss_t, 0, name=\"y_loss_v\")\n",
    "\n",
    "            one = tf.ones_like(y_res_t)\n",
    "            zero = tf.zeros_like(y_res_t)\n",
    "            label_bool = tf.where(y_res_t < 0.5, x=zero, y=one)\n",
    "            self.auc_value, self.auc_op = tf.metrics.auc(self.target_y, label_bool, num_thresholds=4000)\n",
    "            # 猜错的获取 实际盈利值的负数\n",
    "            self.train_list = [y_loss_t]\n",
    "            self.valid_list = [y_loss_v]\n",
    "            self.pred_list = [y_res_v]\n",
    "            # 打印信息\n",
    "            tf.summary.scalar('y_loss_t', y_loss_t)  # 记录标量的变化\n",
    "            tf.summary.scalar('y_loss_v', y_loss_v)  # 记录标量的变化\n",
    "            tf.summary.histogram('mult_layer1', mult_layer1)  # 记录标量的变化\n",
    "            tf.summary.histogram('mult_layer2', mult_layer2)  # 记录标量的变化\n",
    "\n",
    "            tf.summary.scalar('lr', self.learn_rate_p)  # 记录标量的变化\n",
    "            return None\n",
    "\n",
    "    def _nomul_model(self):\n",
    "        with self.graph.as_default():\n",
    "            # 部分1，预测值\n",
    "            dense1 = tf.layers.dense(inputs=self.input_p, units=self.input_dim, activation=tf.nn.softmax,\n",
    "                                     name=\"layer_dense1\")\n",
    "            tf.summary.histogram('dense1', dense1)  # 记录标量的变化\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense2\")\n",
    "            dense3 = tf.layers.dense(inputs=dense2, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense3\")\n",
    "            dense4 = tf.layers.dense(inputs=dense3, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense4\")\n",
    "            dense5 = tf.layers.dense(inputs=dense4, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense5\")\n",
    "            dense6 = tf.layers.dense(inputs=dense5, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense6\")\n",
    "            dense7 = tf.layers.dense(inputs=dense6, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense7\")\n",
    "            dense8 = tf.layers.dense(inputs=dense7, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense8\")\n",
    "            concat1 = tf.concat([self.input_p, dense1, dense2, dense3, dense4, dense5, dense6, dense7, dense8], 1,\n",
    "                                name='concat1')\n",
    "            tf.summary.histogram('concat1', concat1)  # 记录标量的变化\n",
    "            denseo1 = tf.nn.dropout(concat1, keep_prob=self.keep_prob_ph)\n",
    "            denseo2 = tf.layers.dense(inputs=denseo1, units=self.input_dim * 4, activation=tf.nn.elu,\n",
    "                                      name=\"layer_denseo2\")\n",
    "            denseo3 = tf.layers.dense(inputs=denseo2, units=self.input_dim, activation=tf.nn.elu,\n",
    "                                      name=\"layer_denseo3\")\n",
    "            denseo4 = tf.layers.dense(inputs=denseo3, units=self.input_dim // 4, activation=tf.nn.elu,\n",
    "                                      name=\"layer_denseo4\")\n",
    "            y_res_t = tf.layers.dense(inputs=denseo4, units=self.out_dim, activation=None)\n",
    "            y_res_v = tf.nn.sigmoid(y_res_t, name=\"y_res_v\")\n",
    "            tf.summary.histogram('y_res_v', y_res_v)  # 记录标量的变化\n",
    "            # 损失返回值\n",
    "            y_los = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_res_t, labels=self.target_y, name=\"y_los\")\n",
    "            y_loss_t = tf.reduce_mean(y_los, name=\"y_loss_t\")\n",
    "            y_loss_v = tf.add(y_loss_t, 0, name=\"y_loss_v\")\n",
    "\n",
    "            one = tf.ones_like(y_res_t)\n",
    "            zero = tf.zeros_like(y_res_t)\n",
    "            label_bool = tf.where(y_res_t < 0.5, x=zero, y=one)\n",
    "            self.auc_value, self.auc_op = tf.metrics.auc(self.target_y, label_bool, num_thresholds=4000)\n",
    "            # 猜错的获取 实际盈利值的负数\n",
    "            self.train_list = [y_loss_t]\n",
    "            self.valid_list = [y_loss_v]\n",
    "            self.pred_list = [y_res_v]\n",
    "            # 打印信息\n",
    "            tf.summary.scalar('y_loss_t', y_loss_t)  # 记录标量的变化\n",
    "            tf.summary.scalar('y_loss_v', y_loss_v)  # 记录标量的变化\n",
    "\n",
    "            tf.summary.scalar('lr', self.learn_rate_p)  # 记录标量的变化\n",
    "            return None\n",
    "\n",
    "    def batch_train(self, trainpd, labels, batch_size=8, num_epochs=1, retrain=True):\n",
    "        # 设置\n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        with sess.as_default():\n",
    "            with self.graph.as_default():\n",
    "                if self.config[\"retrain\"] == 1:\n",
    "                    model_dir = os.path.join(model_path, \"modelevery_%s\" % self.config[\"tailname\"])\n",
    "                    latest_ckpt = tf.train.latest_checkpoint(model_dir)\n",
    "                    if os.path.isfile(\"{}.index\".format(latest_ckpt)):\n",
    "                        self.saver.restore(sess, latest_ckpt)\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        print(\"retraining {}\".format(latest_ckpt))\n",
    "                    else:\n",
    "                        sess.run(tf.global_variables_initializer())\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        print(\"no old model, training new----\")\n",
    "                writer = tf.summary.FileWriter(os.path.join(log_path, \"logsevery_%s\" % self.config[\"tailname\"]),\n",
    "                                               sess.graph)\n",
    "                global_n = 0\n",
    "                stop_n = 0\n",
    "                startt = time.time()\n",
    "                pre_t_base_loss = pre_t_much_loss = pre_v_much_loss = pre_v_base_loss = 100000\n",
    "\n",
    "                n_splits = 5\n",
    "                kf = KFold(n_splits=n_splits, shuffle=True, random_state=4389)\n",
    "                for epoch in range(num_epochs):\n",
    "                    if self.config[\"learn_rate\"]>0.00001:\n",
    "                        self.config[\"learn_rate\"] *= 0.8\n",
    "                    for train_index, valid_index in kf.split(trainpd):\n",
    "                        inputs_t = np.array(trainpd[feature].iloc[train_index])\n",
    "                        output_t = np.expand_dims(np.array(labels[train_index]),-1)\n",
    "                        inputs_v = np.array(trainpd[feature].iloc[valid_index])\n",
    "                        output_v = np.expand_dims(np.array(labels[valid_index]),-1)\n",
    "                        dataiter = batch_iter_list([inputs_t,output_t], batch_size, num_epochs)\n",
    "                        starte = time.time()\n",
    "                        print(\"iter_trainnum\", inputs_t.shape[0] // batch_size + 1)\n",
    "                        redi = inputs_t.shape[0] % batch_size\n",
    "                        lenth = inputs_t.shape[0] // batch_size\n",
    "                        if 0 != redi:\n",
    "                            lenth += 1\n",
    "                        counter = 0\n",
    "                        for batch_num in range(lenth):\n",
    "                            # 获取数据\n",
    "                            r_inputs_t,r_output_t = next(dataiter)\n",
    "                            feed_dict_t = {\n",
    "                                self.input_p: r_inputs_t,\n",
    "                                self.target_y: r_output_t,\n",
    "                                self.learn_rate_p: self.config[\"learn_rate\"],\n",
    "                                self.lr_decay: 1,\n",
    "                            }\n",
    "                            # 更新学习率\n",
    "                            sess.run(self.train_op, feed_dict_t)\n",
    "                            global_n += 1\n",
    "                            losslist_t = sess.run(self.train_list, feed_dict_t)\n",
    "                            sess.run(self.auc_op, feed_dict=feed_dict_t)\n",
    "                            accu = sess.run(self.auc_value)\n",
    "                            result = sess.run(self.merged, feed_dict_t)\n",
    "                            if batch_num % 200 == 0:\n",
    "                                writer.add_summary(result, global_n)\n",
    "                                self.saver.save(sess,\n",
    "                                                os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                             self.config[\"modelfile\"]), global_step=global_n)\n",
    "                                print(\"epocht {}, batch_num {}, step {}, time: {} s, accu: {}, loss_yt: {}\".format(\n",
    "                                    epoch, batch_num, global_n, time.time() - starte, accu, *losslist_t))\n",
    "                        # valid part\n",
    "                        dataiterv = batch_iter_list([inputs_v,output_v], batch_size, num_epochs)\n",
    "                        redi = inputs_v.shape[0] % batch_size\n",
    "                        vnum_iter = inputs_v.shape[0] // batch_size\n",
    "                        if 0 != redi:\n",
    "                            vnum_iter += 1\n",
    "                        counter = 0\n",
    "                        print(\"iter_validnum\", vnum_iter)\n",
    "                        losslist_va = 0\n",
    "                        accu_va = 0\n",
    "                        dataiter = batch_iter_list([inputs_v,output_v], batch_size, num_epochs)\n",
    "                        for batch_num in range(vnum_iter):\n",
    "                            # 获取数据\n",
    "                            r_inputs_v,r_output_v = next(dataiter)\n",
    "                            feed_dict_v = {\n",
    "                                self.input_p: r_inputs_v,\n",
    "                                self.target_y: r_output_v,\n",
    "                                self.learn_rate_p: self.config[\"learn_rate\"],\n",
    "                                self.lr_decay: 1,\n",
    "                            }\n",
    "                            losslist_v = sess.run(self.valid_list, feed_dict_v)\n",
    "                            sess.run(self.auc_op, feed_dict=feed_dict_v)\n",
    "                            accu = sess.run(self.auc_value)\n",
    "                            losslist_va += losslist_v[0]\n",
    "                            accu_va += accu\n",
    "                        losslist_va /= vnum_iter\n",
    "                        accu_va /= vnum_iter\n",
    "                        result = sess.run(self.merged, feed_dict_v)\n",
    "                        writer.add_summary(result, global_n)\n",
    "                        if losslist_t[0] < pre_t_base_loss and losslist_va < pre_v_base_loss:\n",
    "                            stop_n += 1\n",
    "                            if stop_n > self.config[\"early_stop\"]:\n",
    "                                break\n",
    "                            else:\n",
    "                                self.saver.save(sess,\n",
    "                                                os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                             self.config[\"modelfile\"]), global_step=global_n)\n",
    "                        else:\n",
    "                            stop_n = 0\n",
    "                            self.saver.save(sess, os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                               self.config[\"modelfile\"]), global_step=global_n)\n",
    "                        print(\"epochv {}, step {}, stop_n {}, time: {} s, accu_va: {}, loss_yv: {}\".format(\n",
    "                            epoch, global_n, stop_n, time.time() - starte, accu_va, losslist_va))\n",
    "                        pre_t_base_loss = losslist_t[0]\n",
    "                        pre_v_base_loss = losslist_va\n",
    "                writer.close()\n",
    "                print(\"total time: %s s\" % (time.time() - startt))\n",
    "        # 结束\n",
    "        print(\"train finished!\")\n",
    "        return None\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        model_dir = os.path.join(model_path, \"modelevery_%s\" % self.config[\"tailname\"])\n",
    "        print(\"loading model...\")\n",
    "        latest_ckpt = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        with sess.as_default():\n",
    "            with self.graph.as_default():\n",
    "                if os.path.isfile(\"{}.index\".format(latest_ckpt)):\n",
    "                    self.saver.restore(sess, latest_ckpt)\n",
    "                else:\n",
    "                    raise Exception(\"没有找到模型:{}\".format(latest_ckpt))\n",
    "                nplist = []\n",
    "                oneiter = 2000\n",
    "                redi = inputs.shape[0] % oneiter\n",
    "                lenth = inputs.shape[0] // oneiter\n",
    "                if 0 != redi:\n",
    "                    lenth += 1\n",
    "                counter = 0\n",
    "                for num in range(lenth):\n",
    "                    # 获取数据\n",
    "                    startindex = num * oneiter\n",
    "                    if num == lenth - 1 and redi != 0:\n",
    "                        endindex = num * oneiter + redi\n",
    "                    else:\n",
    "                        endindex = (num + 1) * oneiter\n",
    "                    tmppd = inputs.iloc[startindex:endindex][feature]\n",
    "                    r_inputs_v = np.array(tmppd)\n",
    "                    feed_dict = {\n",
    "                        self.input_p: r_inputs_v,\n",
    "                    }\n",
    "                    teslis = sess.run(self.pred_list, feed_dict)\n",
    "                    nplist.append(teslis)\n",
    "                feed_dict = {\n",
    "                    self.input_p: inputs,\n",
    "                }\n",
    "                teslist = np.concatenate(nplist, axis=1)\n",
    "                return teslist\n",
    "\n",
    "\n",
    "trainconfig = {\n",
    "    \"dropout\": 0.5,\n",
    "    \"early_stop\": 100,\n",
    "#     \"tailname\": \"nomul_modeltail\",\n",
    "#     \"modelname\": \"nomul_model\",\n",
    "    \"tailname\": \"mul_verse\",\n",
    "    \"modelname\": \"cnn_dense_less\",\n",
    "    \"modelfile\": \"v2\",\n",
    "    \"learn_rate\": 0.01,\n",
    "    \"retrain\": 1\n",
    "}\n",
    "modelcrnn = NeurousNet(len(feature), config=trainconfig)\n",
    "modelcrnn.buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  event_id  hit_id          q         t   terror  nhit  nhitreal  \\\n",
      "0      0         7       1   1.050520  767.8790  2.02966   426        70   \n",
      "1      1         7       2   0.999853  -70.5552  2.02966   426        70   \n",
      "2      2         7       3   2.052540 -837.8410  1.85146   426        70   \n",
      "3      3         7       4  19.513100 -973.1950  1.39994   426        70   \n",
      "4      4         7       5   0.800334 -159.1400  2.02966   426        70   \n",
      "\n",
      "   energymc  thetamc    phimc   xcmc    ycmc      fx      fy        fdis  \\\n",
      "0   48348.9  63.1686  11.0982 -40.83  114.03 -101.67 -261.53  280.597095   \n",
      "1   48348.9  63.1686  11.0982 -40.83  114.03  -96.67 -266.53  283.519540   \n",
      "2   48348.9  63.1686  11.0982 -40.83  114.03  -96.67 -246.53  264.805834   \n",
      "3   48348.9  63.1686  11.0982 -40.83  114.03 -101.67 -231.53  252.869393   \n",
      "4   48348.9  63.1686  11.0982 -40.83  114.03  -96.67 -231.53  250.900837   \n",
      "\n",
      "     fsinth    fcosth      fphi   fsinphi   fcosphi      fttrue  nhitratio  \\\n",
      "0  0.892339  0.451367 -1.941568 -0.033880  0.999426  378.328883   6.085714   \n",
      "1  0.892339  0.451367 -1.918739 -0.033482  0.999439  -34.762078   6.085714   \n",
      "2  0.892339  0.451367 -1.944494 -0.033931  0.999424 -452.529895   6.085714   \n",
      "3  0.892339  0.451367 -1.984568 -0.034630  0.999400 -695.169079   6.085714   \n",
      "4  0.892339  0.451367 -1.966320 -0.034312  0.999411  -78.407221   6.085714   \n",
      "\n",
      "    vfsinphi  vfcosphi     vfdis   vfsinth   vfcosth  fdis_stdmean  \n",
      "0 -29.515709  1.000574  0.003564  1.120651  2.215494      0.542283  \n",
      "1 -29.866751  1.000561  0.003527  1.120651  2.215494      0.542283  \n",
      "2 -29.471315  1.000576  0.003776  1.120651  2.215494      0.542283  \n",
      "3 -28.876434  1.000600  0.003955  1.120651  2.215494      0.542283  \n",
      "4 -29.144301  1.000589  0.003986  1.120651  2.215494      0.542283  \n",
      "WARNING:tensorflow:From d:\\programdata\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ..\\data\\particles\\model\\modelevery_mul_verse\\v2-1\n",
      "retraining ..\\data\\particles\\model\\modelevery_mul_verse\\v2-1\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 1, time: 11.614969968795776 s, accu: 0.48464435338974, loss_yt: 18.00465202331543\n",
      "epocht 0, batch_num 200, step 201, time: 14.85432481765747 s, accu: 0.5440226793289185, loss_yt: 0.6000566482543945\n",
      "epocht 0, batch_num 400, step 401, time: 18.09663486480713 s, accu: 0.576420783996582, loss_yt: 0.5894632935523987\n",
      "epocht 0, batch_num 600, step 601, time: 21.030765295028687 s, accu: 0.616756021976471, loss_yt: 0.44155851006507874\n",
      "epocht 0, batch_num 800, step 801, time: 24.379807472229004 s, accu: 0.6598542928695679, loss_yt: 0.5671924352645874\n",
      "WARNING:tensorflow:From d:\\programdata\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "epocht 0, batch_num 1000, step 1001, time: 27.828613996505737 s, accu: 0.6901593208312988, loss_yt: 0.38451117277145386\n",
      "epocht 0, batch_num 1200, step 1201, time: 31.008111476898193 s, accu: 0.7110562324523926, loss_yt: 0.3782978951931\n",
      "epocht 0, batch_num 1400, step 1401, time: 34.63641023635864 s, accu: 0.7275502681732178, loss_yt: 0.32056960463523865\n",
      "epocht 0, batch_num 1600, step 1601, time: 37.77498912811279 s, accu: 0.7403956651687622, loss_yt: 0.36760109663009644\n",
      "epocht 0, batch_num 1800, step 1801, time: 41.30754351615906 s, accu: 0.7500978708267212, loss_yt: 0.3391985595226288\n",
      "epocht 0, batch_num 2000, step 2001, time: 44.34841299057007 s, accu: 0.7585455179214478, loss_yt: 0.41276949644088745\n",
      "epocht 0, batch_num 2200, step 2201, time: 47.49100852012634 s, accu: 0.7654889225959778, loss_yt: 0.36009642481803894\n",
      "epocht 0, batch_num 2400, step 2401, time: 50.86598587036133 s, accu: 0.7714055776596069, loss_yt: 0.3475404381752014\n",
      "epocht 0, batch_num 2600, step 2601, time: 54.21106672286987 s, accu: 0.7763885855674744, loss_yt: 0.3501690924167633\n",
      "epocht 0, batch_num 2800, step 2801, time: 57.656824588775635 s, accu: 0.7808212637901306, loss_yt: 0.32852643728256226\n",
      "epocht 0, batch_num 3000, step 3001, time: 60.988914012908936 s, accu: 0.7836276292800903, loss_yt: 0.3711668848991394\n",
      "epocht 0, batch_num 3200, step 3201, time: 64.42373132705688 s, accu: 0.786940336227417, loss_yt: 0.37940099835395813\n",
      "epocht 0, batch_num 3400, step 3401, time: 67.64910435676575 s, accu: 0.7900996208190918, loss_yt: 0.3283337354660034\n",
      "epocht 0, batch_num 3600, step 3601, time: 70.82565498352051 s, accu: 0.7930947542190552, loss_yt: 0.32752877473831177\n",
      "epocht 0, batch_num 3800, step 3801, time: 73.97123217582703 s, accu: 0.7956418991088867, loss_yt: 0.3071043789386749\n",
      "epocht 0, batch_num 4000, step 4001, time: 77.02406811714172 s, accu: 0.797921359539032, loss_yt: 0.356776624917984\n",
      "epocht 0, batch_num 4200, step 4201, time: 80.16167330741882 s, accu: 0.7999626398086548, loss_yt: 0.33496397733688354\n",
      "epocht 0, batch_num 4400, step 4401, time: 83.24839329719543 s, accu: 0.8018546104431152, loss_yt: 0.35554033517837524\n",
      "epocht 0, batch_num 4600, step 4601, time: 86.38500428199768 s, accu: 0.803589940071106, loss_yt: 0.366046279668808\n",
      "epocht 0, batch_num 4800, step 4801, time: 89.51067161560059 s, accu: 0.8052331805229187, loss_yt: 0.3421843647956848\n",
      "epocht 0, batch_num 5000, step 5001, time: 92.79290223121643 s, accu: 0.8067847490310669, loss_yt: 0.40886160731315613\n",
      "epocht 0, batch_num 5200, step 5201, time: 95.94048738479614 s, accu: 0.8081199526786804, loss_yt: 0.3089989721775055\n",
      "epocht 0, batch_num 5400, step 5401, time: 99.33338212966919 s, accu: 0.8094081878662109, loss_yt: 0.3055916130542755\n",
      "epocht 0, batch_num 5600, step 5601, time: 102.47198700904846 s, accu: 0.8106158375740051, loss_yt: 0.3283899426460266\n",
      "epocht 0, batch_num 5800, step 5801, time: 105.70337677001953 s, accu: 0.8117551803588867, loss_yt: 0.28809356689453125\n",
      "epocht 0, batch_num 6000, step 6001, time: 109.00255727767944 s, accu: 0.8128194212913513, loss_yt: 0.34962499141693115\n",
      "epocht 0, batch_num 6200, step 6201, time: 112.03042793273926 s, accu: 0.8138815760612488, loss_yt: 0.32958829402923584\n",
      "epocht 0, batch_num 6400, step 6401, time: 115.27575063705444 s, accu: 0.81493079662323, loss_yt: 0.36778077483177185\n",
      "epocht 0, batch_num 6600, step 6601, time: 118.5569760799408 s, accu: 0.815930962562561, loss_yt: 0.3623028099536896\n",
      "epocht 0, batch_num 6800, step 6801, time: 121.68265652656555 s, accu: 0.8163653016090393, loss_yt: 0.3782331943511963\n",
      "epocht 0, batch_num 7000, step 7001, time: 125.10748958587646 s, accu: 0.8170309662818909, loss_yt: 0.3630950152873993\n",
      "epocht 0, batch_num 7200, step 7201, time: 128.37372517585754 s, accu: 0.8176618814468384, loss_yt: 0.32284456491470337\n",
      "epocht 0, batch_num 7400, step 7401, time: 131.56818413734436 s, accu: 0.8183688521385193, loss_yt: 0.2830565273761749\n",
      "epocht 0, batch_num 7600, step 7601, time: 134.9541301727295 s, accu: 0.8190453052520752, loss_yt: 0.27765271067619324\n",
      "epocht 0, batch_num 7800, step 7801, time: 138.04087686538696 s, accu: 0.8197197914123535, loss_yt: 0.3202891945838928\n",
      "epocht 0, batch_num 8000, step 8001, time: 141.23634338378906 s, accu: 0.8204607367515564, loss_yt: 0.28190669417381287\n",
      "epocht 0, batch_num 8200, step 8201, time: 144.65917825698853 s, accu: 0.8211182951927185, loss_yt: 0.30339664220809937\n",
      "epocht 0, batch_num 8400, step 8401, time: 147.80576419830322 s, accu: 0.8217397332191467, loss_yt: 0.3643938899040222\n",
      "epocht 0, batch_num 8600, step 8601, time: 150.9912474155426 s, accu: 0.8223907351493835, loss_yt: 0.3715049624443054\n",
      "epocht 0, batch_num 8800, step 8801, time: 154.21465849876404 s, accu: 0.8229884505271912, loss_yt: 0.34449243545532227\n",
      "epocht 0, batch_num 9000, step 9001, time: 157.36026072502136 s, accu: 0.8235909938812256, loss_yt: 0.3515598773956299\n",
      "epocht 0, batch_num 9200, step 9201, time: 160.56763911247253 s, accu: 0.8241722583770752, loss_yt: 0.33937162160873413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 9400, step 9401, time: 163.67134022712708 s, accu: 0.8248606324195862, loss_yt: 0.34573012590408325\n",
      "epocht 0, batch_num 9600, step 9601, time: 166.72716808319092 s, accu: 0.8255152702331543, loss_yt: 0.2683960497379303\n",
      "epocht 0, batch_num 9800, step 9801, time: 169.83289766311646 s, accu: 0.8261365294456482, loss_yt: 0.3087097704410553\n",
      "epocht 0, batch_num 10000, step 10001, time: 173.16594982147217 s, accu: 0.8265796899795532, loss_yt: 0.32787764072418213\n",
      "epocht 0, batch_num 10200, step 10201, time: 176.2556881904602 s, accu: 0.8270098567008972, loss_yt: 0.38646483421325684\n",
      "epocht 0, batch_num 10400, step 10401, time: 179.41327095031738 s, accu: 0.8274662494659424, loss_yt: 0.3124796450138092\n",
      "epocht 0, batch_num 10600, step 10601, time: 182.48706555366516 s, accu: 0.8279534578323364, loss_yt: 0.3220323920249939\n",
      "epocht 0, batch_num 10800, step 10801, time: 185.7363383769989 s, accu: 0.8284055590629578, loss_yt: 0.2865031361579895\n",
      "epocht 0, batch_num 11000, step 11001, time: 188.90885305404663 s, accu: 0.8288264274597168, loss_yt: 0.30183613300323486\n",
      "epocht 0, batch_num 11200, step 11201, time: 191.88692235946655 s, accu: 0.8292722105979919, loss_yt: 0.3285466432571411\n",
      "epocht 0, batch_num 11400, step 11401, time: 194.98959255218506 s, accu: 0.8296387195587158, loss_yt: 0.3257632553577423\n",
      "epocht 0, batch_num 11600, step 11601, time: 198.12720394134521 s, accu: 0.8300498127937317, loss_yt: 0.32779377698898315\n",
      "epocht 0, batch_num 11800, step 11801, time: 201.13220024108887 s, accu: 0.8304568529129028, loss_yt: 0.2953535318374634\n",
      "epocht 0, batch_num 12000, step 12001, time: 204.2488603591919 s, accu: 0.8308721780776978, loss_yt: 0.3066701889038086\n",
      "epocht 0, batch_num 12200, step 12201, time: 207.363538980484 s, accu: 0.8312444686889648, loss_yt: 0.2796946167945862\n",
      "epocht 0, batch_num 12400, step 12401, time: 210.52604866027832 s, accu: 0.8316795229911804, loss_yt: 0.35610026121139526\n",
      "epocht 0, batch_num 12600, step 12601, time: 213.60780787467957 s, accu: 0.8321259617805481, loss_yt: 0.3159419000148773\n",
      "epocht 0, batch_num 12800, step 12801, time: 217.01968383789062 s, accu: 0.8324277400970459, loss_yt: 0.3544582426548004\n",
      "epocht 0, batch_num 13000, step 13001, time: 220.07351779937744 s, accu: 0.8327817916870117, loss_yt: 0.3621808588504791\n",
      "epocht 0, batch_num 13200, step 13201, time: 223.16226077079773 s, accu: 0.8332094550132751, loss_yt: 0.25649401545524597\n",
      "epocht 0, batch_num 13400, step 13401, time: 226.409592628479 s, accu: 0.8336194753646851, loss_yt: 0.26011431217193604\n",
      "epocht 0, batch_num 13600, step 13601, time: 229.41055178642273 s, accu: 0.8340076208114624, loss_yt: 0.2455831915140152\n",
      "epocht 0, batch_num 13800, step 13801, time: 232.9371201992035 s, accu: 0.8343146443367004, loss_yt: 0.29081764817237854\n",
      "epocht 0, batch_num 14000, step 14001, time: 236.062762260437 s, accu: 0.8347162008285522, loss_yt: 0.27467185258865356\n",
      "epocht 0, batch_num 14200, step 14201, time: 239.22231459617615 s, accu: 0.8350698351860046, loss_yt: 0.3423047959804535\n",
      "epocht 0, batch_num 14400, step 14401, time: 242.47461938858032 s, accu: 0.8353896737098694, loss_yt: 0.28011342883110046\n",
      "epocht 0, batch_num 14600, step 14601, time: 245.50851106643677 s, accu: 0.8357815146446228, loss_yt: 0.2981521785259247\n",
      "epocht 0, batch_num 14800, step 14801, time: 248.8047297000885 s, accu: 0.8362162709236145, loss_yt: 0.2611429989337921\n",
      "iter_validnum 3701\n",
      "epochv 0, step 14802, stop_n 1, time: 275.5252785682678 s, accu_va: 0.8392258124620777, loss_yv: 0.297610587198101\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 14803, time: 13.12390685081482 s, accu: 0.8418402075767517, loss_yt: 0.3135164678096771\n",
      "epocht 0, batch_num 200, step 15003, time: 16.30539870262146 s, accu: 0.8420876264572144, loss_yt: 0.3070465624332428\n",
      "epocht 0, batch_num 400, step 15203, time: 19.38519597053528 s, accu: 0.8423361778259277, loss_yt: 0.3293382525444031\n",
      "epocht 0, batch_num 600, step 15403, time: 22.523807764053345 s, accu: 0.8425220847129822, loss_yt: 0.2888593375682831\n",
      "epocht 0, batch_num 800, step 15603, time: 25.914702892303467 s, accu: 0.8427408337593079, loss_yt: 0.3407532274723053\n",
      "epocht 0, batch_num 1000, step 15803, time: 28.984495878219604 s, accu: 0.8429743051528931, loss_yt: 0.3143109083175659\n",
      "epocht 0, batch_num 1200, step 16003, time: 32.13410520553589 s, accu: 0.8431738615036011, loss_yt: 0.30770406126976013\n",
      "epocht 0, batch_num 1400, step 16203, time: 35.521016120910645 s, accu: 0.843338131904602, loss_yt: 0.35439902544021606\n",
      "epocht 0, batch_num 1600, step 16403, time: 38.70552682876587 s, accu: 0.8435207605361938, loss_yt: 0.27297595143318176\n",
      "epocht 0, batch_num 1800, step 16603, time: 42.06355118751526 s, accu: 0.8437937498092651, loss_yt: 0.29389873147010803\n",
      "epocht 0, batch_num 2000, step 16803, time: 45.162235260009766 s, accu: 0.8440008759498596, loss_yt: 0.3133554458618164\n",
      "epocht 0, batch_num 2200, step 17003, time: 48.34671950340271 s, accu: 0.8442287445068359, loss_yt: 0.28313153982162476\n",
      "epocht 0, batch_num 2400, step 17203, time: 51.4145233631134 s, accu: 0.8444762229919434, loss_yt: 0.327877402305603\n",
      "epocht 0, batch_num 2600, step 17403, time: 54.443442583084106 s, accu: 0.8446242213249207, loss_yt: 0.3215554654598236\n",
      "epocht 0, batch_num 2800, step 17603, time: 57.86726212501526 s, accu: 0.8448100090026855, loss_yt: 0.40634864568710327\n",
      "epocht 0, batch_num 3000, step 17803, time: 61.08864760398865 s, accu: 0.8450179696083069, loss_yt: 0.31465449929237366\n",
      "epocht 0, batch_num 3200, step 18003, time: 64.53044366836548 s, accu: 0.845206081867218, loss_yt: 0.2777516841888428\n",
      "epocht 0, batch_num 3400, step 18203, time: 67.95528626441956 s, accu: 0.8453617095947266, loss_yt: 0.2860482633113861\n",
      "epocht 0, batch_num 3600, step 18403, time: 71.0739734172821 s, accu: 0.8455097079277039, loss_yt: 0.29170000553131104\n",
      "epocht 0, batch_num 3800, step 18603, time: 74.21954989433289 s, accu: 0.8457002639770508, loss_yt: 0.32393965125083923\n",
      "epocht 0, batch_num 4000, step 18803, time: 77.20555138587952 s, accu: 0.8458650708198547, loss_yt: 0.27920156717300415\n",
      "epocht 0, batch_num 4200, step 19003, time: 80.45489454269409 s, accu: 0.8460592031478882, loss_yt: 0.29893162846565247\n",
      "epocht 0, batch_num 4400, step 19203, time: 83.62737846374512 s, accu: 0.8462415933609009, loss_yt: 0.33447185158729553\n",
      "epocht 0, batch_num 4600, step 19403, time: 86.79191589355469 s, accu: 0.8464041948318481, loss_yt: 0.356006383895874\n",
      "epocht 0, batch_num 4800, step 19603, time: 89.89860916137695 s, accu: 0.8463059663772583, loss_yt: 0.33073967695236206\n",
      "epocht 0, batch_num 5000, step 19803, time: 93.0990514755249 s, accu: 0.8463029861450195, loss_yt: 0.3303612172603607\n",
      "epocht 0, batch_num 5200, step 20003, time: 96.12396430969238 s, accu: 0.8463121056556702, loss_yt: 0.30414849519729614\n",
      "epocht 0, batch_num 5400, step 20203, time: 99.48599743843079 s, accu: 0.8463389277458191, loss_yt: 0.291802316904068\n",
      "epocht 0, batch_num 5600, step 20403, time: 102.64452695846558 s, accu: 0.8463917374610901, loss_yt: 0.35524359345436096\n",
      "epocht 0, batch_num 5800, step 20603, time: 105.85397124290466 s, accu: 0.8464745283126831, loss_yt: 0.3555634915828705\n",
      "epocht 0, batch_num 6000, step 20803, time: 108.85395550727844 s, accu: 0.8465675115585327, loss_yt: 0.2891939878463745\n",
      "epocht 0, batch_num 6200, step 21003, time: 112.40143585205078 s, accu: 0.846705973148346, loss_yt: 0.3030090630054474\n",
      "epocht 0, batch_num 6400, step 21203, time: 115.59788918495178 s, accu: 0.8468378186225891, loss_yt: 0.2868412137031555\n",
      "epocht 0, batch_num 6600, step 21403, time: 118.65774202346802 s, accu: 0.8469905257225037, loss_yt: 0.3622898459434509\n",
      "epocht 0, batch_num 6800, step 21603, time: 121.75143361091614 s, accu: 0.8471292853355408, loss_yt: 0.29406851530075073\n",
      "epocht 0, batch_num 7000, step 21803, time: 124.87511420249939 s, accu: 0.847267210483551, loss_yt: 0.2953868508338928\n",
      "epocht 0, batch_num 7200, step 22003, time: 127.94687724113464 s, accu: 0.8474191427230835, loss_yt: 0.33174654841423035\n",
      "epocht 0, batch_num 7400, step 22203, time: 131.28908610343933 s, accu: 0.8475303053855896, loss_yt: 0.292502224445343\n",
      "epocht 0, batch_num 7600, step 22403, time: 134.39577841758728 s, accu: 0.8475921750068665, loss_yt: 0.3517777919769287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 7800, step 22603, time: 137.93930315971375 s, accu: 0.8477323651313782, loss_yt: 0.26592591404914856\n",
      "epocht 0, batch_num 8000, step 22803, time: 141.09785676002502 s, accu: 0.847889244556427, loss_yt: 0.32570064067840576\n",
      "epocht 0, batch_num 8200, step 23003, time: 144.40501356124878 s, accu: 0.8480260372161865, loss_yt: 0.32439878582954407\n",
      "epocht 0, batch_num 8400, step 23203, time: 147.63139390945435 s, accu: 0.848183274269104, loss_yt: 0.3243264853954315\n",
      "epocht 0, batch_num 8600, step 23403, time: 151.14499044418335 s, accu: 0.8483421206474304, loss_yt: 0.3092232942581177\n",
      "epocht 0, batch_num 8800, step 23603, time: 154.55989003181458 s, accu: 0.848510205745697, loss_yt: 0.279360294342041\n",
      "epocht 0, batch_num 9000, step 23803, time: 157.793212890625 s, accu: 0.8486669659614563, loss_yt: 0.2782863974571228\n",
      "epocht 0, batch_num 9200, step 24003, time: 160.95376229286194 s, accu: 0.8488179445266724, loss_yt: 0.28636598587036133\n",
      "epocht 0, batch_num 9400, step 24203, time: 164.04150581359863 s, accu: 0.8489882946014404, loss_yt: 0.31333494186401367\n",
      "epocht 0, batch_num 9600, step 24403, time: 167.15720677375793 s, accu: 0.8490955829620361, loss_yt: 0.335562139749527\n",
      "epocht 0, batch_num 9800, step 24603, time: 170.2588803768158 s, accu: 0.8492324352264404, loss_yt: 0.2988016605377197\n",
      "epocht 0, batch_num 10000, step 24803, time: 173.31673049926758 s, accu: 0.8493841290473938, loss_yt: 0.41610202193260193\n",
      "epocht 0, batch_num 10200, step 25003, time: 176.42239809036255 s, accu: 0.8495303392410278, loss_yt: 0.3468819558620453\n",
      "epocht 0, batch_num 10400, step 25203, time: 179.64481377601624 s, accu: 0.8496788144111633, loss_yt: 0.3406521677970886\n",
      "epocht 0, batch_num 10600, step 25403, time: 182.71960473060608 s, accu: 0.8498290181159973, loss_yt: 0.32294827699661255\n",
      "epocht 0, batch_num 10800, step 25603, time: 186.06860423088074 s, accu: 0.8500081896781921, loss_yt: 0.29805031418800354\n",
      "epocht 0, batch_num 11000, step 25803, time: 189.09354901313782 s, accu: 0.8501856327056885, loss_yt: 0.2795926034450531\n",
      "epocht 0, batch_num 11200, step 26003, time: 192.362774848938 s, accu: 0.8503349423408508, loss_yt: 0.23584789037704468\n",
      "epocht 0, batch_num 11400, step 26203, time: 195.48542380332947 s, accu: 0.8504313826560974, loss_yt: 0.2620813548564911\n",
      "epocht 0, batch_num 11600, step 26403, time: 198.60608100891113 s, accu: 0.8505355715751648, loss_yt: 0.3713880479335785\n",
      "epocht 0, batch_num 11800, step 26603, time: 202.0767993927002 s, accu: 0.8506683111190796, loss_yt: 0.31856441497802734\n",
      "epocht 0, batch_num 12000, step 26803, time: 205.04685521125793 s, accu: 0.8507980108261108, loss_yt: 0.3682975769042969\n",
      "epocht 0, batch_num 12200, step 27003, time: 208.13460087776184 s, accu: 0.8509355783462524, loss_yt: 0.26197707653045654\n",
      "epocht 0, batch_num 12400, step 27203, time: 211.38990306854248 s, accu: 0.8510826826095581, loss_yt: 0.32089608907699585\n",
      "epocht 0, batch_num 12600, step 27403, time: 214.5185558795929 s, accu: 0.8511814475059509, loss_yt: 0.33460870385169983\n",
      "epocht 0, batch_num 12800, step 27603, time: 217.684063911438 s, accu: 0.8512887954711914, loss_yt: 0.2923889756202698\n",
      "epocht 0, batch_num 13000, step 27803, time: 220.90149426460266 s, accu: 0.8514611721038818, loss_yt: 0.3176543712615967\n",
      "epocht 0, batch_num 13200, step 28003, time: 224.00818729400635 s, accu: 0.8515879511833191, loss_yt: 0.28363069891929626\n",
      "epocht 0, batch_num 13400, step 28203, time: 227.11085557937622 s, accu: 0.851726770401001, loss_yt: 0.25365719199180603\n",
      "epocht 0, batch_num 13600, step 28403, time: 230.16269612312317 s, accu: 0.8518848419189453, loss_yt: 0.3457011878490448\n",
      "epocht 0, batch_num 13800, step 28603, time: 233.41898822784424 s, accu: 0.8520199060440063, loss_yt: 0.25536417961120605\n",
      "epocht 0, batch_num 14000, step 28803, time: 236.67428421974182 s, accu: 0.8521646857261658, loss_yt: 0.25554707646369934\n",
      "epocht 0, batch_num 14200, step 29003, time: 239.92558884620667 s, accu: 0.852304220199585, loss_yt: 0.28056836128234863\n",
      "epocht 0, batch_num 14400, step 29203, time: 243.269681930542 s, accu: 0.8524103164672852, loss_yt: 0.29482269287109375\n",
      "epocht 0, batch_num 14600, step 29403, time: 246.3833224773407 s, accu: 0.8525258302688599, loss_yt: 0.3004656434059143\n",
      "epocht 0, batch_num 14800, step 29603, time: 249.51297807693481 s, accu: 0.8526217937469482, loss_yt: 0.30809664726257324\n",
      "iter_validnum 3701\n",
      "epochv 0, step 29604, stop_n 0, time: 276.32329392433167 s, accu_va: 0.8527488680956525, loss_yv: 0.31430472190381387\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 29605, time: 9.253295660018921 s, accu: 0.8528631925582886, loss_yt: 0.3547477126121521\n",
      "epocht 0, batch_num 200, step 29805, time: 12.493591070175171 s, accu: 0.8529598712921143, loss_yt: 0.32090601325035095\n",
      "epocht 0, batch_num 400, step 30005, time: 15.784790277481079 s, accu: 0.853033721446991, loss_yt: 0.26373493671417236\n",
      "epocht 0, batch_num 600, step 30205, time: 18.95630979537964 s, accu: 0.8531607985496521, loss_yt: 0.2780628800392151\n",
      "epocht 0, batch_num 800, step 30405, time: 22.173706531524658 s, accu: 0.8532785177230835, loss_yt: 0.23772665858268738\n",
      "epocht 0, batch_num 1000, step 30605, time: 25.28039813041687 s, accu: 0.8534185886383057, loss_yt: 0.27985912561416626\n",
      "epocht 0, batch_num 1200, step 30805, time: 28.33822202682495 s, accu: 0.8535462021827698, loss_yt: 0.2913707494735718\n",
      "epocht 0, batch_num 1400, step 31005, time: 31.557615995407104 s, accu: 0.8536396026611328, loss_yt: 0.2938840985298157\n",
      "epocht 0, batch_num 1600, step 31205, time: 34.817965269088745 s, accu: 0.8537203073501587, loss_yt: 0.2658786177635193\n",
      "epocht 0, batch_num 1800, step 31405, time: 38.222864389419556 s, accu: 0.8538617491722107, loss_yt: 0.2618912160396576\n",
      "epocht 0, batch_num 2000, step 31605, time: 41.45920753479004 s, accu: 0.8539916276931763, loss_yt: 0.2844347059726715\n",
      "epocht 0, batch_num 2200, step 31805, time: 44.663639545440674 s, accu: 0.8541220426559448, loss_yt: 0.27771875262260437\n",
      "epocht 0, batch_num 2400, step 32005, time: 47.754374265670776 s, accu: 0.8542829155921936, loss_yt: 0.2549527585506439\n",
      "epocht 0, batch_num 2600, step 32205, time: 51.18021297454834 s, accu: 0.8543834090232849, loss_yt: 0.30710044503211975\n",
      "epocht 0, batch_num 2800, step 32405, time: 54.31214261054993 s, accu: 0.8544785380363464, loss_yt: 0.2828312814235687\n",
      "epocht 0, batch_num 3000, step 32605, time: 57.55048203468323 s, accu: 0.854560911655426, loss_yt: 0.2789382040500641\n",
      "epocht 0, batch_num 3200, step 32805, time: 60.644206047058105 s, accu: 0.8546231985092163, loss_yt: 0.28644752502441406\n",
      "epocht 0, batch_num 3400, step 33005, time: 63.881550312042236 s, accu: 0.8546963334083557, loss_yt: 0.31930968165397644\n",
      "epocht 0, batch_num 3600, step 33205, time: 67.17776799201965 s, accu: 0.8547596335411072, loss_yt: 0.3270660638809204\n",
      "epocht 0, batch_num 3800, step 33405, time: 70.27208518981934 s, accu: 0.854794979095459, loss_yt: 0.28596624732017517\n",
      "epocht 0, batch_num 4000, step 33605, time: 73.46598529815674 s, accu: 0.8548702001571655, loss_yt: 0.4322611391544342\n",
      "epocht 0, batch_num 4200, step 33805, time: 76.57558417320251 s, accu: 0.8549436330795288, loss_yt: 0.3418775498867035\n",
      "epocht 0, batch_num 4400, step 34005, time: 80.17519760131836 s, accu: 0.8550315499305725, loss_yt: 0.2893125116825104\n",
      "epocht 0, batch_num 4600, step 34205, time: 83.25845670700073 s, accu: 0.855080246925354, loss_yt: 0.29006272554397583\n",
      "epocht 0, batch_num 4800, step 34405, time: 86.24048352241516 s, accu: 0.8551396131515503, loss_yt: 0.2689600884914398\n",
      "epocht 0, batch_num 5000, step 34605, time: 89.31030178070068 s, accu: 0.8552091717720032, loss_yt: 0.32777440547943115\n",
      "epocht 0, batch_num 5200, step 34805, time: 92.37933301925659 s, accu: 0.8552917242050171, loss_yt: 0.3039633631706238\n",
      "epocht 0, batch_num 5400, step 35005, time: 95.85507154464722 s, accu: 0.8553504943847656, loss_yt: 0.29603275656700134\n",
      "epocht 0, batch_num 5600, step 35205, time: 99.09341287612915 s, accu: 0.8554090261459351, loss_yt: 0.29504427313804626\n",
      "epocht 0, batch_num 5800, step 35405, time: 102.22201442718506 s, accu: 0.8555041551589966, loss_yt: 0.3243067264556885\n",
      "epocht 0, batch_num 6000, step 35605, time: 105.3360002040863 s, accu: 0.8555836081504822, loss_yt: 0.29848164319992065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 6200, step 35805, time: 108.60123634338379 s, accu: 0.8556593656539917, loss_yt: 0.3121698796749115\n",
      "epocht 0, batch_num 6400, step 36005, time: 112.04801917076111 s, accu: 0.8557577729225159, loss_yt: 0.24884596467018127\n",
      "epocht 0, batch_num 6600, step 36205, time: 115.14174699783325 s, accu: 0.8558434844017029, loss_yt: 0.27199894189834595\n",
      "epocht 0, batch_num 6800, step 36405, time: 118.23151278495789 s, accu: 0.855943500995636, loss_yt: 0.2650752067565918\n",
      "epocht 0, batch_num 7000, step 36605, time: 121.40675163269043 s, accu: 0.8560405373573303, loss_yt: 0.28499794006347656\n",
      "epocht 0, batch_num 7200, step 36805, time: 124.80067658424377 s, accu: 0.8561513423919678, loss_yt: 0.2623201906681061\n",
      "epocht 0, batch_num 7400, step 37005, time: 128.22548532485962 s, accu: 0.8562653064727783, loss_yt: 0.2875611186027527\n",
      "epocht 0, batch_num 7600, step 37205, time: 131.5665512084961 s, accu: 0.8563296794891357, loss_yt: 0.2658633291721344\n",
      "epocht 0, batch_num 7800, step 37405, time: 134.6592879295349 s, accu: 0.8563902378082275, loss_yt: 0.2668163478374481\n",
      "epocht 0, batch_num 8000, step 37605, time: 138.00832533836365 s, accu: 0.8564808964729309, loss_yt: 0.27423685789108276\n",
      "epocht 0, batch_num 8200, step 37805, time: 141.09606552124023 s, accu: 0.8565336465835571, loss_yt: 0.26237308979034424\n",
      "epocht 0, batch_num 8400, step 38005, time: 144.48001742362976 s, accu: 0.8566192388534546, loss_yt: 0.3305016756057739\n",
      "epocht 0, batch_num 8600, step 38205, time: 147.51792335510254 s, accu: 0.8567104935646057, loss_yt: 0.26710429787635803\n",
      "epocht 0, batch_num 8800, step 38405, time: 150.72431874275208 s, accu: 0.8567710518836975, loss_yt: 0.2661304771900177\n",
      "epocht 0, batch_num 9000, step 38605, time: 153.661465883255 s, accu: 0.8568333387374878, loss_yt: 0.23848333954811096\n",
      "epocht 0, batch_num 9200, step 38805, time: 156.75223350524902 s, accu: 0.8568845987319946, loss_yt: 0.3020648956298828\n",
      "epocht 0, batch_num 9400, step 39005, time: 159.8449637889862 s, accu: 0.8569576740264893, loss_yt: 0.30648738145828247\n",
      "epocht 0, batch_num 9600, step 39205, time: 163.12917637825012 s, accu: 0.8570396304130554, loss_yt: 0.2699674069881439\n",
      "epocht 0, batch_num 9800, step 39405, time: 166.08624267578125 s, accu: 0.8571281433105469, loss_yt: 0.4918517470359802\n",
      "epocht 0, batch_num 10000, step 39605, time: 169.38442182540894 s, accu: 0.8572061657905579, loss_yt: 0.28606611490249634\n",
      "epocht 0, batch_num 10200, step 39805, time: 172.5120713710785 s, accu: 0.8573057651519775, loss_yt: 0.26147618889808655\n",
      "epocht 0, batch_num 10400, step 40005, time: 175.83517289161682 s, accu: 0.8573823571205139, loss_yt: 0.3350905179977417\n",
      "epocht 0, batch_num 10600, step 40205, time: 178.88401985168457 s, accu: 0.8574597835540771, loss_yt: 0.2907898426055908\n",
      "epocht 0, batch_num 10800, step 40405, time: 182.18020677566528 s, accu: 0.8575299382209778, loss_yt: 0.3523229956626892\n",
      "epocht 0, batch_num 11000, step 40605, time: 185.2520191669464 s, accu: 0.8576334714889526, loss_yt: 0.2784230411052704\n",
      "epocht 0, batch_num 11200, step 40805, time: 189.16552686691284 s, accu: 0.857711911201477, loss_yt: 0.2875124514102936\n",
      "epocht 0, batch_num 11400, step 41005, time: 192.5305631160736 s, accu: 0.8577830791473389, loss_yt: 0.2561255693435669\n",
      "epocht 0, batch_num 11600, step 41205, time: 195.68412899971008 s, accu: 0.8578538298606873, loss_yt: 0.2903425991535187\n",
      "epocht 0, batch_num 11800, step 41405, time: 198.79278421401978 s, accu: 0.8579044938087463, loss_yt: 0.3610970079898834\n",
      "epocht 0, batch_num 12000, step 41605, time: 201.9473659992218 s, accu: 0.8579440712928772, loss_yt: 0.3095009922981262\n",
      "epocht 0, batch_num 12200, step 41805, time: 205.05902791023254 s, accu: 0.8579863905906677, loss_yt: 0.3149997293949127\n",
      "epocht 0, batch_num 12400, step 42005, time: 208.2545187473297 s, accu: 0.8580241799354553, loss_yt: 0.28103095293045044\n",
      "epocht 0, batch_num 12600, step 42205, time: 211.37214589118958 s, accu: 0.858087420463562, loss_yt: 0.25192493200302124\n",
      "epocht 0, batch_num 12800, step 42405, time: 214.5227222442627 s, accu: 0.8581626415252686, loss_yt: 0.3027898669242859\n",
      "epocht 0, batch_num 13000, step 42605, time: 217.50278520584106 s, accu: 0.8582174777984619, loss_yt: 0.2926863729953766\n",
      "epocht 0, batch_num 13200, step 42805, time: 220.6373701095581 s, accu: 0.8582699298858643, loss_yt: 0.33341339230537415\n",
      "epocht 0, batch_num 13400, step 43005, time: 223.95649480819702 s, accu: 0.8582478761672974, loss_yt: 0.318209707736969\n",
      "epocht 0, batch_num 13600, step 43205, time: 226.982403755188 s, accu: 0.8583000898361206, loss_yt: 0.2822229564189911\n",
      "epocht 0, batch_num 13800, step 43405, time: 230.4731023311615 s, accu: 0.8583762049674988, loss_yt: 0.369211882352829\n",
      "epocht 0, batch_num 14000, step 43605, time: 233.5967173576355 s, accu: 0.8584593534469604, loss_yt: 0.27399584650993347\n",
      "epocht 0, batch_num 14200, step 43805, time: 236.68844890594482 s, accu: 0.8585509061813354, loss_yt: 0.286557137966156\n",
      "epocht 0, batch_num 14400, step 44005, time: 239.76921319961548 s, accu: 0.8585999608039856, loss_yt: 0.3187330663204193\n",
      "epocht 0, batch_num 14600, step 44205, time: 242.85498714447021 s, accu: 0.8586642742156982, loss_yt: 0.26070520281791687\n",
      "epocht 0, batch_num 14800, step 44405, time: 245.98858189582825 s, accu: 0.8587409853935242, loss_yt: 0.2712562680244446\n",
      "iter_validnum 3701\n",
      "epochv 0, step 44406, stop_n 1, time: 273.11903619766235 s, accu_va: 0.8596931971333794, loss_yv: 0.27061673773362555\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 44407, time: 9.50754976272583 s, accu: 0.8606171607971191, loss_yt: 0.2699936628341675\n",
      "epocht 0, batch_num 200, step 44607, time: 12.69602370262146 s, accu: 0.8606661558151245, loss_yt: 0.27099689841270447\n",
      "epocht 0, batch_num 400, step 44807, time: 15.924392938613892 s, accu: 0.8607205152511597, loss_yt: 0.235672265291214\n",
      "epocht 0, batch_num 600, step 45007, time: 19.23952555656433 s, accu: 0.860785722732544, loss_yt: 0.2766062021255493\n",
      "epocht 0, batch_num 800, step 45207, time: 22.29435706138611 s, accu: 0.8608706593513489, loss_yt: 0.2862910032272339\n",
      "epocht 0, batch_num 1000, step 45407, time: 25.344227075576782 s, accu: 0.8609275221824646, loss_yt: 0.2391640543937683\n",
      "epocht 0, batch_num 1200, step 45607, time: 28.51671814918518 s, accu: 0.8609855771064758, loss_yt: 0.28067201375961304\n",
      "epocht 0, batch_num 1400, step 45807, time: 31.564568758010864 s, accu: 0.8610062599182129, loss_yt: 0.3241070508956909\n",
      "epocht 0, batch_num 1600, step 46007, time: 35.14499568939209 s, accu: 0.8610182404518127, loss_yt: 0.3480985760688782\n",
      "epocht 0, batch_num 1800, step 46207, time: 38.30059242248535 s, accu: 0.8610336780548096, loss_yt: 0.3233816623687744\n",
      "epocht 0, batch_num 2000, step 46407, time: 41.5299232006073 s, accu: 0.8610273003578186, loss_yt: 0.2919580936431885\n",
      "epocht 0, batch_num 2200, step 46607, time: 44.85406446456909 s, accu: 0.8610422015190125, loss_yt: 0.26305773854255676\n",
      "epocht 0, batch_num 2400, step 46807, time: 47.87495470046997 s, accu: 0.8610845804214478, loss_yt: 0.2962239682674408\n",
      "epocht 0, batch_num 2600, step 47007, time: 51.40654730796814 s, accu: 0.8611469268798828, loss_yt: 0.29064488410949707\n",
      "epocht 0, batch_num 2800, step 47207, time: 54.57803130149841 s, accu: 0.8611758947372437, loss_yt: 0.29652246832847595\n",
      "epocht 0, batch_num 3000, step 47407, time: 57.69369888305664 s, accu: 0.8612207174301147, loss_yt: 0.28345993161201477\n",
      "epocht 0, batch_num 3200, step 47607, time: 60.951985597610474 s, accu: 0.8612003922462463, loss_yt: 0.28722018003463745\n",
      "epocht 0, batch_num 3400, step 47807, time: 63.958984375 s, accu: 0.861153781414032, loss_yt: 0.3327009379863739\n",
      "epocht 0, batch_num 3600, step 48007, time: 67.28305673599243 s, accu: 0.8611294031143188, loss_yt: 0.29255032539367676\n",
      "epocht 0, batch_num 3800, step 48207, time: 70.43861794471741 s, accu: 0.861171543598175, loss_yt: 0.29045164585113525\n",
      "epocht 0, batch_num 4000, step 48407, time: 73.56328964233398 s, accu: 0.8612329363822937, loss_yt: 0.31937605142593384\n",
      "epocht 0, batch_num 4200, step 48607, time: 76.75273489952087 s, accu: 0.8612915277481079, loss_yt: 0.2866053283214569\n",
      "epocht 0, batch_num 4400, step 48807, time: 79.84247946739197 s, accu: 0.8613227605819702, loss_yt: 0.31752821803092957\n",
      "epocht 0, batch_num 4600, step 49007, time: 82.91724991798401 s, accu: 0.8613646030426025, loss_yt: 0.30066871643066406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 4800, step 49207, time: 85.93022012710571 s, accu: 0.8614141941070557, loss_yt: 0.3409787714481354\n",
      "epocht 0, batch_num 5000, step 49407, time: 89.12766861915588 s, accu: 0.8614843487739563, loss_yt: 0.27936074137687683\n",
      "epocht 0, batch_num 5200, step 49607, time: 92.10668206214905 s, accu: 0.8615529537200928, loss_yt: 0.26920533180236816\n",
      "epocht 0, batch_num 5400, step 49807, time: 95.33405208587646 s, accu: 0.8615960478782654, loss_yt: 0.2327905148267746\n",
      "epocht 0, batch_num 5600, step 50007, time: 98.70503878593445 s, accu: 0.8616555333137512, loss_yt: 0.2992474436759949\n",
      "epocht 0, batch_num 5800, step 50207, time: 101.96834564208984 s, accu: 0.8617175221443176, loss_yt: 0.3618507385253906\n",
      "epocht 0, batch_num 6000, step 50407, time: 105.06602907180786 s, accu: 0.8617626428604126, loss_yt: 0.31552350521087646\n",
      "epocht 0, batch_num 6200, step 50607, time: 108.08196473121643 s, accu: 0.8618224263191223, loss_yt: 0.21974018216133118\n",
      "epocht 0, batch_num 6400, step 50807, time: 111.16575193405151 s, accu: 0.8618969917297363, loss_yt: 0.28134825825691223\n",
      "epocht 0, batch_num 6600, step 51007, time: 114.39111876487732 s, accu: 0.8619523644447327, loss_yt: 0.3038877844810486\n",
      "epocht 0, batch_num 6800, step 51207, time: 117.63940858840942 s, accu: 0.8620103597640991, loss_yt: 0.3095278739929199\n",
      "epocht 0, batch_num 7000, step 51407, time: 120.89174818992615 s, accu: 0.8620684146881104, loss_yt: 0.2438109964132309\n",
      "epocht 0, batch_num 7200, step 51607, time: 124.14002418518066 s, accu: 0.8621188998222351, loss_yt: 0.275093674659729\n",
      "epocht 0, batch_num 7400, step 51807, time: 127.66659617424011 s, accu: 0.8621281385421753, loss_yt: 0.27250465750694275\n",
      "epocht 0, batch_num 7600, step 52007, time: 130.88203287124634 s, accu: 0.8621787428855896, loss_yt: 0.25432586669921875\n",
      "epocht 0, batch_num 7800, step 52207, time: 133.93087577819824 s, accu: 0.8622274398803711, loss_yt: 0.2942821681499481\n",
      "epocht 0, batch_num 8000, step 52407, time: 137.2348666191101 s, accu: 0.862292468547821, loss_yt: 0.3184528350830078\n",
      "epocht 0, batch_num 8200, step 52607, time: 140.29568147659302 s, accu: 0.8623467087745667, loss_yt: 0.25386250019073486\n",
      "epocht 0, batch_num 8400, step 52807, time: 143.63278770446777 s, accu: 0.8623947501182556, loss_yt: 0.27477556467056274\n",
      "epocht 0, batch_num 8600, step 53007, time: 146.93193888664246 s, accu: 0.8624395728111267, loss_yt: 0.26962149143218994\n",
      "epocht 0, batch_num 8800, step 53207, time: 150.01172637939453 s, accu: 0.8624916672706604, loss_yt: 0.29848814010620117\n",
      "epocht 0, batch_num 9000, step 53407, time: 153.23607850074768 s, accu: 0.8625307083129883, loss_yt: 0.25433820486068726\n",
      "epocht 0, batch_num 9200, step 53607, time: 156.37668085098267 s, accu: 0.8625861406326294, loss_yt: 0.2887725234031677\n",
      "epocht 0, batch_num 9400, step 53807, time: 159.98902082443237 s, accu: 0.8626468181610107, loss_yt: 0.274075984954834\n",
      "epocht 0, batch_num 9600, step 54007, time: 163.09473490715027 s, accu: 0.8627033233642578, loss_yt: 0.2691075801849365\n",
      "epocht 0, batch_num 9800, step 54207, time: 166.11367344856262 s, accu: 0.8627485632896423, loss_yt: 0.3203009366989136\n",
      "epocht 0, batch_num 10000, step 54407, time: 169.4626874923706 s, accu: 0.8628017902374268, loss_yt: 0.2727242112159729\n",
      "epocht 0, batch_num 10200, step 54607, time: 172.61825227737427 s, accu: 0.8628265261650085, loss_yt: 0.2505062520503998\n",
      "epocht 0, batch_num 10400, step 54807, time: 175.8924958705902 s, accu: 0.8628699779510498, loss_yt: 0.2930908501148224\n",
      "epocht 0, batch_num 10600, step 55007, time: 178.938378572464 s, accu: 0.8629189729690552, loss_yt: 0.31165072321891785\n",
      "epocht 0, batch_num 10800, step 55207, time: 182.29437637329102 s, accu: 0.8629764318466187, loss_yt: 0.25236183404922485\n",
      "epocht 0, batch_num 11000, step 55407, time: 185.37613463401794 s, accu: 0.8630214929580688, loss_yt: 0.2795024514198303\n",
      "epocht 0, batch_num 11200, step 55607, time: 188.5217695236206 s, accu: 0.863084614276886, loss_yt: 0.29195988178253174\n",
      "epocht 0, batch_num 11400, step 55807, time: 191.85281825065613 s, accu: 0.86313396692276, loss_yt: 0.3814584016799927\n",
      "epocht 0, batch_num 11600, step 56007, time: 195.1211051940918 s, accu: 0.8631700873374939, loss_yt: 0.27397534251213074\n",
      "epocht 0, batch_num 11800, step 56207, time: 198.44319367408752 s, accu: 0.8632090091705322, loss_yt: 0.28735965490341187\n",
      "epocht 0, batch_num 12000, step 56407, time: 201.57682275772095 s, accu: 0.8632630109786987, loss_yt: 0.2918976843357086\n",
      "epocht 0, batch_num 12200, step 56607, time: 204.59274911880493 s, accu: 0.8633071184158325, loss_yt: 0.28832486271858215\n",
      "epocht 0, batch_num 12400, step 56807, time: 207.8351068496704 s, accu: 0.8633548021316528, loss_yt: 0.24960064888000488\n",
      "epocht 0, batch_num 12600, step 57007, time: 211.07046031951904 s, accu: 0.8633873462677002, loss_yt: 0.26978930830955505\n",
      "epocht 0, batch_num 12800, step 57207, time: 214.21206259727478 s, accu: 0.8634384870529175, loss_yt: 0.24384719133377075\n",
      "epocht 0, batch_num 13000, step 57407, time: 217.25189900398254 s, accu: 0.8634845614433289, loss_yt: 0.254096120595932\n",
      "epocht 0, batch_num 13200, step 57607, time: 220.4384160041809 s, accu: 0.8635232448577881, loss_yt: 0.26734212040901184\n",
      "epocht 0, batch_num 13400, step 57807, time: 223.6258566379547 s, accu: 0.8635624647140503, loss_yt: 0.24533134698867798\n",
      "epocht 0, batch_num 13600, step 58007, time: 226.81036520004272 s, accu: 0.8636164665222168, loss_yt: 0.26081231236457825\n",
      "epocht 0, batch_num 13800, step 58207, time: 230.25413179397583 s, accu: 0.8636651635169983, loss_yt: 0.2901974320411682\n",
      "epocht 0, batch_num 14000, step 58407, time: 233.549320936203 s, accu: 0.8637179136276245, loss_yt: 0.21969854831695557\n",
      "epocht 0, batch_num 14200, step 58607, time: 236.80564379692078 s, accu: 0.8637646436691284, loss_yt: 0.21963128447532654\n",
      "epocht 0, batch_num 14400, step 58807, time: 239.75675010681152 s, accu: 0.8638118505477905, loss_yt: 0.2892144024372101\n",
      "epocht 0, batch_num 14600, step 59007, time: 242.98808002471924 s, accu: 0.8638623356819153, loss_yt: 0.24899260699748993\n",
      "epocht 0, batch_num 14800, step 59207, time: 246.39899611473083 s, accu: 0.863915741443634, loss_yt: 0.2583458721637726\n",
      "iter_validnum 3701\n",
      "epochv 0, step 59208, stop_n 0, time: 273.42272448539734 s, accu_va: 0.8644126681693983, loss_yv: 0.2846344291068708\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 59209, time: 9.116581678390503 s, accu: 0.8648877143859863, loss_yt: 0.3230462670326233\n",
      "epocht 0, batch_num 200, step 59409, time: 12.218316316604614 s, accu: 0.8649349808692932, loss_yt: 0.22228220105171204\n",
      "epocht 0, batch_num 400, step 59609, time: 15.437680721282959 s, accu: 0.8649775981903076, loss_yt: 0.2946961224079132\n",
      "epocht 0, batch_num 600, step 59809, time: 18.76578187942505 s, accu: 0.8650152087211609, loss_yt: 0.27354151010513306\n",
      "epocht 0, batch_num 800, step 60009, time: 21.847571849822998 s, accu: 0.8650473952293396, loss_yt: 0.4946684241294861\n",
      "epocht 0, batch_num 1000, step 60209, time: 25.11981511116028 s, accu: 0.8650822639465332, loss_yt: 0.31698665022850037\n",
      "epocht 0, batch_num 1200, step 60409, time: 28.465842962265015 s, accu: 0.8651179075241089, loss_yt: 0.2996700406074524\n",
      "epocht 0, batch_num 1400, step 60609, time: 31.484769582748413 s, accu: 0.8651506304740906, loss_yt: 0.2963577210903168\n",
      "epocht 0, batch_num 1600, step 60809, time: 34.674272775650024 s, accu: 0.8651648759841919, loss_yt: 0.3224490284919739\n",
      "epocht 0, batch_num 1800, step 61009, time: 37.8417706489563 s, accu: 0.8651803135871887, loss_yt: 0.29409047961235046\n",
      "epocht 0, batch_num 2000, step 61209, time: 40.991347789764404 s, accu: 0.8652060627937317, loss_yt: 0.24510803818702698\n",
      "epocht 0, batch_num 2200, step 61409, time: 44.44910264015198 s, accu: 0.865217924118042, loss_yt: 0.2711019515991211\n",
      "epocht 0, batch_num 2400, step 61609, time: 47.73431992530823 s, accu: 0.8652100563049316, loss_yt: 0.3422444462776184\n",
      "epocht 0, batch_num 2600, step 61809, time: 51.01855492591858 s, accu: 0.8652366995811462, loss_yt: 0.29536330699920654\n",
      "epocht 0, batch_num 2800, step 62009, time: 54.094342947006226 s, accu: 0.865267813205719, loss_yt: 0.31381624937057495\n",
      "epocht 0, batch_num 3000, step 62209, time: 57.17008638381958 s, accu: 0.8653092384338379, loss_yt: 0.2676210105419159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 3200, step 62409, time: 60.373520374298096 s, accu: 0.8653562068939209, loss_yt: 0.2703211009502411\n",
      "epocht 0, batch_num 3400, step 62609, time: 63.60488271713257 s, accu: 0.8654029965400696, loss_yt: 0.2585136592388153\n",
      "epocht 0, batch_num 3600, step 62809, time: 66.81432437896729 s, accu: 0.8654552698135376, loss_yt: 0.2443966269493103\n",
      "epocht 0, batch_num 3800, step 63009, time: 70.11946129798889 s, accu: 0.8654957413673401, loss_yt: 0.23500649631023407\n",
      "epocht 0, batch_num 4000, step 63209, time: 73.35780048370361 s, accu: 0.8655359745025635, loss_yt: 0.27146071195602417\n",
      "epocht 0, batch_num 4200, step 63409, time: 76.67891883850098 s, accu: 0.8655788898468018, loss_yt: 0.26053690910339355\n",
      "epocht 0, batch_num 4400, step 63609, time: 79.99109959602356 s, accu: 0.8656208515167236, loss_yt: 0.24989436566829681\n",
      "epocht 0, batch_num 4600, step 63809, time: 82.98808097839355 s, accu: 0.8656437397003174, loss_yt: 0.2728290557861328\n",
      "epocht 0, batch_num 4800, step 64009, time: 86.29919505119324 s, accu: 0.8656651377677917, loss_yt: 0.31607019901275635\n",
      "epocht 0, batch_num 5000, step 64209, time: 89.38992977142334 s, accu: 0.8656915426254272, loss_yt: 0.2887754738330841\n",
      "epocht 0, batch_num 5200, step 64409, time: 92.61033487319946 s, accu: 0.8657273054122925, loss_yt: 0.29104095697402954\n",
      "epocht 0, batch_num 5400, step 64609, time: 95.78587031364441 s, accu: 0.8657802939414978, loss_yt: 0.30698761343955994\n",
      "epocht 0, batch_num 5600, step 64809, time: 99.07304501533508 s, accu: 0.8657627105712891, loss_yt: 0.34884458780288696\n",
      "epocht 0, batch_num 5800, step 65009, time: 102.24455785751343 s, accu: 0.8657700419425964, loss_yt: 0.28386837244033813\n",
      "epocht 0, batch_num 6000, step 65209, time: 105.28645062446594 s, accu: 0.8657941222190857, loss_yt: 0.2335321605205536\n",
      "epocht 0, batch_num 6200, step 65409, time: 108.6683886051178 s, accu: 0.8658302426338196, loss_yt: 0.2815355956554413\n",
      "epocht 0, batch_num 6400, step 65609, time: 111.90771651268005 s, accu: 0.8658462166786194, loss_yt: 0.2666834890842438\n",
      "epocht 0, batch_num 6600, step 65809, time: 115.09519290924072 s, accu: 0.8658888936042786, loss_yt: 0.285757839679718\n",
      "epocht 0, batch_num 6800, step 66009, time: 118.3804407119751 s, accu: 0.8658905029296875, loss_yt: 0.3235365152359009\n",
      "epocht 0, batch_num 7000, step 66209, time: 121.4511981010437 s, accu: 0.865888237953186, loss_yt: 0.29579439759254456\n",
      "epocht 0, batch_num 7200, step 66409, time: 124.9797625541687 s, accu: 0.8659211993217468, loss_yt: 0.2694302201271057\n",
      "epocht 0, batch_num 7400, step 66609, time: 128.25101566314697 s, accu: 0.865963876247406, loss_yt: 0.2810875177383423\n",
      "epocht 0, batch_num 7600, step 66809, time: 131.4434769153595 s, accu: 0.8660000562667847, loss_yt: 0.2676631510257721\n",
      "epocht 0, batch_num 7800, step 67009, time: 134.74767470359802 s, accu: 0.8660399913787842, loss_yt: 0.2706912159919739\n",
      "epocht 0, batch_num 8000, step 67209, time: 137.85832524299622 s, accu: 0.8660815954208374, loss_yt: 0.3593980371952057\n",
      "epocht 0, batch_num 8200, step 67409, time: 141.16849732398987 s, accu: 0.8661174774169922, loss_yt: 0.26769179105758667\n",
      "epocht 0, batch_num 8400, step 67609, time: 144.42676424980164 s, accu: 0.8661510944366455, loss_yt: 0.27657222747802734\n",
      "epocht 0, batch_num 8600, step 67809, time: 147.514502286911 s, accu: 0.8661634922027588, loss_yt: 0.2923522889614105\n",
      "epocht 0, batch_num 8800, step 68009, time: 150.67806267738342 s, accu: 0.8661928176879883, loss_yt: 0.2435968518257141\n",
      "epocht 0, batch_num 9000, step 68209, time: 153.86156392097473 s, accu: 0.8662074208259583, loss_yt: 0.25108951330184937\n",
      "epocht 0, batch_num 9200, step 68409, time: 156.8984100818634 s, accu: 0.8662360906600952, loss_yt: 0.28248167037963867\n",
      "epocht 0, batch_num 9400, step 68609, time: 160.27238821983337 s, accu: 0.8662632703781128, loss_yt: 0.3083186149597168\n",
      "epocht 0, batch_num 9600, step 68809, time: 163.34720969200134 s, accu: 0.8662949204444885, loss_yt: 0.28683096170425415\n",
      "epocht 0, batch_num 9800, step 69009, time: 166.9365737438202 s, accu: 0.8663086891174316, loss_yt: 0.3217678368091583\n",
      "epocht 0, batch_num 10000, step 69209, time: 170.1370449066162 s, accu: 0.8663377165794373, loss_yt: 0.3183847665786743\n",
      "epocht 0, batch_num 10200, step 69409, time: 173.19882202148438 s, accu: 0.8663408756256104, loss_yt: 0.28017446398735046\n",
      "epocht 0, batch_num 10400, step 69609, time: 176.67256569862366 s, accu: 0.8663659691810608, loss_yt: 0.2755098342895508\n",
      "epocht 0, batch_num 10600, step 69809, time: 179.85306191444397 s, accu: 0.8663555383682251, loss_yt: 0.3113985061645508\n",
      "epocht 0, batch_num 10800, step 70009, time: 183.24296617507935 s, accu: 0.8663517236709595, loss_yt: 0.2803283929824829\n",
      "epocht 0, batch_num 11000, step 70209, time: 186.40151810646057 s, accu: 0.8663748502731323, loss_yt: 0.28610825538635254\n",
      "epocht 0, batch_num 11200, step 70409, time: 189.66180205345154 s, accu: 0.8664024472236633, loss_yt: 0.2959398329257965\n",
      "epocht 0, batch_num 11400, step 70609, time: 192.9799280166626 s, accu: 0.866420328617096, loss_yt: 0.27066731452941895\n",
      "epocht 0, batch_num 11600, step 70809, time: 196.03475880622864 s, accu: 0.8664363622665405, loss_yt: 0.30455368757247925\n",
      "epocht 0, batch_num 11800, step 71009, time: 199.39776635169983 s, accu: 0.8664596080780029, loss_yt: 0.2631503641605377\n",
      "epocht 0, batch_num 12000, step 71209, time: 202.53341102600098 s, accu: 0.8664840459823608, loss_yt: 0.2686956822872162\n",
      "epocht 0, batch_num 12200, step 71409, time: 205.59818863868713 s, accu: 0.8665028214454651, loss_yt: 0.5188921093940735\n",
      "epocht 0, batch_num 12400, step 71609, time: 208.9791464805603 s, accu: 0.8664996027946472, loss_yt: 0.33447468280792236\n",
      "epocht 0, batch_num 12600, step 71809, time: 212.17762565612793 s, accu: 0.8664875626564026, loss_yt: 0.26817357540130615\n",
      "epocht 0, batch_num 12800, step 72009, time: 215.42590761184692 s, accu: 0.8665096759796143, loss_yt: 0.3011191487312317\n",
      "epocht 0, batch_num 13000, step 72209, time: 218.9544711112976 s, accu: 0.8665295839309692, loss_yt: 0.3055376708507538\n",
      "epocht 0, batch_num 13200, step 72409, time: 222.01528763771057 s, accu: 0.8665633797645569, loss_yt: 0.30640894174575806\n",
      "epocht 0, batch_num 13400, step 72609, time: 225.27759623527527 s, accu: 0.8665742874145508, loss_yt: 0.2700991630554199\n",
      "epocht 0, batch_num 13600, step 72809, time: 228.36630368232727 s, accu: 0.8665965795516968, loss_yt: 0.29356274008750916\n",
      "epocht 0, batch_num 13800, step 73009, time: 231.4610276222229 s, accu: 0.8666093349456787, loss_yt: 0.2703426778316498\n",
      "epocht 0, batch_num 14000, step 73209, time: 234.7991042137146 s, accu: 0.8666372895240784, loss_yt: 0.29495370388031006\n",
      "epocht 0, batch_num 14200, step 73409, time: 237.81902718544006 s, accu: 0.8666717410087585, loss_yt: 0.27402210235595703\n",
      "epocht 0, batch_num 14400, step 73609, time: 241.1760778427124 s, accu: 0.8666815161705017, loss_yt: 0.28877729177474976\n",
      "epocht 0, batch_num 14600, step 73809, time: 244.39045357704163 s, accu: 0.8666979670524597, loss_yt: 0.27352648973464966\n",
      "epocht 0, batch_num 14800, step 74009, time: 247.9339838027954 s, accu: 0.8666792511940002, loss_yt: 0.3250983953475952\n",
      "iter_validnum 3701\n",
      "epochv 0, step 74010, stop_n 0, time: 275.5331778526306 s, accu_va: 0.8662678955278342, loss_yv: 0.3346866046100396\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 74011, time: 9.678120136260986 s, accu: 0.8658666014671326, loss_yt: 0.395478755235672\n",
      "epocht 1, batch_num 200, step 74211, time: 12.972316026687622 s, accu: 0.8658878207206726, loss_yt: 0.2798756957054138\n",
      "epocht 1, batch_num 400, step 74411, time: 16.270515203475952 s, accu: 0.8659356832504272, loss_yt: 0.28716176748275757\n",
      "epocht 1, batch_num 600, step 74611, time: 19.550745964050293 s, accu: 0.8659748435020447, loss_yt: 0.2753733694553375\n",
      "epocht 1, batch_num 800, step 74811, time: 22.86186957359314 s, accu: 0.8660162687301636, loss_yt: 0.2872871458530426\n",
      "epocht 1, batch_num 1000, step 75011, time: 26.2258780002594 s, accu: 0.8660562634468079, loss_yt: 0.24069365859031677\n",
      "epocht 1, batch_num 1200, step 75211, time: 29.380467653274536 s, accu: 0.8660842180252075, loss_yt: 0.23830287158489227\n",
      "epocht 1, batch_num 1400, step 75411, time: 33.170334815979004 s, accu: 0.8660852313041687, loss_yt: 0.3695756196975708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 1600, step 75611, time: 36.57519578933716 s, accu: 0.8660430312156677, loss_yt: 0.30058273673057556\n",
      "epocht 1, batch_num 1800, step 75811, time: 39.87038493156433 s, accu: 0.8660626411437988, loss_yt: 0.2427511215209961\n",
      "epocht 1, batch_num 2000, step 76011, time: 42.919262647628784 s, accu: 0.8661043643951416, loss_yt: 0.22428469359874725\n",
      "epocht 1, batch_num 2200, step 76211, time: 46.014954805374146 s, accu: 0.8661428689956665, loss_yt: 0.24438323080539703\n",
      "epocht 1, batch_num 2400, step 76411, time: 49.58943247795105 s, accu: 0.8661736845970154, loss_yt: 0.27414199709892273\n",
      "epocht 1, batch_num 2600, step 76611, time: 52.74096894264221 s, accu: 0.8662055134773254, loss_yt: 0.2616119682788849\n",
      "epocht 1, batch_num 2800, step 76811, time: 55.98133635520935 s, accu: 0.8662356734275818, loss_yt: 0.26608437299728394\n",
      "epocht 1, batch_num 3000, step 77011, time: 59.18274426460266 s, accu: 0.866268515586853, loss_yt: 0.259907066822052\n",
      "epocht 1, batch_num 3200, step 77211, time: 62.32234764099121 s, accu: 0.8663048148155212, loss_yt: 0.2556687891483307\n",
      "epocht 1, batch_num 3400, step 77411, time: 66.05439805984497 s, accu: 0.8663433790206909, loss_yt: 0.39222708344459534\n",
      "epocht 1, batch_num 3600, step 77611, time: 69.26677989959717 s, accu: 0.8663551211357117, loss_yt: 0.2319244146347046\n",
      "epocht 1, batch_num 3800, step 77811, time: 72.41436195373535 s, accu: 0.8663662075996399, loss_yt: 0.3119410574436188\n",
      "epocht 1, batch_num 4000, step 78011, time: 75.48913955688477 s, accu: 0.8664081692695618, loss_yt: 0.25070786476135254\n",
      "epocht 1, batch_num 4200, step 78211, time: 78.64271402359009 s, accu: 0.866457462310791, loss_yt: 0.26936647295951843\n",
      "epocht 1, batch_num 4400, step 78411, time: 82.22413063049316 s, accu: 0.8664947748184204, loss_yt: 0.2965208888053894\n",
      "epocht 1, batch_num 4600, step 78611, time: 85.38068985939026 s, accu: 0.8665213584899902, loss_yt: 0.25817960500717163\n",
      "epocht 1, batch_num 4800, step 78811, time: 88.7177963256836 s, accu: 0.8665541410446167, loss_yt: 0.23414283990859985\n",
      "epocht 1, batch_num 5000, step 79011, time: 92.01195883750916 s, accu: 0.8665804862976074, loss_yt: 0.23293963074684143\n",
      "epocht 1, batch_num 5200, step 79211, time: 95.26429414749146 s, accu: 0.8665957450866699, loss_yt: 0.34650078415870667\n",
      "epocht 1, batch_num 5400, step 79411, time: 98.8775987625122 s, accu: 0.8666139841079712, loss_yt: 0.2631400227546692\n",
      "epocht 1, batch_num 5600, step 79611, time: 102.11494445800781 s, accu: 0.8666402101516724, loss_yt: 0.24510535597801208\n",
      "epocht 1, batch_num 5800, step 79811, time: 105.22562384605408 s, accu: 0.866667628288269, loss_yt: 0.2680501341819763\n",
      "epocht 1, batch_num 6000, step 80011, time: 108.64348649978638 s, accu: 0.866706132888794, loss_yt: 0.28234782814979553\n",
      "epocht 1, batch_num 6200, step 80211, time: 111.86487030982971 s, accu: 0.8667439222335815, loss_yt: 0.2673570513725281\n",
      "epocht 1, batch_num 6400, step 80411, time: 115.41440677642822 s, accu: 0.8667936325073242, loss_yt: 0.2324531525373459\n",
      "epocht 1, batch_num 6600, step 80611, time: 118.58291268348694 s, accu: 0.8668376207351685, loss_yt: 0.2807081937789917\n",
      "epocht 1, batch_num 6800, step 80811, time: 121.89704465866089 s, accu: 0.866860032081604, loss_yt: 0.27076584100723267\n",
      "epocht 1, batch_num 7000, step 81011, time: 125.12640905380249 s, accu: 0.8668866157531738, loss_yt: 0.29271918535232544\n",
      "epocht 1, batch_num 7200, step 81211, time: 128.26900672912598 s, accu: 0.8669070601463318, loss_yt: 0.2161685824394226\n",
      "epocht 1, batch_num 7400, step 81411, time: 131.52031111717224 s, accu: 0.8669467568397522, loss_yt: 0.2420678734779358\n",
      "epocht 1, batch_num 7600, step 81611, time: 134.68884301185608 s, accu: 0.8669923543930054, loss_yt: 0.294910728931427\n",
      "epocht 1, batch_num 7800, step 81811, time: 137.88133215904236 s, accu: 0.8670016527175903, loss_yt: 0.29367417097091675\n",
      "epocht 1, batch_num 8000, step 82011, time: 141.0229012966156 s, accu: 0.8670348525047302, loss_yt: 0.28817275166511536\n",
      "epocht 1, batch_num 8200, step 82211, time: 144.2373390197754 s, accu: 0.8670598268508911, loss_yt: 0.2804473042488098\n",
      "epocht 1, batch_num 8400, step 82411, time: 147.5444951057434 s, accu: 0.8670856356620789, loss_yt: 0.3057263493537903\n",
      "epocht 1, batch_num 8600, step 82611, time: 150.78480792045593 s, accu: 0.8671218156814575, loss_yt: 0.26516830921173096\n",
      "epocht 1, batch_num 8800, step 82811, time: 154.08000540733337 s, accu: 0.8671542406082153, loss_yt: 0.32454514503479004\n",
      "epocht 1, batch_num 9000, step 83011, time: 157.35323405265808 s, accu: 0.8671863079071045, loss_yt: 0.26829293370246887\n",
      "epocht 1, batch_num 9200, step 83211, time: 160.4748866558075 s, accu: 0.8672119379043579, loss_yt: 0.24110029637813568\n",
      "epocht 1, batch_num 9400, step 83411, time: 163.9596004486084 s, accu: 0.867235541343689, loss_yt: 0.22428953647613525\n",
      "epocht 1, batch_num 9600, step 83611, time: 167.25475692749023 s, accu: 0.8672730326652527, loss_yt: 0.26140525937080383\n",
      "epocht 1, batch_num 9800, step 83811, time: 170.45519995689392 s, accu: 0.8673076629638672, loss_yt: 0.2897067964076996\n",
      "epocht 1, batch_num 10000, step 84011, time: 173.62971830368042 s, accu: 0.8673471212387085, loss_yt: 0.28754544258117676\n",
      "epocht 1, batch_num 10200, step 84211, time: 176.90099334716797 s, accu: 0.8673956394195557, loss_yt: 0.28962481021881104\n",
      "epocht 1, batch_num 10400, step 84411, time: 180.34475564956665 s, accu: 0.8674273490905762, loss_yt: 0.29987770318984985\n",
      "epocht 1, batch_num 10600, step 84611, time: 183.44948601722717 s, accu: 0.8674449324607849, loss_yt: 0.3433411121368408\n",
      "epocht 1, batch_num 10800, step 84811, time: 186.65890288352966 s, accu: 0.8674598336219788, loss_yt: 0.27448296546936035\n",
      "epocht 1, batch_num 11000, step 85011, time: 190.020911693573 s, accu: 0.8675041794776917, loss_yt: 0.2618720531463623\n",
      "epocht 1, batch_num 11200, step 85211, time: 193.09166860580444 s, accu: 0.8675438761711121, loss_yt: 0.26472729444503784\n",
      "epocht 1, batch_num 11400, step 85411, time: 196.70600247383118 s, accu: 0.8675875067710876, loss_yt: 0.273933082818985\n",
      "epocht 1, batch_num 11600, step 85611, time: 199.88151264190674 s, accu: 0.8676193952560425, loss_yt: 0.2543397545814514\n",
      "epocht 1, batch_num 11800, step 85811, time: 203.32829451560974 s, accu: 0.8676360249519348, loss_yt: 0.2507137656211853\n",
      "epocht 1, batch_num 12000, step 86011, time: 206.8917670249939 s, accu: 0.867655873298645, loss_yt: 0.2581874132156372\n",
      "epocht 1, batch_num 12200, step 86211, time: 210.02738165855408 s, accu: 0.8676716089248657, loss_yt: 0.24186936020851135\n",
      "epocht 1, batch_num 12400, step 86411, time: 213.06728601455688 s, accu: 0.8676900267601013, loss_yt: 0.21233172714710236\n",
      "epocht 1, batch_num 12600, step 86611, time: 216.3594491481781 s, accu: 0.8677133321762085, loss_yt: 0.3142768442630768\n",
      "epocht 1, batch_num 12800, step 86811, time: 219.66862535476685 s, accu: 0.8677482008934021, loss_yt: 0.25394898653030396\n",
      "epocht 1, batch_num 13000, step 87011, time: 222.99271273612976 s, accu: 0.8677870631217957, loss_yt: 0.2608855962753296\n",
      "epocht 1, batch_num 13200, step 87211, time: 226.38663864135742 s, accu: 0.8678147196769714, loss_yt: 0.2857196033000946\n",
      "epocht 1, batch_num 13400, step 87411, time: 229.83641290664673 s, accu: 0.8678545355796814, loss_yt: 0.2186165452003479\n",
      "epocht 1, batch_num 13600, step 87611, time: 233.2462956905365 s, accu: 0.8678990006446838, loss_yt: 0.2460578978061676\n",
      "epocht 1, batch_num 13800, step 87811, time: 236.34800004959106 s, accu: 0.8679317831993103, loss_yt: 0.3685939908027649\n",
      "epocht 1, batch_num 14000, step 88011, time: 239.51951956748962 s, accu: 0.8679642677307129, loss_yt: 0.2356276959180832\n",
      "epocht 1, batch_num 14200, step 88211, time: 242.6591236591339 s, accu: 0.8680126070976257, loss_yt: 0.27552565932273865\n",
      "epocht 1, batch_num 14400, step 88411, time: 245.9403510093689 s, accu: 0.8680531978607178, loss_yt: 0.24200715124607086\n",
      "epocht 1, batch_num 14600, step 88611, time: 249.22057962417603 s, accu: 0.86808842420578, loss_yt: 0.27689215540885925\n",
      "epocht 1, batch_num 14800, step 88811, time: 252.49183201789856 s, accu: 0.8681302070617676, loss_yt: 0.21160469949245453\n",
      "iter_validnum 3701\n",
      "epochv 1, step 88812, stop_n 1, time: 280.10503816604614 s, accu_va: 0.8685268828173773, loss_yv: 0.2657881026427767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 88813, time: 9.505575895309448 s, accu: 0.8689063191413879, loss_yt: 0.21974530816078186\n",
      "epocht 1, batch_num 200, step 89013, time: 12.973304986953735 s, accu: 0.8689393997192383, loss_yt: 0.26399850845336914\n",
      "epocht 1, batch_num 400, step 89213, time: 16.283453464508057 s, accu: 0.8689720034599304, loss_yt: 0.2834821045398712\n",
      "epocht 1, batch_num 600, step 89413, time: 19.373223304748535 s, accu: 0.8690106868743896, loss_yt: 0.2646888196468353\n",
      "epocht 1, batch_num 800, step 89613, time: 22.523805379867554 s, accu: 0.8690456748008728, loss_yt: 0.2991027235984802\n",
      "epocht 1, batch_num 1000, step 89813, time: 25.692320108413696 s, accu: 0.8690794110298157, loss_yt: 0.24480682611465454\n",
      "epocht 1, batch_num 1200, step 90013, time: 29.00842523574829 s, accu: 0.8691179752349854, loss_yt: 0.2688157558441162\n",
      "epocht 1, batch_num 1400, step 90213, time: 32.065285205841064 s, accu: 0.8691517114639282, loss_yt: 0.2132166624069214\n",
      "epocht 1, batch_num 1600, step 90413, time: 35.28364539146423 s, accu: 0.8691843748092651, loss_yt: 0.26064333319664\n",
      "epocht 1, batch_num 1800, step 90613, time: 38.4691264629364 s, accu: 0.8691945672035217, loss_yt: 0.2612537145614624\n",
      "epocht 1, batch_num 2000, step 90813, time: 41.79722785949707 s, accu: 0.8692333102226257, loss_yt: 0.252998948097229\n",
      "epocht 1, batch_num 2200, step 91013, time: 44.955809354782104 s, accu: 0.8692693114280701, loss_yt: 0.3103708326816559\n",
      "epocht 1, batch_num 2400, step 91213, time: 48.17417597770691 s, accu: 0.8693020343780518, loss_yt: 0.25084757804870605\n",
      "epocht 1, batch_num 2600, step 91413, time: 51.35666465759277 s, accu: 0.8693345785140991, loss_yt: 0.3459221422672272\n",
      "epocht 1, batch_num 2800, step 91613, time: 54.50527596473694 s, accu: 0.8693528175354004, loss_yt: 0.24500206112861633\n",
      "epocht 1, batch_num 3000, step 91813, time: 58.1595094203949 s, accu: 0.8693856596946716, loss_yt: 0.2503024935722351\n",
      "epocht 1, batch_num 3200, step 92013, time: 61.215304374694824 s, accu: 0.8694204092025757, loss_yt: 0.24483607709407806\n",
      "epocht 1, batch_num 3400, step 92213, time: 64.31105852127075 s, accu: 0.8694494366645813, loss_yt: 0.23877127468585968\n",
      "epocht 1, batch_num 3600, step 92413, time: 67.57016611099243 s, accu: 0.8694843053817749, loss_yt: 0.31455445289611816\n",
      "epocht 1, batch_num 3800, step 92613, time: 70.77357387542725 s, accu: 0.8695173263549805, loss_yt: 0.2202562689781189\n",
      "epocht 1, batch_num 4000, step 92813, time: 74.34904456138611 s, accu: 0.86955326795578, loss_yt: 0.23254533112049103\n",
      "epocht 1, batch_num 4200, step 93013, time: 77.56441378593445 s, accu: 0.8695911169052124, loss_yt: 0.24882517755031586\n",
      "epocht 1, batch_num 4400, step 93213, time: 80.61226391792297 s, accu: 0.8696203827857971, loss_yt: 0.29503390192985535\n",
      "epocht 1, batch_num 4600, step 93413, time: 83.78378343582153 s, accu: 0.869646430015564, loss_yt: 0.2782970666885376\n",
      "epocht 1, batch_num 4800, step 93613, time: 86.82066226005554 s, accu: 0.8696786165237427, loss_yt: 0.23478324711322784\n",
      "epocht 1, batch_num 5000, step 93813, time: 89.97426342964172 s, accu: 0.869705855846405, loss_yt: 0.2228480875492096\n",
      "epocht 1, batch_num 5200, step 94013, time: 93.16968441009521 s, accu: 0.8697332143783569, loss_yt: 0.2806704342365265\n",
      "epocht 1, batch_num 5400, step 94213, time: 96.21557307243347 s, accu: 0.8697689771652222, loss_yt: 0.3060179352760315\n",
      "epocht 1, batch_num 5600, step 94413, time: 99.37810897827148 s, accu: 0.8697701692581177, loss_yt: 0.2536564767360687\n",
      "epocht 1, batch_num 5800, step 94613, time: 102.56160426139832 s, accu: 0.8697983026504517, loss_yt: 0.37335655093193054\n",
      "epocht 1, batch_num 6000, step 94813, time: 105.80091571807861 s, accu: 0.8698260188102722, loss_yt: 0.34932732582092285\n",
      "epocht 1, batch_num 6200, step 95013, time: 108.92754817008972 s, accu: 0.8698593378067017, loss_yt: 0.2649714946746826\n",
      "epocht 1, batch_num 6400, step 95213, time: 112.17690682411194 s, accu: 0.8698821663856506, loss_yt: 0.3215317130088806\n",
      "epocht 1, batch_num 6600, step 95413, time: 115.31446933746338 s, accu: 0.8699153065681458, loss_yt: 0.28200000524520874\n",
      "epocht 1, batch_num 6800, step 95613, time: 118.3822672367096 s, accu: 0.8699504733085632, loss_yt: 0.27468082308769226\n",
      "epocht 1, batch_num 7000, step 95813, time: 121.5588047504425 s, accu: 0.8699827194213867, loss_yt: 0.27299922704696655\n",
      "epocht 1, batch_num 7200, step 96013, time: 124.6146023273468 s, accu: 0.8700060248374939, loss_yt: 0.24571096897125244\n",
      "epocht 1, batch_num 7400, step 96213, time: 127.94369888305664 s, accu: 0.8700297474861145, loss_yt: 0.23396630585193634\n",
      "epocht 1, batch_num 7600, step 96413, time: 131.14015173912048 s, accu: 0.8700586557388306, loss_yt: 0.2672618329524994\n",
      "epocht 1, batch_num 7800, step 96613, time: 134.30169796943665 s, accu: 0.8700854778289795, loss_yt: 0.2626930773258209\n",
      "epocht 1, batch_num 8000, step 96813, time: 137.4871802330017 s, accu: 0.8701120018959045, loss_yt: 0.28627434372901917\n",
      "epocht 1, batch_num 8200, step 97013, time: 140.42133378982544 s, accu: 0.8701311945915222, loss_yt: 0.2500777244567871\n",
      "epocht 1, batch_num 8400, step 97213, time: 143.6975724697113 s, accu: 0.8701492547988892, loss_yt: 0.31883788108825684\n",
      "epocht 1, batch_num 8600, step 97413, time: 146.74143362045288 s, accu: 0.8701795339584351, loss_yt: 0.28260141611099243\n",
      "epocht 1, batch_num 8800, step 97613, time: 149.91599416732788 s, accu: 0.8701960444450378, loss_yt: 0.2953813672065735\n",
      "epocht 1, batch_num 9000, step 97813, time: 152.97775793075562 s, accu: 0.8702264428138733, loss_yt: 0.2662000358104706\n",
      "epocht 1, batch_num 9200, step 98013, time: 156.2839241027832 s, accu: 0.8702541589736938, loss_yt: 0.3202468156814575\n",
      "epocht 1, batch_num 9400, step 98213, time: 159.46840167045593 s, accu: 0.870271623134613, loss_yt: 0.23670673370361328\n",
      "epocht 1, batch_num 9600, step 98413, time: 162.5192687511444 s, accu: 0.8703041076660156, loss_yt: 0.2743346393108368\n",
      "epocht 1, batch_num 9800, step 98613, time: 165.6688539981842 s, accu: 0.8703176379203796, loss_yt: 0.2698324918746948\n",
      "epocht 1, batch_num 10000, step 98813, time: 168.6379098892212 s, accu: 0.8703383803367615, loss_yt: 0.25752297043800354\n",
      "epocht 1, batch_num 10200, step 99013, time: 171.8882224559784 s, accu: 0.8703620433807373, loss_yt: 0.247599259018898\n",
      "epocht 1, batch_num 10400, step 99213, time: 175.09265422821045 s, accu: 0.8703844547271729, loss_yt: 0.24251475930213928\n",
      "epocht 1, batch_num 10600, step 99413, time: 178.07764506340027 s, accu: 0.8703973889350891, loss_yt: 0.2839221954345703\n",
      "epocht 1, batch_num 10800, step 99613, time: 181.2970564365387 s, accu: 0.8704172372817993, loss_yt: 0.2902225852012634\n",
      "epocht 1, batch_num 11000, step 99813, time: 184.64408111572266 s, accu: 0.8704477548599243, loss_yt: 0.28963762521743774\n",
      "epocht 1, batch_num 11200, step 100013, time: 187.78069472312927 s, accu: 0.8704842925071716, loss_yt: 0.27364563941955566\n",
      "epocht 1, batch_num 11400, step 100213, time: 190.96617531776428 s, accu: 0.8705115914344788, loss_yt: 0.28836753964424133\n",
      "epocht 1, batch_num 11600, step 100413, time: 194.19055366516113 s, accu: 0.8705247640609741, loss_yt: 0.29399728775024414\n",
      "epocht 1, batch_num 11800, step 100613, time: 197.28128838539124 s, accu: 0.8705450892448425, loss_yt: 0.23940688371658325\n",
      "epocht 1, batch_num 12000, step 100813, time: 200.44685626029968 s, accu: 0.8705722689628601, loss_yt: 0.2875637114048004\n",
      "epocht 1, batch_num 12200, step 101013, time: 203.66923451423645 s, accu: 0.8706005215644836, loss_yt: 0.35960260033607483\n",
      "epocht 1, batch_num 12400, step 101213, time: 207.00728154182434 s, accu: 0.8706316351890564, loss_yt: 0.23342175781726837\n",
      "epocht 1, batch_num 12600, step 101413, time: 210.2157015800476 s, accu: 0.8706609010696411, loss_yt: 0.3054839074611664\n",
      "epocht 1, batch_num 12800, step 101613, time: 213.57475328445435 s, accu: 0.870686411857605, loss_yt: 0.31583040952682495\n",
      "epocht 1, batch_num 13000, step 101813, time: 216.75621247291565 s, accu: 0.8707059025764465, loss_yt: 0.26092150807380676\n",
      "epocht 1, batch_num 13200, step 102013, time: 219.7282919883728 s, accu: 0.8707236647605896, loss_yt: 0.2871696949005127\n",
      "epocht 1, batch_num 13400, step 102213, time: 223.04340052604675 s, accu: 0.8707412481307983, loss_yt: 0.2390100657939911\n",
      "epocht 1, batch_num 13600, step 102413, time: 226.2039520740509 s, accu: 0.8707610368728638, loss_yt: 0.254120409488678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 13800, step 102613, time: 229.16306900978088 s, accu: 0.8707882165908813, loss_yt: 0.28867435455322266\n",
      "epocht 1, batch_num 14000, step 102813, time: 232.33854460716248 s, accu: 0.8708097338676453, loss_yt: 0.26806968450546265\n",
      "epocht 1, batch_num 14200, step 103013, time: 235.53103280067444 s, accu: 0.8708351850509644, loss_yt: 0.2924177050590515\n",
      "epocht 1, batch_num 14400, step 103213, time: 238.9139904975891 s, accu: 0.8708558082580566, loss_yt: 0.26534655690193176\n",
      "epocht 1, batch_num 14600, step 103413, time: 241.9977171421051 s, accu: 0.8708789944648743, loss_yt: 0.29542097449302673\n",
      "epocht 1, batch_num 14800, step 103613, time: 245.00467443466187 s, accu: 0.8708940148353577, loss_yt: 0.28411564230918884\n",
      "iter_validnum 3701\n",
      "epochv 1, step 103614, stop_n 2, time: 271.6095678806305 s, accu_va: 0.8711548039185876, loss_yv: 0.26518468712591153\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 103615, time: 9.361968040466309 s, accu: 0.8714136481285095, loss_yt: 0.27158841490745544\n",
      "epocht 1, batch_num 200, step 103815, time: 12.534483432769775 s, accu: 0.8714371919631958, loss_yt: 0.22914351522922516\n",
      "epocht 1, batch_num 400, step 104015, time: 15.792772769927979 s, accu: 0.8714636564254761, loss_yt: 0.2742336094379425\n",
      "epocht 1, batch_num 600, step 104215, time: 19.0670485496521 s, accu: 0.8714919090270996, loss_yt: 0.28133976459503174\n",
      "epocht 1, batch_num 800, step 104415, time: 22.518786191940308 s, accu: 0.8714931607246399, loss_yt: 0.26521095633506775\n",
      "epocht 1, batch_num 1000, step 104615, time: 25.745160341262817 s, accu: 0.871512770652771, loss_yt: 0.2726355195045471\n",
      "epocht 1, batch_num 1200, step 104815, time: 28.82193088531494 s, accu: 0.8715279698371887, loss_yt: 0.2650502622127533\n",
      "epocht 1, batch_num 1400, step 105015, time: 31.98447847366333 s, accu: 0.8715542554855347, loss_yt: 0.2805912494659424\n",
      "epocht 1, batch_num 1600, step 105215, time: 35.258718490600586 s, accu: 0.8715798854827881, loss_yt: 0.31114357709884644\n",
      "epocht 1, batch_num 1800, step 105415, time: 38.6961612701416 s, accu: 0.8716062307357788, loss_yt: 0.3013869822025299\n",
      "epocht 1, batch_num 2000, step 105615, time: 42.008275508880615 s, accu: 0.8716327548027039, loss_yt: 0.21817368268966675\n",
      "epocht 1, batch_num 2200, step 105815, time: 45.32940220832825 s, accu: 0.8716509938240051, loss_yt: 0.311678946018219\n",
      "epocht 1, batch_num 2400, step 106015, time: 48.52285408973694 s, accu: 0.8716656565666199, loss_yt: 0.28429991006851196\n",
      "epocht 1, batch_num 2600, step 106215, time: 51.69337725639343 s, accu: 0.8716704249382019, loss_yt: 0.24700388312339783\n",
      "epocht 1, batch_num 2800, step 106415, time: 55.04345154762268 s, accu: 0.8716802597045898, loss_yt: 0.29351967573165894\n",
      "epocht 1, batch_num 3000, step 106615, time: 58.190003395080566 s, accu: 0.8716936707496643, loss_yt: 0.2911130487918854\n",
      "epocht 1, batch_num 3200, step 106815, time: 61.23289895057678 s, accu: 0.8717125654220581, loss_yt: 0.26912611722946167\n",
      "epocht 1, batch_num 3400, step 107015, time: 64.51110100746155 s, accu: 0.8717324137687683, loss_yt: 0.25697630643844604\n",
      "epocht 1, batch_num 3600, step 107215, time: 67.68361806869507 s, accu: 0.8717584609985352, loss_yt: 0.26266732811927795\n",
      "epocht 1, batch_num 3800, step 107415, time: 71.4595205783844 s, accu: 0.8717849850654602, loss_yt: 0.29207998514175415\n",
      "epocht 1, batch_num 4000, step 107615, time: 74.68592548370361 s, accu: 0.871807336807251, loss_yt: 0.283220499753952\n",
      "epocht 1, batch_num 4200, step 107815, time: 78.03493738174438 s, accu: 0.8718240261077881, loss_yt: 0.28239890933036804\n",
      "epocht 1, batch_num 4400, step 108015, time: 81.20449566841125 s, accu: 0.871846616268158, loss_yt: 0.2631259560585022\n",
      "epocht 1, batch_num 4600, step 108215, time: 84.22342777252197 s, accu: 0.8718701601028442, loss_yt: 0.2711089849472046\n",
      "epocht 1, batch_num 4800, step 108415, time: 87.4637279510498 s, accu: 0.8718921542167664, loss_yt: 0.2832571864128113\n",
      "epocht 1, batch_num 5000, step 108615, time: 90.81476616859436 s, accu: 0.871904194355011, loss_yt: 0.2717572748661041\n",
      "epocht 1, batch_num 5200, step 108815, time: 94.13516807556152 s, accu: 0.8719139099121094, loss_yt: 0.2833314836025238\n",
      "epocht 1, batch_num 5400, step 109015, time: 97.4173891544342 s, accu: 0.8719411492347717, loss_yt: 0.20036621391773224\n",
      "epocht 1, batch_num 5600, step 109215, time: 100.71158027648926 s, accu: 0.8719685077667236, loss_yt: 0.2540964186191559\n",
      "epocht 1, batch_num 5800, step 109415, time: 103.88612532615662 s, accu: 0.871982753276825, loss_yt: 0.28641170263290405\n",
      "epocht 1, batch_num 6000, step 109615, time: 107.04867005348206 s, accu: 0.8719969987869263, loss_yt: 0.31096896529197693\n",
      "epocht 1, batch_num 6200, step 109815, time: 110.13088369369507 s, accu: 0.8720144629478455, loss_yt: 0.24441778659820557\n",
      "epocht 1, batch_num 6400, step 110015, time: 113.41116333007812 s, accu: 0.8720301985740662, loss_yt: 0.2594683766365051\n",
      "epocht 1, batch_num 6600, step 110215, time: 116.70234537124634 s, accu: 0.8720515966415405, loss_yt: 0.3285970091819763\n",
      "epocht 1, batch_num 6800, step 110415, time: 119.83992266654968 s, accu: 0.8720688819885254, loss_yt: 0.3321084976196289\n",
      "epocht 1, batch_num 7000, step 110615, time: 123.4941520690918 s, accu: 0.87208092212677, loss_yt: 0.2782525420188904\n",
      "epocht 1, batch_num 7200, step 110815, time: 126.81829953193665 s, accu: 0.8720993995666504, loss_yt: 0.3191709518432617\n",
      "epocht 1, batch_num 7400, step 111015, time: 130.06857466697693 s, accu: 0.8721229434013367, loss_yt: 0.2890475392341614\n",
      "epocht 1, batch_num 7600, step 111215, time: 133.21515893936157 s, accu: 0.8721457719802856, loss_yt: 0.2776835262775421\n",
      "epocht 1, batch_num 7800, step 111415, time: 136.38568019866943 s, accu: 0.8721659779548645, loss_yt: 0.32493558526039124\n",
      "epocht 1, batch_num 8000, step 111615, time: 139.7257800102234 s, accu: 0.8721924424171448, loss_yt: 0.26392754912376404\n",
      "epocht 1, batch_num 8200, step 111815, time: 142.9701042175293 s, accu: 0.8722075819969177, loss_yt: 0.31905463337898254\n",
      "epocht 1, batch_num 8400, step 112015, time: 146.11170411109924 s, accu: 0.8722262382507324, loss_yt: 0.3237604796886444\n",
      "epocht 1, batch_num 8600, step 112215, time: 149.27922677993774 s, accu: 0.8722426295280457, loss_yt: 0.2629024088382721\n",
      "epocht 1, batch_num 8800, step 112415, time: 152.54048013687134 s, accu: 0.8722671270370483, loss_yt: 0.23950649797916412\n",
      "epocht 1, batch_num 9000, step 112615, time: 156.03913187980652 s, accu: 0.8722876906394958, loss_yt: 0.2824241518974304\n",
      "epocht 1, batch_num 9200, step 112815, time: 159.23757338523865 s, accu: 0.8723134994506836, loss_yt: 0.26270782947540283\n",
      "epocht 1, batch_num 9400, step 113015, time: 162.4948627948761 s, accu: 0.8723347187042236, loss_yt: 0.2948734164237976\n",
      "epocht 1, batch_num 9600, step 113215, time: 165.7132568359375 s, accu: 0.872359037399292, loss_yt: 0.3203255236148834\n",
      "epocht 1, batch_num 9800, step 113415, time: 168.7790858745575 s, accu: 0.8723791837692261, loss_yt: 0.3313860595226288\n",
      "epocht 1, batch_num 10000, step 113615, time: 172.12710452079773 s, accu: 0.8724032640457153, loss_yt: 0.27720174193382263\n",
      "epocht 1, batch_num 10200, step 113815, time: 175.44324159622192 s, accu: 0.8724273443222046, loss_yt: 0.2906118631362915\n",
      "epocht 1, batch_num 10400, step 114015, time: 178.64568305015564 s, accu: 0.8724479675292969, loss_yt: 0.2175491899251938\n",
      "epocht 1, batch_num 10600, step 114215, time: 181.93889737129211 s, accu: 0.872467577457428, loss_yt: 0.2540101408958435\n",
      "epocht 1, batch_num 10800, step 114415, time: 185.05453658103943 s, accu: 0.8724799752235413, loss_yt: 0.2647829055786133\n",
      "epocht 1, batch_num 11000, step 114615, time: 188.6090521812439 s, accu: 0.8724963665008545, loss_yt: 0.2621912360191345\n",
      "epocht 1, batch_num 11200, step 114815, time: 191.7735698223114 s, accu: 0.8725144863128662, loss_yt: 0.25697624683380127\n",
      "epocht 1, batch_num 11400, step 115015, time: 195.1525695323944 s, accu: 0.8725327253341675, loss_yt: 0.3048149645328522\n",
      "epocht 1, batch_num 11600, step 115215, time: 198.38289833068848 s, accu: 0.872555136680603, loss_yt: 0.1820632815361023\n",
      "epocht 1, batch_num 11800, step 115415, time: 201.79078459739685 s, accu: 0.8725795745849609, loss_yt: 0.24274404346942902\n",
      "epocht 1, batch_num 12000, step 115615, time: 205.09096026420593 s, accu: 0.8725939989089966, loss_yt: 0.27936431765556335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 12200, step 115815, time: 208.32753729820251 s, accu: 0.8726115226745605, loss_yt: 0.2765395939350128\n",
      "epocht 1, batch_num 12400, step 116015, time: 211.48008179664612 s, accu: 0.872626543045044, loss_yt: 0.29547202587127686\n",
      "epocht 1, batch_num 12600, step 116215, time: 214.55585741996765 s, accu: 0.8726393580436707, loss_yt: 0.340549111366272\n",
      "epocht 1, batch_num 12800, step 116415, time: 217.76627445220947 s, accu: 0.8726555109024048, loss_yt: 0.30099666118621826\n",
      "epocht 1, batch_num 13000, step 116615, time: 221.0255582332611 s, accu: 0.8726626634597778, loss_yt: 0.26294195652008057\n",
      "epocht 1, batch_num 13200, step 116815, time: 224.41748809814453 s, accu: 0.8726800084114075, loss_yt: 0.2869107127189636\n",
      "epocht 1, batch_num 13400, step 117015, time: 227.66882753372192 s, accu: 0.872697651386261, loss_yt: 0.3098662197589874\n",
      "epocht 1, batch_num 13600, step 117215, time: 230.66581177711487 s, accu: 0.8727190494537354, loss_yt: 0.32089605927467346\n",
      "epocht 1, batch_num 13800, step 117415, time: 233.85026597976685 s, accu: 0.872734010219574, loss_yt: 0.27008765935897827\n",
      "epocht 1, batch_num 14000, step 117615, time: 237.1923267841339 s, accu: 0.8727452158927917, loss_yt: 0.2938354015350342\n",
      "epocht 1, batch_num 14200, step 117815, time: 240.35088109970093 s, accu: 0.8727576732635498, loss_yt: 0.36127811670303345\n",
      "epocht 1, batch_num 14400, step 118015, time: 243.6510841846466 s, accu: 0.8727733492851257, loss_yt: 0.3163270950317383\n",
      "epocht 1, batch_num 14600, step 118215, time: 246.85352444648743 s, accu: 0.8727946877479553, loss_yt: 0.22821319103240967\n",
      "epocht 1, batch_num 14800, step 118415, time: 250.12477707862854 s, accu: 0.8728165030479431, loss_yt: 0.3113187551498413\n",
      "iter_validnum 3701\n",
      "epochv 1, step 118416, stop_n 0, time: 278.0401258468628 s, accu_va: 0.8728677811927971, loss_yv: 0.29065375966547113\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 118417, time: 9.493646383285522 s, accu: 0.8729115724563599, loss_yt: 0.3094824552536011\n",
      "epocht 1, batch_num 200, step 118617, time: 12.958356857299805 s, accu: 0.8729272484779358, loss_yt: 0.22786660492420197\n",
      "epocht 1, batch_num 400, step 118817, time: 16.18472123146057 s, accu: 0.8729400634765625, loss_yt: 0.28135061264038086\n",
      "epocht 1, batch_num 600, step 119017, time: 19.558732271194458 s, accu: 0.8729552626609802, loss_yt: 0.3390783965587616\n",
      "epocht 1, batch_num 800, step 119217, time: 22.807013750076294 s, accu: 0.8729780316352844, loss_yt: 0.2845531702041626\n",
      "epocht 1, batch_num 1000, step 119417, time: 26.02142882347107 s, accu: 0.8729995489120483, loss_yt: 0.2880215346813202\n",
      "epocht 1, batch_num 1200, step 119617, time: 29.404372453689575 s, accu: 0.873027503490448, loss_yt: 0.3441457748413086\n",
      "epocht 1, batch_num 1400, step 119817, time: 32.67665457725525 s, accu: 0.8730530142784119, loss_yt: 0.28141194581985474\n",
      "epocht 1, batch_num 1600, step 120017, time: 35.87909150123596 s, accu: 0.8730703592300415, loss_yt: 0.23483571410179138\n",
      "epocht 1, batch_num 1800, step 120217, time: 39.093464612960815 s, accu: 0.873093843460083, loss_yt: 0.25217270851135254\n",
      "epocht 1, batch_num 2000, step 120417, time: 42.456472873687744 s, accu: 0.8731158971786499, loss_yt: 0.31522834300994873\n",
      "epocht 1, batch_num 2200, step 120617, time: 45.57616114616394 s, accu: 0.873132586479187, loss_yt: 0.23820960521697998\n",
      "epocht 1, batch_num 2400, step 120817, time: 48.68282175064087 s, accu: 0.8731558918952942, loss_yt: 0.25271591544151306\n",
      "epocht 1, batch_num 2600, step 121017, time: 51.81873035430908 s, accu: 0.8731756806373596, loss_yt: 0.2772195339202881\n",
      "epocht 1, batch_num 2800, step 121217, time: 55.16775059700012 s, accu: 0.8731930255889893, loss_yt: 0.24704335629940033\n",
      "epocht 1, batch_num 3000, step 121417, time: 58.428033113479614 s, accu: 0.8732127547264099, loss_yt: 0.25703856348991394\n",
      "epocht 1, batch_num 3200, step 121617, time: 61.95460057258606 s, accu: 0.8732317090034485, loss_yt: 0.23837925493717194\n",
      "epocht 1, batch_num 3400, step 121817, time: 65.21289825439453 s, accu: 0.8732519745826721, loss_yt: 0.2747035026550293\n",
      "epocht 1, batch_num 3600, step 122017, time: 68.49611186981201 s, accu: 0.8732690811157227, loss_yt: 0.2563174068927765\n",
      "epocht 1, batch_num 3800, step 122217, time: 71.8122410774231 s, accu: 0.8732847571372986, loss_yt: 0.2566539943218231\n",
      "epocht 1, batch_num 4000, step 122417, time: 74.94087719917297 s, accu: 0.8732894659042358, loss_yt: 0.2734399139881134\n",
      "epocht 1, batch_num 4200, step 122617, time: 78.14034795761108 s, accu: 0.8733111023902893, loss_yt: 0.23197321593761444\n",
      "epocht 1, batch_num 4400, step 122817, time: 81.37367486953735 s, accu: 0.8733300566673279, loss_yt: 0.32312631607055664\n",
      "epocht 1, batch_num 4600, step 123017, time: 84.55717396736145 s, accu: 0.8733475208282471, loss_yt: 0.25467467308044434\n",
      "epocht 1, batch_num 4800, step 123217, time: 88.06182336807251 s, accu: 0.8733622431755066, loss_yt: 0.28970468044281006\n",
      "epocht 1, batch_num 5000, step 123417, time: 91.33304262161255 s, accu: 0.8733727335929871, loss_yt: 0.2629491686820984\n",
      "epocht 1, batch_num 5200, step 123617, time: 94.61330366134644 s, accu: 0.8733918070793152, loss_yt: 0.24662289023399353\n",
      "epocht 1, batch_num 5400, step 123817, time: 97.7827959060669 s, accu: 0.8734055161476135, loss_yt: 0.30826205015182495\n",
      "epocht 1, batch_num 5600, step 124017, time: 100.89647030830383 s, accu: 0.8734254240989685, loss_yt: 0.21206441521644592\n",
      "epocht 1, batch_num 5800, step 124217, time: 104.18367981910706 s, accu: 0.8734431862831116, loss_yt: 0.2580147981643677\n",
      "epocht 1, batch_num 6000, step 124417, time: 107.341237783432 s, accu: 0.8734627962112427, loss_yt: 0.27104946970939636\n",
      "epocht 1, batch_num 6200, step 124617, time: 110.55464339256287 s, accu: 0.8734806776046753, loss_yt: 0.2920845150947571\n",
      "epocht 1, batch_num 6400, step 124817, time: 113.8239016532898 s, accu: 0.8734834790229797, loss_yt: 0.3297077417373657\n",
      "epocht 1, batch_num 6600, step 125017, time: 117.2517364025116 s, accu: 0.8734741806983948, loss_yt: 0.3497581481933594\n",
      "epocht 1, batch_num 6800, step 125217, time: 120.3425030708313 s, accu: 0.873491108417511, loss_yt: 0.28218400478363037\n",
      "epocht 1, batch_num 7000, step 125417, time: 123.65912675857544 s, accu: 0.8734998106956482, loss_yt: 0.24283212423324585\n",
      "epocht 1, batch_num 7200, step 125617, time: 126.76581120491028 s, accu: 0.8735227584838867, loss_yt: 0.24103686213493347\n",
      "epocht 1, batch_num 7400, step 125817, time: 130.1218340396881 s, accu: 0.8735429644584656, loss_yt: 0.327578604221344\n",
      "epocht 1, batch_num 7600, step 126017, time: 133.34820699691772 s, accu: 0.873563826084137, loss_yt: 0.27802804112434387\n",
      "epocht 1, batch_num 7800, step 126217, time: 136.52770495414734 s, accu: 0.8735860586166382, loss_yt: 0.23068606853485107\n",
      "epocht 1, batch_num 8000, step 126417, time: 139.80095195770264 s, accu: 0.8736001253128052, loss_yt: 0.20553213357925415\n",
      "epocht 1, batch_num 8200, step 126617, time: 143.2218074798584 s, accu: 0.8736230731010437, loss_yt: 0.255979061126709\n",
      "epocht 1, batch_num 8400, step 126817, time: 146.57683396339417 s, accu: 0.8736401796340942, loss_yt: 0.2779616415500641\n",
      "epocht 1, batch_num 8600, step 127017, time: 149.90792655944824 s, accu: 0.8736650943756104, loss_yt: 0.22930853068828583\n",
      "epocht 1, batch_num 8800, step 127217, time: 152.9717333316803 s, accu: 0.8736889958381653, loss_yt: 0.2517298460006714\n",
      "epocht 1, batch_num 9000, step 127417, time: 156.32477378845215 s, accu: 0.873710572719574, loss_yt: 0.28747159242630005\n",
      "epocht 1, batch_num 9200, step 127617, time: 159.42946434020996 s, accu: 0.8737348914146423, loss_yt: 0.2604902982711792\n",
      "epocht 1, batch_num 9400, step 127817, time: 162.665860414505 s, accu: 0.8737342953681946, loss_yt: 0.30262210965156555\n",
      "epocht 1, batch_num 9600, step 128017, time: 166.00388503074646 s, accu: 0.8737285137176514, loss_yt: 0.2905813455581665\n",
      "epocht 1, batch_num 9800, step 128217, time: 169.31802248954773 s, accu: 0.8737476468086243, loss_yt: 0.31051188707351685\n",
      "epocht 1, batch_num 10000, step 128417, time: 172.53741359710693 s, accu: 0.8737484812736511, loss_yt: 0.3217565715312958\n",
      "epocht 1, batch_num 10200, step 128617, time: 175.90939807891846 s, accu: 0.8737485408782959, loss_yt: 0.28097274899482727\n",
      "epocht 1, batch_num 10400, step 128817, time: 179.07094311714172 s, accu: 0.873771071434021, loss_yt: 0.37455061078071594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 10600, step 129017, time: 182.39007711410522 s, accu: 0.8737886548042297, loss_yt: 0.3241809010505676\n",
      "epocht 1, batch_num 10800, step 129217, time: 185.5905418395996 s, accu: 0.8738038539886475, loss_yt: 0.26461392641067505\n",
      "epocht 1, batch_num 11000, step 129417, time: 189.13702917099 s, accu: 0.8738196492195129, loss_yt: 0.327557772397995\n",
      "epocht 1, batch_num 11200, step 129617, time: 192.3544249534607 s, accu: 0.8738354444503784, loss_yt: 0.27171632647514343\n",
      "epocht 1, batch_num 11400, step 129817, time: 195.43521976470947 s, accu: 0.8738511204719543, loss_yt: 0.27836012840270996\n",
      "epocht 1, batch_num 11600, step 130017, time: 198.6286473274231 s, accu: 0.8738694190979004, loss_yt: 0.2313752919435501\n",
      "epocht 1, batch_num 11800, step 130217, time: 201.98869466781616 s, accu: 0.8738833069801331, loss_yt: 0.25407928228378296\n",
      "epocht 1, batch_num 12000, step 130417, time: 205.1961169242859 s, accu: 0.8738982677459717, loss_yt: 0.32672005891799927\n",
      "epocht 1, batch_num 12200, step 130617, time: 208.51321458816528 s, accu: 0.8739113211631775, loss_yt: 0.24762722849845886\n",
      "epocht 1, batch_num 12400, step 130817, time: 211.7196397781372 s, accu: 0.8739305138587952, loss_yt: 0.2545182406902313\n",
      "epocht 1, batch_num 12600, step 131017, time: 215.4337351322174 s, accu: 0.8739476203918457, loss_yt: 0.23449525237083435\n",
      "epocht 1, batch_num 12800, step 131217, time: 218.69498944282532 s, accu: 0.8739625215530396, loss_yt: 0.2963794767856598\n",
      "epocht 1, batch_num 13000, step 131417, time: 221.79772019386292 s, accu: 0.8739747405052185, loss_yt: 0.3112706243991852\n",
      "epocht 1, batch_num 13200, step 131617, time: 225.08988881111145 s, accu: 0.8739907741546631, loss_yt: 0.3133392333984375\n",
      "epocht 1, batch_num 13400, step 131817, time: 228.35119462013245 s, accu: 0.8740111589431763, loss_yt: 0.23521240055561066\n",
      "epocht 1, batch_num 13600, step 132017, time: 231.85778999328613 s, accu: 0.8740243315696716, loss_yt: 0.27560797333717346\n",
      "epocht 1, batch_num 13800, step 132217, time: 235.02534580230713 s, accu: 0.8740299940109253, loss_yt: 0.3148351013660431\n",
      "epocht 1, batch_num 14000, step 132417, time: 238.22978448867798 s, accu: 0.8740290403366089, loss_yt: 0.32872384786605835\n",
      "epocht 1, batch_num 14200, step 132617, time: 241.41822719573975 s, accu: 0.8740196228027344, loss_yt: 0.32387498021125793\n",
      "epocht 1, batch_num 14400, step 132817, time: 244.65560173988342 s, accu: 0.8740257024765015, loss_yt: 0.29849663376808167\n",
      "epocht 1, batch_num 14600, step 133017, time: 247.87296652793884 s, accu: 0.8740411400794983, loss_yt: 0.3523409962654114\n",
      "epocht 1, batch_num 14800, step 133217, time: 251.12028217315674 s, accu: 0.8740580677986145, loss_yt: 0.2769809365272522\n",
      "iter_validnum 3701\n",
      "epochv 1, step 133218, stop_n 0, time: 278.78530502319336 s, accu_va: 0.8742396656934135, loss_yv: 0.26913193336485913\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 133219, time: 10.066056489944458 s, accu: 0.8744193315505981, loss_yt: 0.2578980326652527\n",
      "epocht 1, batch_num 200, step 133419, time: 13.168760776519775 s, accu: 0.8744098544120789, loss_yt: 0.3106192946434021\n",
      "epocht 1, batch_num 400, step 133619, time: 16.81102180480957 s, accu: 0.8743917942047119, loss_yt: 0.26319026947021484\n",
      "epocht 1, batch_num 600, step 133819, time: 20.06731390953064 s, accu: 0.8744125366210938, loss_yt: 0.26657769083976746\n",
      "epocht 1, batch_num 800, step 134019, time: 23.308645248413086 s, accu: 0.8744214177131653, loss_yt: 0.25513243675231934\n",
      "epocht 1, batch_num 1000, step 134219, time: 26.52009630203247 s, accu: 0.8744421601295471, loss_yt: 0.20317314565181732\n",
      "epocht 1, batch_num 1200, step 134419, time: 29.856139421463013 s, accu: 0.8744620680809021, loss_yt: 0.27565446496009827\n",
      "epocht 1, batch_num 1400, step 134619, time: 33.000728368759155 s, accu: 0.8744767904281616, loss_yt: 0.2563888132572174\n",
      "epocht 1, batch_num 1600, step 134819, time: 36.17525124549866 s, accu: 0.8744844198226929, loss_yt: 0.28274303674697876\n",
      "epocht 1, batch_num 1800, step 135019, time: 39.456499099731445 s, accu: 0.8744760155677795, loss_yt: 0.28352221846580505\n",
      "epocht 1, batch_num 2000, step 135219, time: 42.90624260902405 s, accu: 0.8744869828224182, loss_yt: 0.3487539291381836\n",
      "epocht 1, batch_num 2200, step 135419, time: 46.119648933410645 s, accu: 0.8745064735412598, loss_yt: 0.2841147184371948\n",
      "epocht 1, batch_num 2400, step 135619, time: 49.50758910179138 s, accu: 0.8745232224464417, loss_yt: 0.2477225810289383\n",
      "epocht 1, batch_num 2600, step 135819, time: 52.60930132865906 s, accu: 0.8745418190956116, loss_yt: 0.30117881298065186\n",
      "epocht 1, batch_num 2800, step 136019, time: 55.71897912025452 s, accu: 0.8745586276054382, loss_yt: 0.23605524003505707\n",
      "epocht 1, batch_num 3000, step 136219, time: 58.90745425224304 s, accu: 0.8745769262313843, loss_yt: 0.2355349361896515\n",
      "epocht 1, batch_num 3200, step 136419, time: 62.08994436264038 s, accu: 0.8745890259742737, loss_yt: 0.2668899893760681\n",
      "epocht 1, batch_num 3400, step 136619, time: 65.31631588935852 s, accu: 0.8746025562286377, loss_yt: 0.3138906955718994\n",
      "epocht 1, batch_num 3600, step 136819, time: 68.69927334785461 s, accu: 0.8746206760406494, loss_yt: 0.287288635969162\n",
      "epocht 1, batch_num 3800, step 137019, time: 72.0732479095459 s, accu: 0.8746340274810791, loss_yt: 0.2755662798881531\n",
      "epocht 1, batch_num 4000, step 137219, time: 75.6128158569336 s, accu: 0.8746557831764221, loss_yt: 0.26778462529182434\n",
      "epocht 1, batch_num 4200, step 137419, time: 78.99774599075317 s, accu: 0.8746628761291504, loss_yt: 0.2915017306804657\n",
      "epocht 1, batch_num 4400, step 137619, time: 82.49238681793213 s, accu: 0.87466961145401, loss_yt: 0.2676650285720825\n",
      "epocht 1, batch_num 4600, step 137819, time: 85.64894652366638 s, accu: 0.8746811747550964, loss_yt: 0.24208898842334747\n",
      "epocht 1, batch_num 4800, step 138019, time: 88.79852485656738 s, accu: 0.8746972680091858, loss_yt: 0.2560745179653168\n",
      "epocht 1, batch_num 5000, step 138219, time: 92.15754246711731 s, accu: 0.874713122844696, loss_yt: 0.27338194847106934\n",
      "epocht 1, batch_num 5200, step 138419, time: 95.47068333625793 s, accu: 0.8747280240058899, loss_yt: 0.26215696334838867\n",
      "epocht 1, batch_num 5400, step 138619, time: 99.14685297012329 s, accu: 0.8747403025627136, loss_yt: 0.2657153606414795\n",
      "epocht 1, batch_num 5600, step 138819, time: 102.47894310951233 s, accu: 0.8747637271881104, loss_yt: 0.23308540880680084\n",
      "epocht 1, batch_num 5800, step 139019, time: 105.65647983551025 s, accu: 0.8747863173484802, loss_yt: 0.291335791349411\n",
      "epocht 1, batch_num 6000, step 139219, time: 108.9945433139801 s, accu: 0.8748064637184143, loss_yt: 0.24272337555885315\n",
      "epocht 1, batch_num 6200, step 139419, time: 112.31564021110535 s, accu: 0.8748195171356201, loss_yt: 0.26657187938690186\n",
      "epocht 1, batch_num 6400, step 139619, time: 115.78037357330322 s, accu: 0.8748322129249573, loss_yt: 0.30140239000320435\n",
      "epocht 1, batch_num 6600, step 139819, time: 118.93094897270203 s, accu: 0.87484210729599, loss_yt: 0.30699968338012695\n",
      "epocht 1, batch_num 6800, step 140019, time: 122.10847878456116 s, accu: 0.8748540878295898, loss_yt: 0.28043967485427856\n",
      "epocht 1, batch_num 7000, step 140219, time: 125.42757987976074 s, accu: 0.8748667240142822, loss_yt: 0.2992503345012665\n",
      "epocht 1, batch_num 7200, step 140419, time: 128.75571084022522 s, accu: 0.8748810887336731, loss_yt: 0.31628918647766113\n",
      "epocht 1, batch_num 7400, step 140619, time: 131.8583824634552 s, accu: 0.8748925924301147, loss_yt: 0.23622667789459229\n",
      "epocht 1, batch_num 7600, step 140819, time: 135.12667417526245 s, accu: 0.8749078512191772, loss_yt: 0.22728745639324188\n",
      "epocht 1, batch_num 7800, step 141019, time: 138.23932480812073 s, accu: 0.8749123811721802, loss_yt: 0.29971370100975037\n",
      "epocht 1, batch_num 8000, step 141219, time: 141.66915011405945 s, accu: 0.874934196472168, loss_yt: 0.2533089518547058\n",
      "epocht 1, batch_num 8200, step 141419, time: 144.87258100509644 s, accu: 0.874952495098114, loss_yt: 0.24855610728263855\n",
      "epocht 1, batch_num 8400, step 141619, time: 148.0550720691681 s, accu: 0.8749752044677734, loss_yt: 0.2680334448814392\n",
      "epocht 1, batch_num 8600, step 141819, time: 151.3632287979126 s, accu: 0.8749921917915344, loss_yt: 0.24587975442409515\n",
      "epocht 1, batch_num 8800, step 142019, time: 154.57862663269043 s, accu: 0.8750112056732178, loss_yt: 0.2595718204975128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 9000, step 142219, time: 157.81499814987183 s, accu: 0.8750262260437012, loss_yt: 0.28408950567245483\n",
      "epocht 1, batch_num 9200, step 142419, time: 160.77905011177063 s, accu: 0.8750466108322144, loss_yt: 0.3162574768066406\n",
      "epocht 1, batch_num 9400, step 142619, time: 164.2188491821289 s, accu: 0.8750596642494202, loss_yt: 0.23587597906589508\n",
      "epocht 1, batch_num 9600, step 142819, time: 167.43425011634827 s, accu: 0.8750666975975037, loss_yt: 0.3402005434036255\n",
      "epocht 1, batch_num 9800, step 143019, time: 170.77032995224 s, accu: 0.875074565410614, loss_yt: 0.2538970708847046\n",
      "epocht 1, batch_num 10000, step 143219, time: 174.1113986968994 s, accu: 0.8750888109207153, loss_yt: 0.3092503845691681\n",
      "epocht 1, batch_num 10200, step 143419, time: 177.4076144695282 s, accu: 0.8751048445701599, loss_yt: 0.24154463410377502\n",
      "epocht 1, batch_num 10400, step 143619, time: 180.64293003082275 s, accu: 0.8751221299171448, loss_yt: 0.3337242007255554\n",
      "epocht 1, batch_num 10600, step 143819, time: 183.8343963623047 s, accu: 0.8751367330551147, loss_yt: 0.25690510869026184\n",
      "epocht 1, batch_num 10800, step 144019, time: 186.92715859413147 s, accu: 0.8751546740531921, loss_yt: 0.2498365193605423\n",
      "epocht 1, batch_num 11000, step 144219, time: 190.3210792541504 s, accu: 0.8751706480979919, loss_yt: 0.22049933671951294\n",
      "epocht 1, batch_num 11200, step 144419, time: 193.55341053009033 s, accu: 0.8751860857009888, loss_yt: 0.25509515404701233\n",
      "epocht 1, batch_num 11400, step 144619, time: 196.70198798179626 s, accu: 0.8752005100250244, loss_yt: 0.24268871545791626\n",
      "epocht 1, batch_num 11600, step 144819, time: 200.0171239376068 s, accu: 0.8751906752586365, loss_yt: 0.2866981327533722\n",
      "epocht 1, batch_num 11800, step 145019, time: 203.31134676933289 s, accu: 0.8751875758171082, loss_yt: 0.26991015672683716\n",
      "epocht 1, batch_num 12000, step 145219, time: 206.7182056903839 s, accu: 0.8752071261405945, loss_yt: 0.27549195289611816\n",
      "epocht 1, batch_num 12200, step 145419, time: 209.94261002540588 s, accu: 0.8752256631851196, loss_yt: 0.277969628572464\n",
      "epocht 1, batch_num 12400, step 145619, time: 213.0722210407257 s, accu: 0.875236988067627, loss_yt: 0.2751581072807312\n",
      "epocht 1, batch_num 12600, step 145819, time: 216.23774933815002 s, accu: 0.8752517104148865, loss_yt: 0.3035610318183899\n",
      "epocht 1, batch_num 12800, step 146019, time: 219.28061199188232 s, accu: 0.8752695918083191, loss_yt: 0.20720523595809937\n",
      "epocht 1, batch_num 13000, step 146219, time: 222.984708070755 s, accu: 0.8752910494804382, loss_yt: 0.2464977502822876\n",
      "epocht 1, batch_num 13200, step 146419, time: 226.24000334739685 s, accu: 0.8753108382225037, loss_yt: 0.3222440779209137\n",
      "epocht 1, batch_num 13400, step 146619, time: 229.55816268920898 s, accu: 0.875326931476593, loss_yt: 0.25179439783096313\n",
      "epocht 1, batch_num 13600, step 146819, time: 232.91119742393494 s, accu: 0.875342071056366, loss_yt: 0.270203173160553\n",
      "epocht 1, batch_num 13800, step 147019, time: 236.27217602729797 s, accu: 0.8753572106361389, loss_yt: 0.25865626335144043\n",
      "epocht 1, batch_num 14000, step 147219, time: 239.3748815059662 s, accu: 0.8753621578216553, loss_yt: 0.28502973914146423\n",
      "epocht 1, batch_num 14200, step 147419, time: 242.58828783035278 s, accu: 0.8753725290298462, loss_yt: 0.24029354751110077\n",
      "epocht 1, batch_num 14400, step 147619, time: 245.67204213142395 s, accu: 0.875388503074646, loss_yt: 0.2575129568576813\n",
      "epocht 1, batch_num 14600, step 147819, time: 249.12181615829468 s, accu: 0.8754032254219055, loss_yt: 0.2529010474681854\n",
      "epocht 1, batch_num 14800, step 148019, time: 252.49479699134827 s, accu: 0.8754191994667053, loss_yt: 0.2627519369125366\n",
      "iter_validnum 3701\n",
      "epochv 1, step 148020, stop_n 1, time: 280.1877827644348 s, accu_va: 0.8755691163896129, loss_yv: 0.26594313289854404\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 148021, time: 9.275223016738892 s, accu: 0.8757148385047913, loss_yt: 0.2504993975162506\n",
      "epocht 2, batch_num 200, step 148221, time: 13.068058013916016 s, accu: 0.8757269382476807, loss_yt: 0.2488764524459839\n",
      "epocht 2, batch_num 400, step 148421, time: 16.43807888031006 s, accu: 0.875742495059967, loss_yt: 0.30689895153045654\n",
      "epocht 2, batch_num 600, step 148621, time: 19.574663162231445 s, accu: 0.875756561756134, loss_yt: 0.2837482988834381\n",
      "epocht 2, batch_num 800, step 148821, time: 22.898802518844604 s, accu: 0.8757765889167786, loss_yt: 0.2838976979255676\n",
      "epocht 2, batch_num 1000, step 149021, time: 26.148112058639526 s, accu: 0.8757961988449097, loss_yt: 0.24292822182178497\n",
      "epocht 2, batch_num 1200, step 149221, time: 29.23186707496643 s, accu: 0.8758158683776855, loss_yt: 0.2588231861591339\n",
      "epocht 2, batch_num 1400, step 149421, time: 32.50209021568298 s, accu: 0.8758352994918823, loss_yt: 0.2518596649169922\n",
      "epocht 2, batch_num 1600, step 149621, time: 35.66263937950134 s, accu: 0.8758494257926941, loss_yt: 0.2977096736431122\n",
      "epocht 2, batch_num 1800, step 149821, time: 38.78429055213928 s, accu: 0.875866174697876, loss_yt: 0.2678181529045105\n",
      "epocht 2, batch_num 2000, step 150021, time: 42.05853533744812 s, accu: 0.8758782148361206, loss_yt: 0.3187614679336548\n",
      "epocht 2, batch_num 2200, step 150221, time: 45.20711636543274 s, accu: 0.8758980631828308, loss_yt: 0.29401782155036926\n",
      "epocht 2, batch_num 2400, step 150421, time: 48.53125882148743 s, accu: 0.8759192824363708, loss_yt: 0.2295328676700592\n",
      "epocht 2, batch_num 2600, step 150621, time: 51.693803787231445 s, accu: 0.8759413361549377, loss_yt: 0.2911939024925232\n",
      "epocht 2, batch_num 2800, step 150821, time: 55.029882192611694 s, accu: 0.8759551048278809, loss_yt: 0.25414395332336426\n",
      "epocht 2, batch_num 3000, step 151021, time: 58.175437450408936 s, accu: 0.8759703636169434, loss_yt: 0.2769337594509125\n",
      "epocht 2, batch_num 3200, step 151221, time: 61.30809497833252 s, accu: 0.8759905099868774, loss_yt: 0.28431764245033264\n",
      "epocht 2, batch_num 3400, step 151421, time: 64.61624813079834 s, accu: 0.8760085701942444, loss_yt: 0.22793102264404297\n",
      "epocht 2, batch_num 3600, step 151621, time: 67.72191095352173 s, accu: 0.8760254979133606, loss_yt: 0.30034735798835754\n",
      "epocht 2, batch_num 3800, step 151821, time: 71.2913670539856 s, accu: 0.8760331869125366, loss_yt: 0.30981066823005676\n",
      "epocht 2, batch_num 4000, step 152021, time: 74.50480628013611 s, accu: 0.8760537505149841, loss_yt: 0.2603139579296112\n",
      "epocht 2, batch_num 4200, step 152221, time: 77.5665864944458 s, accu: 0.8760669231414795, loss_yt: 0.25065985321998596\n",
      "epocht 2, batch_num 4400, step 152421, time: 80.91167330741882 s, accu: 0.8760817646980286, loss_yt: 0.3046723008155823\n",
      "epocht 2, batch_num 4600, step 152621, time: 84.27464866638184 s, accu: 0.8760940432548523, loss_yt: 0.2146856188774109\n",
      "epocht 2, batch_num 4800, step 152821, time: 87.71946358680725 s, accu: 0.8761025667190552, loss_yt: 0.24819041788578033\n",
      "epocht 2, batch_num 5000, step 153021, time: 90.85607671737671 s, accu: 0.8761095404624939, loss_yt: 0.25048258900642395\n",
      "epocht 2, batch_num 5200, step 153221, time: 94.02660393714905 s, accu: 0.8761199116706848, loss_yt: 0.2098327875137329\n",
      "epocht 2, batch_num 5400, step 153421, time: 97.27890706062317 s, accu: 0.8761293888092041, loss_yt: 0.28795501589775085\n",
      "epocht 2, batch_num 5600, step 153621, time: 100.44940400123596 s, accu: 0.8761423826217651, loss_yt: 0.3405226171016693\n",
      "epocht 2, batch_num 5800, step 153821, time: 104.04181838035583 s, accu: 0.8761546611785889, loss_yt: 0.3165673017501831\n",
      "epocht 2, batch_num 6000, step 154021, time: 107.22428774833679 s, accu: 0.8761677145957947, loss_yt: 0.2382776439189911\n",
      "epocht 2, batch_num 6200, step 154221, time: 110.4087643623352 s, accu: 0.8761781454086304, loss_yt: 0.28029853105545044\n",
      "epocht 2, batch_num 6400, step 154421, time: 113.71195721626282 s, accu: 0.87618488073349, loss_yt: 0.2797463536262512\n",
      "epocht 2, batch_num 6600, step 154621, time: 116.89542031288147 s, accu: 0.8761979341506958, loss_yt: 0.3029238283634186\n",
      "epocht 2, batch_num 6800, step 154821, time: 120.45191168785095 s, accu: 0.8762082457542419, loss_yt: 0.24992012977600098\n",
      "epocht 2, batch_num 7000, step 155021, time: 123.59852027893066 s, accu: 0.8762212991714478, loss_yt: 0.25634095072746277\n",
      "epocht 2, batch_num 7200, step 155221, time: 126.71416521072388 s, accu: 0.8762305974960327, loss_yt: 0.29526984691619873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 7400, step 155421, time: 129.91959381103516 s, accu: 0.876240074634552, loss_yt: 0.2430693805217743\n",
      "epocht 2, batch_num 7600, step 155621, time: 132.99938488006592 s, accu: 0.8762562274932861, loss_yt: 0.28564032912254333\n",
      "epocht 2, batch_num 7800, step 155821, time: 136.36937856674194 s, accu: 0.8762744069099426, loss_yt: 0.2872575521469116\n",
      "epocht 2, batch_num 8000, step 156021, time: 139.63062500953674 s, accu: 0.8762850165367126, loss_yt: 0.2668343782424927\n",
      "epocht 2, batch_num 8200, step 156221, time: 142.8420376777649 s, accu: 0.8763025403022766, loss_yt: 0.2532953917980194\n",
      "epocht 2, batch_num 8400, step 156421, time: 146.32971620559692 s, accu: 0.8763168454170227, loss_yt: 0.2839909493923187\n",
      "epocht 2, batch_num 8600, step 156621, time: 149.58803153038025 s, accu: 0.876333475112915, loss_yt: 0.2591026723384857\n",
      "epocht 2, batch_num 8800, step 156821, time: 153.14751887321472 s, accu: 0.8763490319252014, loss_yt: 0.27529236674308777\n",
      "epocht 2, batch_num 9000, step 157021, time: 156.34094738960266 s, accu: 0.8763673901557922, loss_yt: 0.25734198093414307\n",
      "epocht 2, batch_num 9200, step 157221, time: 159.50647711753845 s, accu: 0.8763861060142517, loss_yt: 0.26756638288497925\n",
      "epocht 2, batch_num 9400, step 157421, time: 162.76078605651855 s, accu: 0.8764026165008545, loss_yt: 0.2950790524482727\n",
      "epocht 2, batch_num 9600, step 157621, time: 166.05097723007202 s, accu: 0.8764204978942871, loss_yt: 0.20925626158714294\n",
      "epocht 2, batch_num 9800, step 157821, time: 169.12276315689087 s, accu: 0.8764402866363525, loss_yt: 0.26230961084365845\n",
      "epocht 2, batch_num 10000, step 158021, time: 172.34616923332214 s, accu: 0.8764591217041016, loss_yt: 0.25896191596984863\n",
      "epocht 2, batch_num 10200, step 158221, time: 175.5944902896881 s, accu: 0.8764702677726746, loss_yt: 0.2450743019580841\n",
      "epocht 2, batch_num 10400, step 158421, time: 178.9475245475769 s, accu: 0.8764904737472534, loss_yt: 0.2678305506706238\n",
      "epocht 2, batch_num 10600, step 158621, time: 182.10804677009583 s, accu: 0.8764888644218445, loss_yt: 0.27849501371383667\n",
      "epocht 2, batch_num 10800, step 158821, time: 185.332417011261 s, accu: 0.8765019774436951, loss_yt: 0.23910783231258392\n",
      "epocht 2, batch_num 11000, step 159021, time: 188.78621459007263 s, accu: 0.8765235543251038, loss_yt: 0.2847212553024292\n",
      "epocht 2, batch_num 11200, step 159221, time: 192.02555298805237 s, accu: 0.8765448927879333, loss_yt: 0.24183717370033264\n",
      "epocht 2, batch_num 11400, step 159421, time: 195.30876660346985 s, accu: 0.8765655755996704, loss_yt: 0.2732475697994232\n",
      "epocht 2, batch_num 11600, step 159621, time: 198.58797240257263 s, accu: 0.876585066318512, loss_yt: 0.23844292759895325\n",
      "epocht 2, batch_num 11800, step 159821, time: 202.20530104637146 s, accu: 0.8766027688980103, loss_yt: 0.3791273832321167\n",
      "epocht 2, batch_num 12000, step 160021, time: 205.4516191482544 s, accu: 0.876624345779419, loss_yt: 0.25040823221206665\n",
      "epocht 2, batch_num 12200, step 160221, time: 208.739825963974 s, accu: 0.8766373991966248, loss_yt: 0.27717599272727966\n",
      "epocht 2, batch_num 12400, step 160421, time: 212.0998432636261 s, accu: 0.8766540884971619, loss_yt: 0.3451511561870575\n",
      "epocht 2, batch_num 12600, step 160621, time: 215.5526089668274 s, accu: 0.8766722679138184, loss_yt: 0.24191319942474365\n",
      "epocht 2, batch_num 12800, step 160821, time: 218.7949414253235 s, accu: 0.8766904473304749, loss_yt: 0.256803959608078\n",
      "epocht 2, batch_num 13000, step 161021, time: 222.00435781478882 s, accu: 0.876702606678009, loss_yt: 0.24806295335292816\n",
      "epocht 2, batch_num 13200, step 161221, time: 225.34045243263245 s, accu: 0.8767235279083252, loss_yt: 0.2579842507839203\n",
      "epocht 2, batch_num 13400, step 161421, time: 228.65257906913757 s, accu: 0.8767431974411011, loss_yt: 0.2807341516017914\n",
      "epocht 2, batch_num 13600, step 161621, time: 231.97170329093933 s, accu: 0.8767592906951904, loss_yt: 0.27431637048721313\n",
      "epocht 2, batch_num 13800, step 161821, time: 235.1642050743103 s, accu: 0.876779317855835, loss_yt: 0.26213338971138\n",
      "epocht 2, batch_num 14000, step 162021, time: 238.27289748191833 s, accu: 0.8767964839935303, loss_yt: 0.29408395290374756\n",
      "epocht 2, batch_num 14200, step 162221, time: 241.61491751670837 s, accu: 0.8768161535263062, loss_yt: 0.22888360917568207\n",
      "epocht 2, batch_num 14400, step 162421, time: 244.74454927444458 s, accu: 0.8768337965011597, loss_yt: 0.2613983750343323\n",
      "epocht 2, batch_num 14600, step 162621, time: 247.9988465309143 s, accu: 0.876847505569458, loss_yt: 0.22858145833015442\n",
      "epocht 2, batch_num 14800, step 162821, time: 251.2860562801361 s, accu: 0.8768601417541504, loss_yt: 0.2650965452194214\n",
      "iter_validnum 3701\n",
      "epochv 2, step 162822, stop_n 2, time: 278.82245564460754 s, accu_va: 0.877035341917325, loss_yv: 0.2564654377900855\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 162823, time: 9.321075439453125 s, accu: 0.8772075176239014, loss_yt: 0.23486223816871643\n",
      "epocht 2, batch_num 200, step 163023, time: 12.456692457199097 s, accu: 0.8772238492965698, loss_yt: 0.19749003648757935\n",
      "epocht 2, batch_num 400, step 163223, time: 15.725948095321655 s, accu: 0.8772386908531189, loss_yt: 0.23599117994308472\n",
      "epocht 2, batch_num 600, step 163423, time: 19.111904859542847 s, accu: 0.8772519826889038, loss_yt: 0.2493087351322174\n",
      "epocht 2, batch_num 800, step 163623, time: 22.35522150993347 s, accu: 0.8772671222686768, loss_yt: 0.2728964686393738\n",
      "epocht 2, batch_num 1000, step 163823, time: 25.64043617248535 s, accu: 0.8772844672203064, loss_yt: 0.23841674625873566\n",
      "epocht 2, batch_num 1200, step 164023, time: 29.182974815368652 s, accu: 0.8773011565208435, loss_yt: 0.2614598274230957\n",
      "epocht 2, batch_num 1400, step 164223, time: 32.56195592880249 s, accu: 0.8773185014724731, loss_yt: 0.30510398745536804\n",
      "epocht 2, batch_num 1600, step 164423, time: 35.66864585876465 s, accu: 0.8773328065872192, loss_yt: 0.29780298471450806\n",
      "epocht 2, batch_num 1800, step 164623, time: 38.759355545043945 s, accu: 0.8773389458656311, loss_yt: 0.2666994035243988\n",
      "epocht 2, batch_num 2000, step 164823, time: 42.086461544036865 s, accu: 0.8773524165153503, loss_yt: 0.2993989586830139\n",
      "epocht 2, batch_num 2200, step 165023, time: 45.31482696533203 s, accu: 0.8773671984672546, loss_yt: 0.2316979318857193\n",
      "epocht 2, batch_num 2400, step 165223, time: 48.560181856155396 s, accu: 0.877383828163147, loss_yt: 0.27879369258880615\n",
      "epocht 2, batch_num 2600, step 165423, time: 51.75562930107117 s, accu: 0.8773995041847229, loss_yt: 0.29805803298950195\n",
      "epocht 2, batch_num 2800, step 165623, time: 55.69307589530945 s, accu: 0.8774145245552063, loss_yt: 0.2508622705936432\n",
      "epocht 2, batch_num 3000, step 165823, time: 58.95136213302612 s, accu: 0.8774271011352539, loss_yt: 0.24917347729206085\n",
      "epocht 2, batch_num 3200, step 166023, time: 62.11094689369202 s, accu: 0.8774407505989075, loss_yt: 0.25472861528396606\n",
      "epocht 2, batch_num 3400, step 166223, time: 65.32432198524475 s, accu: 0.8774545192718506, loss_yt: 0.2997407913208008\n",
      "epocht 2, batch_num 3600, step 166423, time: 68.71927523612976 s, accu: 0.877464234828949, loss_yt: 0.24201735854148865\n",
      "epocht 2, batch_num 3800, step 166623, time: 72.23683667182922 s, accu: 0.8774750232696533, loss_yt: 0.2511993646621704\n",
      "epocht 2, batch_num 4000, step 166823, time: 75.46820306777954 s, accu: 0.8774906992912292, loss_yt: 0.28522706031799316\n",
      "epocht 2, batch_num 4200, step 167023, time: 78.6427390575409 s, accu: 0.8775013089179993, loss_yt: 0.29359257221221924\n",
      "epocht 2, batch_num 4400, step 167223, time: 81.91794967651367 s, accu: 0.8775111436843872, loss_yt: 0.24448347091674805\n",
      "epocht 2, batch_num 4600, step 167423, time: 85.2361090183258 s, accu: 0.8775234818458557, loss_yt: 0.26176023483276367\n",
      "epocht 2, batch_num 4800, step 167623, time: 88.42458271980286 s, accu: 0.8775378465652466, loss_yt: 0.2420460283756256\n",
      "epocht 2, batch_num 5000, step 167823, time: 91.63097643852234 s, accu: 0.8775529265403748, loss_yt: 0.25484371185302734\n",
      "epocht 2, batch_num 5200, step 168023, time: 94.92320489883423 s, accu: 0.8775676488876343, loss_yt: 0.2612985372543335\n",
      "epocht 2, batch_num 5400, step 168223, time: 98.11464047431946 s, accu: 0.8775820732116699, loss_yt: 0.23256047070026398\n",
      "epocht 2, batch_num 5600, step 168423, time: 101.1535131931305 s, accu: 0.8776003122329712, loss_yt: 0.243080735206604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 5800, step 168623, time: 104.30611491203308 s, accu: 0.8776137828826904, loss_yt: 0.25603848695755005\n",
      "epocht 2, batch_num 6000, step 168823, time: 107.44768261909485 s, accu: 0.8776317834854126, loss_yt: 0.2370939999818802\n",
      "epocht 2, batch_num 6200, step 169023, time: 110.84260582923889 s, accu: 0.8776495456695557, loss_yt: 0.24300052225589752\n",
      "epocht 2, batch_num 6400, step 169223, time: 114.28938722610474 s, accu: 0.8776641488075256, loss_yt: 0.24862053990364075\n",
      "epocht 2, batch_num 6600, step 169423, time: 117.42802858352661 s, accu: 0.8776770234107971, loss_yt: 0.27730047702789307\n",
      "epocht 2, batch_num 6800, step 169623, time: 120.856853723526 s, accu: 0.8776891827583313, loss_yt: 0.26290878653526306\n",
      "epocht 2, batch_num 7000, step 169823, time: 124.08120465278625 s, accu: 0.8777051568031311, loss_yt: 0.2599285840988159\n",
      "epocht 2, batch_num 7200, step 170023, time: 127.17096662521362 s, accu: 0.8777178525924683, loss_yt: 0.22684645652770996\n",
      "epocht 2, batch_num 7400, step 170223, time: 130.1958532333374 s, accu: 0.8777305483818054, loss_yt: 0.23816139996051788\n",
      "epocht 2, batch_num 7600, step 170423, time: 133.35341095924377 s, accu: 0.8777439594268799, loss_yt: 0.23646536469459534\n",
      "epocht 2, batch_num 7800, step 170623, time: 136.6715362071991 s, accu: 0.8777546286582947, loss_yt: 0.2486957609653473\n",
      "epocht 2, batch_num 8000, step 170823, time: 139.84308886528015 s, accu: 0.8777675628662109, loss_yt: 0.36403971910476685\n",
      "epocht 2, batch_num 8200, step 171023, time: 143.0564842224121 s, accu: 0.8777794241905212, loss_yt: 0.29753410816192627\n",
      "epocht 2, batch_num 8400, step 171223, time: 146.417475938797 s, accu: 0.8777955770492554, loss_yt: 0.23340541124343872\n",
      "epocht 2, batch_num 8600, step 171423, time: 149.5760371685028 s, accu: 0.8778126835823059, loss_yt: 0.2663721442222595\n",
      "epocht 2, batch_num 8800, step 171623, time: 152.6398367881775 s, accu: 0.8778258562088013, loss_yt: 0.24869504570960999\n",
      "epocht 2, batch_num 9000, step 171823, time: 155.92904353141785 s, accu: 0.8778419494628906, loss_yt: 0.2789509892463684\n",
      "epocht 2, batch_num 9200, step 172023, time: 159.0995807647705 s, accu: 0.8778589367866516, loss_yt: 0.254641056060791\n",
      "epocht 2, batch_num 9400, step 172223, time: 162.23919582366943 s, accu: 0.8778741359710693, loss_yt: 0.27832961082458496\n",
      "epocht 2, batch_num 9600, step 172423, time: 165.44861888885498 s, accu: 0.8778905868530273, loss_yt: 0.23686012625694275\n",
      "epocht 2, batch_num 9800, step 172623, time: 168.74381017684937 s, accu: 0.8779087066650391, loss_yt: 0.22966067492961884\n",
      "epocht 2, batch_num 10000, step 172823, time: 172.082848072052 s, accu: 0.8779268860816956, loss_yt: 0.29361099004745483\n",
      "epocht 2, batch_num 10200, step 173023, time: 175.33315443992615 s, accu: 0.8779428005218506, loss_yt: 0.2849406599998474\n",
      "epocht 2, batch_num 10400, step 173223, time: 178.64932084083557 s, accu: 0.8779594898223877, loss_yt: 0.20264467597007751\n",
      "epocht 2, batch_num 10600, step 173423, time: 181.7869246006012 s, accu: 0.8779756426811218, loss_yt: 0.2708504796028137\n",
      "epocht 2, batch_num 10800, step 173623, time: 185.03525066375732 s, accu: 0.8779906630516052, loss_yt: 0.23766331374645233\n",
      "epocht 2, batch_num 11000, step 173823, time: 188.11298084259033 s, accu: 0.8779991865158081, loss_yt: 0.23991015553474426\n",
      "epocht 2, batch_num 11200, step 174023, time: 191.3214020729065 s, accu: 0.8780155181884766, loss_yt: 0.28951898217201233\n",
      "epocht 2, batch_num 11400, step 174223, time: 195.00654792785645 s, accu: 0.8780343532562256, loss_yt: 0.236436128616333\n",
      "epocht 2, batch_num 11600, step 174423, time: 198.2419285774231 s, accu: 0.8780498504638672, loss_yt: 0.2633131146430969\n",
      "epocht 2, batch_num 11800, step 174623, time: 201.36754035949707 s, accu: 0.8780611753463745, loss_yt: 0.24571719765663147\n",
      "epocht 2, batch_num 12000, step 174823, time: 204.52413129806519 s, accu: 0.8780705332756042, loss_yt: 0.24048647284507751\n",
      "epocht 2, batch_num 12200, step 175023, time: 207.68265199661255 s, accu: 0.8780765533447266, loss_yt: 0.27413636445999146\n",
      "epocht 2, batch_num 12400, step 175223, time: 211.15337228775024 s, accu: 0.8780895471572876, loss_yt: 0.2929847240447998\n",
      "epocht 2, batch_num 12600, step 175423, time: 214.254079580307 s, accu: 0.8781028389930725, loss_yt: 0.25905418395996094\n",
      "epocht 2, batch_num 12800, step 175623, time: 217.33883094787598 s, accu: 0.8781137466430664, loss_yt: 0.2579592168331146\n",
      "epocht 2, batch_num 13000, step 175823, time: 220.6180648803711 s, accu: 0.8781269788742065, loss_yt: 0.25060439109802246\n",
      "epocht 2, batch_num 13200, step 176023, time: 223.93618965148926 s, accu: 0.8781415224075317, loss_yt: 0.20723149180412292\n",
      "epocht 2, batch_num 13400, step 176223, time: 227.26428937911987 s, accu: 0.8781583309173584, loss_yt: 0.2654716670513153\n",
      "epocht 2, batch_num 13600, step 176423, time: 230.54354596138 s, accu: 0.8781729340553284, loss_yt: 0.2880498468875885\n",
      "epocht 2, batch_num 13800, step 176623, time: 233.75892567634583 s, accu: 0.8781893253326416, loss_yt: 0.2735162377357483\n",
      "epocht 2, batch_num 14000, step 176823, time: 236.8476960659027 s, accu: 0.8782020807266235, loss_yt: 0.2313121110200882\n",
      "epocht 2, batch_num 14200, step 177023, time: 239.99126386642456 s, accu: 0.8782173991203308, loss_yt: 0.2719821631908417\n",
      "epocht 2, batch_num 14400, step 177223, time: 243.2455599308014 s, accu: 0.8782305121421814, loss_yt: 0.46833235025405884\n",
      "epocht 2, batch_num 14600, step 177423, time: 246.41308498382568 s, accu: 0.8782415390014648, loss_yt: 0.27334973216056824\n",
      "epocht 2, batch_num 14800, step 177623, time: 249.61951160430908 s, accu: 0.8782551288604736, loss_yt: 0.25308728218078613\n",
      "iter_validnum 3701\n",
      "epochv 2, step 177624, stop_n 0, time: 277.31146264076233 s, accu_va: 0.8783915254013888, loss_yv: 0.25602204730078454\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 177625, time: 9.498591184616089 s, accu: 0.8785284161567688, loss_yt: 0.22070033848285675\n",
      "epocht 2, batch_num 200, step 177825, time: 12.623220443725586 s, accu: 0.8785403966903687, loss_yt: 0.218027725815773\n",
      "epocht 2, batch_num 400, step 178025, time: 15.946355104446411 s, accu: 0.8785529732704163, loss_yt: 0.281874418258667\n",
      "epocht 2, batch_num 600, step 178225, time: 19.05500864982605 s, accu: 0.8785669803619385, loss_yt: 0.21273267269134521\n",
      "epocht 2, batch_num 800, step 178425, time: 22.458940505981445 s, accu: 0.878577709197998, loss_yt: 0.2641471028327942\n",
      "epocht 2, batch_num 1000, step 178625, time: 25.597527027130127 s, accu: 0.8785861730575562, loss_yt: 0.23757675290107727\n",
      "epocht 2, batch_num 1200, step 178825, time: 29.1340594291687 s, accu: 0.878593921661377, loss_yt: 0.27032211422920227\n",
      "epocht 2, batch_num 1400, step 179025, time: 32.44021773338318 s, accu: 0.878604531288147, loss_yt: 0.27901414036750793\n",
      "epocht 2, batch_num 1600, step 179225, time: 35.73441004753113 s, accu: 0.8786167502403259, loss_yt: 0.21417668461799622\n",
      "epocht 2, batch_num 1800, step 179425, time: 38.89794993400574 s, accu: 0.8786216378211975, loss_yt: 0.3018066883087158\n",
      "epocht 2, batch_num 2000, step 179625, time: 42.61401152610779 s, accu: 0.8786276578903198, loss_yt: 0.26231664419174194\n",
      "epocht 2, batch_num 2200, step 179825, time: 45.64091944694519 s, accu: 0.8786349296569824, loss_yt: 0.2495695948600769\n",
      "epocht 2, batch_num 2400, step 180025, time: 49.18249440193176 s, accu: 0.878644585609436, loss_yt: 0.2234278917312622\n",
      "epocht 2, batch_num 2600, step 180225, time: 52.30410051345825 s, accu: 0.8786486983299255, loss_yt: 0.27471306920051575\n",
      "epocht 2, batch_num 2800, step 180425, time: 55.47561979293823 s, accu: 0.8786522150039673, loss_yt: 0.2946920692920685\n",
      "epocht 2, batch_num 3000, step 180625, time: 58.82267904281616 s, accu: 0.8786559104919434, loss_yt: 0.27545252442359924\n",
      "epocht 2, batch_num 3200, step 180825, time: 61.99621772766113 s, accu: 0.8786618709564209, loss_yt: 0.2681676149368286\n",
      "epocht 2, batch_num 3400, step 181025, time: 65.42401719093323 s, accu: 0.878669261932373, loss_yt: 0.2960583567619324\n",
      "epocht 2, batch_num 3600, step 181225, time: 68.69427442550659 s, accu: 0.8786768317222595, loss_yt: 0.3134247064590454\n",
      "epocht 2, batch_num 3800, step 181425, time: 71.76309895515442 s, accu: 0.8786866664886475, loss_yt: 0.23325009644031525\n",
      "epocht 2, batch_num 4000, step 181625, time: 74.92660689353943 s, accu: 0.8786928057670593, loss_yt: 0.25875359773635864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 4200, step 181825, time: 78.24673056602478 s, accu: 0.8786988258361816, loss_yt: 0.2849293351173401\n",
      "epocht 2, batch_num 4400, step 182025, time: 81.72542762756348 s, accu: 0.8787044286727905, loss_yt: 0.2917308211326599\n",
      "epocht 2, batch_num 4600, step 182225, time: 84.9987063407898 s, accu: 0.878704845905304, loss_yt: 0.3006320595741272\n",
      "epocht 2, batch_num 4800, step 182425, time: 88.22704315185547 s, accu: 0.8787080645561218, loss_yt: 0.26243796944618225\n",
      "epocht 2, batch_num 5000, step 182625, time: 91.58406472206116 s, accu: 0.8787160515785217, loss_yt: 0.28280875086784363\n",
      "epocht 2, batch_num 5200, step 182825, time: 94.78553771972656 s, accu: 0.8787283897399902, loss_yt: 0.28291356563568115\n",
      "epocht 2, batch_num 5400, step 183025, time: 98.20037174224854 s, accu: 0.8787387609481812, loss_yt: 0.2894778549671173\n",
      "epocht 2, batch_num 5600, step 183225, time: 101.25623869895935 s, accu: 0.8787485361099243, loss_yt: 0.23957812786102295\n",
      "epocht 2, batch_num 5800, step 183425, time: 104.38383865356445 s, accu: 0.8787574172019958, loss_yt: 0.2669079303741455\n",
      "epocht 2, batch_num 6000, step 183625, time: 107.60026955604553 s, accu: 0.8787670135498047, loss_yt: 0.24327842891216278\n",
      "epocht 2, batch_num 6200, step 183825, time: 111.21556949615479 s, accu: 0.8787750005722046, loss_yt: 0.23210348188877106\n",
      "epocht 2, batch_num 6400, step 184025, time: 114.45790100097656 s, accu: 0.8787877559661865, loss_yt: 0.2447880506515503\n",
      "epocht 2, batch_num 6600, step 184225, time: 117.81492304801941 s, accu: 0.878800630569458, loss_yt: 0.3111625015735626\n",
      "epocht 2, batch_num 6800, step 184425, time: 121.31955218315125 s, accu: 0.8788125514984131, loss_yt: 0.23227006196975708\n",
      "epocht 2, batch_num 7000, step 184625, time: 124.61673498153687 s, accu: 0.8788260817527771, loss_yt: 0.21923202276229858\n",
      "epocht 2, batch_num 7200, step 184825, time: 127.72744488716125 s, accu: 0.8788368105888367, loss_yt: 0.2829793095588684\n",
      "epocht 2, batch_num 7400, step 185025, time: 130.88102436065674 s, accu: 0.8788434267044067, loss_yt: 0.24518369138240814\n",
      "epocht 2, batch_num 7600, step 185225, time: 134.0215880870819 s, accu: 0.8788506388664246, loss_yt: 0.25944775342941284\n",
      "epocht 2, batch_num 7800, step 185425, time: 137.25793194770813 s, accu: 0.8788636922836304, loss_yt: 0.26002737879753113\n",
      "epocht 2, batch_num 8000, step 185625, time: 140.49328064918518 s, accu: 0.8788764476776123, loss_yt: 0.2348504215478897\n",
      "epocht 2, batch_num 8200, step 185825, time: 143.66982007026672 s, accu: 0.8788881301879883, loss_yt: 0.23047295212745667\n",
      "epocht 2, batch_num 8400, step 186025, time: 146.92607927322388 s, accu: 0.8788982033729553, loss_yt: 0.2440441995859146\n",
      "epocht 2, batch_num 8600, step 186225, time: 150.23125171661377 s, accu: 0.8789114952087402, loss_yt: 0.21833299100399017\n",
      "epocht 2, batch_num 8800, step 186425, time: 153.3090114593506 s, accu: 0.8789251446723938, loss_yt: 0.3047640919685364\n",
      "epocht 2, batch_num 9000, step 186625, time: 156.57128882408142 s, accu: 0.878940224647522, loss_yt: 0.2555235028266907\n",
      "epocht 2, batch_num 9200, step 186825, time: 159.72884392738342 s, accu: 0.8789501190185547, loss_yt: 0.22768646478652954\n",
      "epocht 2, batch_num 9400, step 187025, time: 163.05295538902283 s, accu: 0.8789644837379456, loss_yt: 0.27691569924354553\n",
      "epocht 2, batch_num 9600, step 187225, time: 166.32221341133118 s, accu: 0.8789772987365723, loss_yt: 0.22901852428913116\n",
      "epocht 2, batch_num 9800, step 187425, time: 169.6273763179779 s, accu: 0.8789897561073303, loss_yt: 0.30987513065338135\n",
      "epocht 2, batch_num 10000, step 187625, time: 172.89563536643982 s, accu: 0.8790040016174316, loss_yt: 0.2418849617242813\n",
      "epocht 2, batch_num 10200, step 187825, time: 176.25964975357056 s, accu: 0.8790020942687988, loss_yt: 0.3224770426750183\n",
      "epocht 2, batch_num 10400, step 188025, time: 179.36633324623108 s, accu: 0.8790104985237122, loss_yt: 0.2864135503768921\n",
      "epocht 2, batch_num 10600, step 188225, time: 182.86198687553406 s, accu: 0.8790181279182434, loss_yt: 0.31486406922340393\n",
      "epocht 2, batch_num 10800, step 188425, time: 186.07838606834412 s, accu: 0.8790159225463867, loss_yt: 0.29336732625961304\n",
      "epocht 2, batch_num 11000, step 188625, time: 189.3047580718994 s, accu: 0.8790208697319031, loss_yt: 0.278890460729599\n",
      "epocht 2, batch_num 11200, step 188825, time: 192.55007910728455 s, accu: 0.8790348768234253, loss_yt: 0.2711060047149658\n",
      "epocht 2, batch_num 11400, step 189025, time: 195.87721061706543 s, accu: 0.8790506720542908, loss_yt: 0.30807510018348694\n",
      "epocht 2, batch_num 11600, step 189225, time: 199.43370628356934 s, accu: 0.8790662884712219, loss_yt: 0.24139374494552612\n",
      "epocht 2, batch_num 11800, step 189425, time: 202.67201280593872 s, accu: 0.8790737390518188, loss_yt: 0.2688262164592743\n",
      "epocht 2, batch_num 12000, step 189625, time: 205.7468237876892 s, accu: 0.8790724277496338, loss_yt: 0.2697588801383972\n",
      "epocht 2, batch_num 12200, step 189825, time: 208.83752751350403 s, accu: 0.8790834546089172, loss_yt: 0.25424283742904663\n",
      "epocht 2, batch_num 12400, step 190025, time: 212.0968108177185 s, accu: 0.8790836930274963, loss_yt: 0.3155253827571869\n",
      "epocht 2, batch_num 12600, step 190225, time: 215.53462648391724 s, accu: 0.8790883421897888, loss_yt: 0.2959541380405426\n",
      "epocht 2, batch_num 12800, step 190425, time: 218.76697635650635 s, accu: 0.8791007995605469, loss_yt: 0.2732788324356079\n",
      "epocht 2, batch_num 13000, step 190625, time: 221.85870957374573 s, accu: 0.8791141510009766, loss_yt: 0.26119646430015564\n",
      "epocht 2, batch_num 13200, step 190825, time: 225.2666392326355 s, accu: 0.8791284561157227, loss_yt: 0.29282552003860474\n",
      "epocht 2, batch_num 13400, step 191025, time: 228.44310212135315 s, accu: 0.8791366815567017, loss_yt: 0.25306352972984314\n",
      "epocht 2, batch_num 13600, step 191225, time: 231.53884840011597 s, accu: 0.8791495561599731, loss_yt: 0.24760586023330688\n",
      "epocht 2, batch_num 13800, step 191425, time: 234.5388011932373 s, accu: 0.8791585564613342, loss_yt: 0.1939665675163269\n",
      "epocht 2, batch_num 14000, step 191625, time: 237.7362620830536 s, accu: 0.8791685700416565, loss_yt: 0.31932830810546875\n",
      "epocht 2, batch_num 14200, step 191825, time: 240.9267225265503 s, accu: 0.8791775107383728, loss_yt: 0.2268623411655426\n",
      "epocht 2, batch_num 14400, step 192025, time: 243.90776801109314 s, accu: 0.8791858553886414, loss_yt: 0.32947421073913574\n",
      "epocht 2, batch_num 14600, step 192225, time: 247.109219789505 s, accu: 0.879197359085083, loss_yt: 0.29657888412475586\n",
      "epocht 2, batch_num 14800, step 192425, time: 250.47119688987732 s, accu: 0.8792094588279724, loss_yt: 0.23806634545326233\n",
      "iter_validnum 3701\n",
      "epochv 2, step 192426, stop_n 0, time: 278.09932017326355 s, accu_va: 0.8793098913338081, loss_yv: 0.26132273522221505\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 192427, time: 9.510571479797363 s, accu: 0.8794100284576416, loss_yt: 0.28125083446502686\n",
      "epocht 2, batch_num 200, step 192627, time: 12.776836633682251 s, accu: 0.8794174194335938, loss_yt: 0.20899252593517303\n",
      "epocht 2, batch_num 400, step 192827, time: 15.947357416152954 s, accu: 0.8794271945953369, loss_yt: 0.2448219507932663\n",
      "epocht 2, batch_num 600, step 193027, time: 19.160765171051025 s, accu: 0.8794364333152771, loss_yt: 0.2437630593776703\n",
      "epocht 2, batch_num 800, step 193227, time: 22.209643602371216 s, accu: 0.8794428706169128, loss_yt: 0.24598392844200134\n",
      "epocht 2, batch_num 1000, step 193427, time: 25.542726755142212 s, accu: 0.8794539570808411, loss_yt: 0.2776103913784027\n",
      "epocht 2, batch_num 1200, step 193627, time: 28.73220419883728 s, accu: 0.8794650435447693, loss_yt: 0.2038211077451706\n",
      "epocht 2, batch_num 1400, step 193827, time: 31.89870309829712 s, accu: 0.8794763684272766, loss_yt: 0.22977161407470703\n",
      "epocht 2, batch_num 1600, step 194027, time: 35.486111640930176 s, accu: 0.8794875144958496, loss_yt: 0.2786538898944855\n",
      "epocht 2, batch_num 1800, step 194227, time: 38.75138711929321 s, accu: 0.8795005679130554, loss_yt: 0.24319081008434296\n",
      "epocht 2, batch_num 2000, step 194427, time: 41.901978731155396 s, accu: 0.8795149326324463, loss_yt: 0.22141104936599731\n",
      "epocht 2, batch_num 2200, step 194627, time: 45.18218183517456 s, accu: 0.8795267343521118, loss_yt: 0.3308810889720917\n",
      "epocht 2, batch_num 2400, step 194827, time: 48.28688073158264 s, accu: 0.8795355558395386, loss_yt: 0.29385313391685486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 2600, step 195027, time: 51.499322175979614 s, accu: 0.8795456886291504, loss_yt: 0.26595786213874817\n",
      "epocht 2, batch_num 2800, step 195227, time: 55.19843077659607 s, accu: 0.8795549273490906, loss_yt: 0.24880680441856384\n",
      "epocht 2, batch_num 3000, step 195427, time: 58.200403690338135 s, accu: 0.8795626759529114, loss_yt: 0.2658672332763672\n",
      "epocht 2, batch_num 3200, step 195627, time: 61.41477656364441 s, accu: 0.8795709609985352, loss_yt: 0.21942125260829926\n",
      "epocht 2, batch_num 3400, step 195827, time: 64.69102025032043 s, accu: 0.879571795463562, loss_yt: 0.2737627625465393\n",
      "epocht 2, batch_num 3600, step 196027, time: 67.79674339294434 s, accu: 0.8795732855796814, loss_yt: 0.2874433398246765\n",
      "epocht 2, batch_num 3800, step 196227, time: 71.14977812767029 s, accu: 0.8795819282531738, loss_yt: 0.3144981861114502\n",
      "epocht 2, batch_num 4000, step 196427, time: 74.28436875343323 s, accu: 0.8795874714851379, loss_yt: 0.2875139117240906\n",
      "epocht 2, batch_num 4200, step 196627, time: 77.51474070549011 s, accu: 0.8795995712280273, loss_yt: 0.293987512588501\n",
      "epocht 2, batch_num 4400, step 196827, time: 80.70918321609497 s, accu: 0.8796097040176392, loss_yt: 0.2649085223674774\n",
      "epocht 2, batch_num 4600, step 197027, time: 83.77697944641113 s, accu: 0.879621684551239, loss_yt: 0.28361737728118896\n",
      "epocht 2, batch_num 4800, step 197227, time: 87.01435589790344 s, accu: 0.8796336054801941, loss_yt: 0.2767919600009918\n",
      "epocht 2, batch_num 5000, step 197427, time: 90.11706256866455 s, accu: 0.8796465992927551, loss_yt: 0.23032721877098083\n",
      "epocht 2, batch_num 5200, step 197627, time: 93.19283413887024 s, accu: 0.8796553015708923, loss_yt: 0.26288455724716187\n",
      "epocht 2, batch_num 5400, step 197827, time: 96.32545280456543 s, accu: 0.8796696066856384, loss_yt: 0.28872257471084595\n",
      "epocht 2, batch_num 5600, step 198027, time: 99.94777584075928 s, accu: 0.8796840906143188, loss_yt: 0.239785298705101\n",
      "epocht 2, batch_num 5800, step 198227, time: 103.08734250068665 s, accu: 0.8796951770782471, loss_yt: 0.25376754999160767\n",
      "epocht 2, batch_num 6000, step 198427, time: 106.32670783996582 s, accu: 0.8797040581703186, loss_yt: 0.22076024115085602\n",
      "epocht 2, batch_num 6200, step 198627, time: 109.58397030830383 s, accu: 0.8797107934951782, loss_yt: 0.2557981610298157\n",
      "epocht 2, batch_num 6400, step 198827, time: 112.64279055595398 s, accu: 0.879719614982605, loss_yt: 0.263499915599823\n",
      "epocht 2, batch_num 6600, step 199027, time: 116.18033337593079 s, accu: 0.879731297492981, loss_yt: 0.24478629231452942\n",
      "epocht 2, batch_num 6800, step 199227, time: 119.31694483757019 s, accu: 0.8797417879104614, loss_yt: 0.2748296856880188\n",
      "epocht 2, batch_num 7000, step 199427, time: 122.37376999855042 s, accu: 0.8797521591186523, loss_yt: 0.27199000120162964\n",
      "epocht 2, batch_num 7200, step 199627, time: 125.52437782287598 s, accu: 0.8797613978385925, loss_yt: 0.2740771472454071\n",
      "epocht 2, batch_num 7400, step 199827, time: 128.58975386619568 s, accu: 0.8797692656517029, loss_yt: 0.2606469988822937\n",
      "epocht 2, batch_num 7600, step 200027, time: 132.1302788257599 s, accu: 0.879781186580658, loss_yt: 0.2428789734840393\n",
      "epocht 2, batch_num 7800, step 200227, time: 135.43342208862305 s, accu: 0.8797820210456848, loss_yt: 0.30981460213661194\n",
      "epocht 2, batch_num 8000, step 200427, time: 138.4882686138153 s, accu: 0.8797923922538757, loss_yt: 0.3128202557563782\n",
      "epocht 2, batch_num 8200, step 200627, time: 141.69169878959656 s, accu: 0.8798014521598816, loss_yt: 0.3347250819206238\n",
      "epocht 2, batch_num 8400, step 200827, time: 144.95596027374268 s, accu: 0.8798109292984009, loss_yt: 0.2730639576911926\n",
      "epocht 2, batch_num 8600, step 201027, time: 148.13947176933289 s, accu: 0.8798201084136963, loss_yt: 0.2861250042915344\n",
      "epocht 2, batch_num 8800, step 201227, time: 151.29503846168518 s, accu: 0.8798304200172424, loss_yt: 0.2202126383781433\n",
      "epocht 2, batch_num 9000, step 201427, time: 154.38773655891418 s, accu: 0.8798414468765259, loss_yt: 0.2672065496444702\n",
      "epocht 2, batch_num 9200, step 201627, time: 157.57625651359558 s, accu: 0.8798543810844421, loss_yt: 0.24299433827400208\n",
      "epocht 2, batch_num 9400, step 201827, time: 160.91628289222717 s, accu: 0.879866898059845, loss_yt: 0.24693109095096588\n",
      "epocht 2, batch_num 9600, step 202027, time: 164.12669372558594 s, accu: 0.8798766136169434, loss_yt: 0.32830506563186646\n",
      "epocht 2, batch_num 9800, step 202227, time: 167.27826738357544 s, accu: 0.8798902630805969, loss_yt: 0.2616901695728302\n",
      "epocht 2, batch_num 10000, step 202427, time: 170.9903666973114 s, accu: 0.8799000978469849, loss_yt: 0.24915283918380737\n",
      "epocht 2, batch_num 10200, step 202627, time: 174.25261735916138 s, accu: 0.8799011707305908, loss_yt: 0.28504154086112976\n",
      "epocht 2, batch_num 10400, step 202827, time: 177.4850001335144 s, accu: 0.8799107074737549, loss_yt: 0.2854245901107788\n",
      "epocht 2, batch_num 10600, step 203027, time: 180.5717225074768 s, accu: 0.8799231648445129, loss_yt: 0.2667941451072693\n",
      "epocht 2, batch_num 10800, step 203227, time: 183.5786793231964 s, accu: 0.8799372315406799, loss_yt: 0.22083960473537445\n",
      "epocht 2, batch_num 11000, step 203427, time: 186.73626947402954 s, accu: 0.8799507021903992, loss_yt: 0.2697972357273102\n",
      "epocht 2, batch_num 11200, step 203627, time: 190.0054931640625 s, accu: 0.8799635767936707, loss_yt: 0.2500329315662384\n",
      "epocht 2, batch_num 11400, step 203827, time: 193.13213300704956 s, accu: 0.8799768686294556, loss_yt: 0.49604594707489014\n",
      "epocht 2, batch_num 11600, step 204027, time: 196.4382939338684 s, accu: 0.879986584186554, loss_yt: 0.27850794792175293\n"
     ]
    }
   ],
   "source": [
    "# batch_size, num_epochs = 4096, 1000\n",
    "batch_size, num_epochs = 512, 1000\n",
    "print(trainpd.head())\n",
    "globalstep = modelcrnn.batch_train(trainpd, labels, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelcrnn.predict(testpd[feature])\n",
    "y_pred = np.squeeze(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#阈值大概在0.2-0.4之间 本题对召回率较敏感，可适当降低一下阈值\n",
    "thre = 0.5\n",
    "#生成提交文件\n",
    "sub = pd.DataFrame()\n",
    "sub['hit_id'] = testpd['hit_id']\n",
    "sub['flag_pred'] = y_pred\n",
    "sub['event_id'] = testpd['event_id']\n",
    "sub['flag_pred'] = sub['flag_pred'].apply(lambda x: 1 if x >= thre else 0)\n",
    "sub.to_csv(os.path.join(pathf, \"subsample.csv\").format(sub['flag_pred'].mean()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}