{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from scipy.stats import entropy, kurtosis\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x      y  z        t   terror        q  flag  event_id  hit_id\n",
      "0 -142.5 -147.5  0  767.879  2.02966  1.05052     0         7       1\n",
      "(9473201, 9)\n",
      "   event_id  nhit  nhitreal  energymc  thetamc    phimc   xcmc    ycmc\n",
      "0         7   426        70   48348.9  63.1686  11.0982 -40.83  114.03\n",
      "(13315, 8)\n",
      "       x      y  z        t  terror        q  event_id  hit_id\n",
      "0 -142.5 -127.5  0  848.061  1.9984  1.15067         9       1\n",
      "(4086511, 8)\n"
     ]
    }
   ],
   "source": [
    "pathf = os.path.join(\"..\", \"data\", \"particles\")\n",
    "model_path = os.path.join(pathf, \"model\")\n",
    "log_path = os.path.join(pathf, \"model\")\n",
    "trainpd = pd.read_csv(os.path.join(pathf, \"train.csv\"))\n",
    "print(trainpd.head(1))\n",
    "trainshape = trainpd.shape\n",
    "print(trainshape)\n",
    "eventpd = pd.read_csv(os.path.join(pathf, \"event.csv\"))\n",
    "print(eventpd.head(1))\n",
    "print(eventpd.shape)\n",
    "testpd = pd.read_csv(os.path.join(pathf, \"test.csv\"))\n",
    "testshape = testpd.shape\n",
    "print(testpd.head(1))\n",
    "print(testpd.shape)\n",
    "\n",
    "data = pd.concat([trainpd, testpd], ignore_index=True)\n",
    "data = pd.merge(data, eventpd, on='event_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (k(q,mc)*(t0+L))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+L) = (t+L)^2\n",
    "# t0 方程 \n",
    "# a = k(q,mc)^2\n",
    "# b = 2*L*k(q,mc)^2 -dis*cos(phi)*sin(thmc)\n",
    "# c = L^2 * k(q,mc)^2 + dis^2 - dis*cos(phi)*sin(thmc)*L - (t+L)^2 \n",
    "# t0 = (-b +- (b^2 - 4*a*c)^(1/2))/2*a\n",
    "data = pd.concat([trainpd, testpd], ignore_index=True)\n",
    "data = pd.merge(data, eventpd, on='event_id', how='left')\n",
    "\n",
    "data['fx'] = data['x'] - data['xcmc']\n",
    "data['fy'] = data['y'] - data['ycmc']\n",
    "data['phimc'] = data['phimc'] * np.pi / 180.\n",
    "data['fphi'] = np.arctan2(data['fy'], data['fx']) - data['phimc']\n",
    "data['fdis'] = np.sqrt(data['fx'] ** 2 + data['fy'] ** 2)\n",
    "data['thetamc'] = data['thetamc'] * np.pi / 180.\n",
    "\n",
    "data['fsinthmc'] = np.sin(data['thetamc'])\n",
    "data['fsinthmc_v'] = 1.0/data['fsinthmc']\n",
    "data['fcosphi'] = np.cos(data['fphi'])\n",
    "data['fcosphi_v'] = 1.0/data['fcosphi']\n",
    "\n",
    "data['fcosthmc'] = np.cos(data['thetamc'])\n",
    "data['fcosthmc_v'] = 1.0/data['fcosthmc']\n",
    "data['fsinphi'] = np.sin(data['fphi'])\n",
    "data['fsinphi_v'] = 1.0/data['fsinphi']\n",
    "\n",
    "data['ftanphi'] = np.tan(data['fphi'])\n",
    "data['ftanphi_v'] = 1.0/data['ftanphi']\n",
    "data['ftanthmc'] = np.tan(data['thetamc'])\n",
    "data['ftanthmc_v'] = 1.0/data['ftanthmc']\n",
    "\n",
    "\n",
    "# data['ft2'] = data['t'] ** 2\n",
    "# data['fdis2'] = data['fdis'] ** 2\n",
    "\n",
    "data['fttrue'] = data['t'] / data['terror']\n",
    "data['fttrue_v'] = 1.0 / data['fttrue']\n",
    "data['fttrue2'] = data['fttrue'] ** 2\n",
    "data['fttrue2_v'] = 1.0 / data['fttrue2'] \n",
    "data['nhitratio'] = data['nhit'] / data['nhitreal']\n",
    "data['nhitratio_v'] = data['nhitratio']\n",
    "data['energymc_v'] = 1.0 / data['energymc']\n",
    "data['fenergymc2'] = data['energymc'] ** 2\n",
    "data['fenergymc2_v'] = 1.0 / data['fenergymc2'] \n",
    "data['q_v'] = 1.0 / data['q']\n",
    "data['q2'] = data['q']\n",
    "data['q2_v'] = 1.0 / data['q2']\n",
    "\n",
    "del data['fx']\n",
    "del data['fy']\n",
    "del data['x']\n",
    "del data['y']\n",
    "del data['z']\n",
    "del data['xcmc']\n",
    "del data['ycmc']\n",
    "del data['fphi']\n",
    "del data['phimc']\n",
    "del data['fsinphi']\n",
    "del data['nhitreal']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_new = pd.DataFrame()\n",
    "info_new[\"event_id\"] = data.groupby([\"event_id\"])[\"event_id\"].mean()\n",
    "info_new[\"fdis_mean\"] = data.groupby([\"event_id\"])[\"fdis\"].mean()\n",
    "info_new[\"fdis_std\"] = data.groupby([\"event_id\"])[\"fdis\"].std()\n",
    "info_new[\"fdis_stdmean\"] = info_new[\"fdis_std\"] / info_new[\"fdis_mean\"]\n",
    "info_new[\"ft_min\"] = data.groupby([\"event_id\"])[\"t\"].min()\n",
    "info_new[\"ft_max\"] = data.groupby([\"event_id\"])[\"t\"].max()\n",
    "info_new[\"t_mean\"] = data.groupby([\"event_id\"])[\"t\"].mean()\n",
    "info_new[\"ft_std\"] = data.groupby([\"event_id\"])[\"t\"].std()\n",
    "info_new[\"ft_stdmean\"] = info_new[\"ft_std\"] / info_new[\"t_mean\"]\n",
    "info_new[\"ft_mean\"] = (info_new['t_mean']-info_new['ft_min']) / (info_new['ft_max']-info_new['ft_min'])\n",
    "info_new.reset_index(drop=True, inplace=True)\n",
    "data = pd.merge(data, info_new, on='event_id', how='left')\n",
    "\n",
    "data['ft_rel'] = data['t'] / data['ft_std']\n",
    "data['ft2_rel'] = data['ft_rel'] ** 2\n",
    "data['ft_rel_v'] = 1.0 / data['ft_rel']\n",
    "data['ft2_rel_v'] = 1.0 / data['ft2_rel'] \n",
    "\n",
    "# (k(q,mc)*(t0+L))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+L) = (t+L)^2\n",
    "data = data.sort_values(by=['event_id', 'ft_rel']).reset_index(drop=True)\n",
    "for i in [1, 5, 7, 11]:\n",
    "    data[f'ft_{i}diff'] = data.groupby('event_id')['ft_rel'].diff(periods=i).fillna(0)\n",
    "del data['t']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9473201\n",
      "(13559712, 49)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainshape[0])\n",
    "print(data.shape)\n",
    "testpd = data[data.flag.isna()].reset_index()\n",
    "trainpd = data[data.flag.notna()].reset_index()\n",
    "trainpd['flag'] = trainpd['flag'].astype('int')\n",
    "# trainpd = data[:trainshape[0]].reset_index()\n",
    "# testpd = data[trainshape[0]:].reset_index()\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'terror', 'q', 'flag', 'event_id', 'hit_id', 'nhit',\n",
      "       'energymc', 'thetamc', 'fdis', 'fsinthmc', 'fsinthmc_v', 'fcosphi',\n",
      "       'fcosphi_v', 'fcosthmc', 'fcosthmc_v', 'fsinphi_v', 'ftanphi',\n",
      "       'ftanphi_v', 'ftanthmc', 'ftanthmc_v', 'fttrue', 'fttrue_v', 'fttrue2',\n",
      "       'fttrue2_v', 'nhitratio', 'nhitratio_v', 'energymc_v', 'fenergymc2',\n",
      "       'fenergymc2_v', 'q_v', 'q2', 'q2_v', 'fdis_mean', 'fdis_std',\n",
      "       'fdis_stdmean', 'ft_min', 'ft_max', 't_mean', 'ft_std', 'ft_stdmean',\n",
      "       'ft_mean', 'ft_rel', 'ft2_rel', 'ft_rel_v', 'ft2_rel_v', 'ft_1diff',\n",
      "       'ft_5diff', 'ft_7diff', 'ft_11diff'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(trainpd.columns)\n",
    "feature = [x for x in trainpd.columns if x not in ['flag', 'index', 'hit_id', 'event_id']]\n",
    "labels = trainpd['flag']\n",
    "del trainpd['flag']\n",
    "del testpd['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index   terror         q  event_id  hit_id  nhit  energymc  thetamc  \\\n",
      "0      0  2.02966  1.450270         7     219   426   48348.9   1.1025   \n",
      "1      1  1.68831  3.100560         7     138   426   48348.9   1.1025   \n",
      "2      2  2.02966  1.098740         7     198   426   48348.9   1.1025   \n",
      "3      3  2.02966  0.799937         7      87   426   48348.9   1.1025   \n",
      "4      4  2.02966  0.748987         7      98   426   48348.9   1.1025   \n",
      "\n",
      "         fdis  fsinthmc  fsinthmc_v   fcosphi  fcosphi_v  fcosthmc  \\\n",
      "0   34.491010  0.892339    1.120651  0.423789   2.359667  0.451367   \n",
      "1  105.107230  0.892339    1.120651 -0.650026  -1.538400  0.451367   \n",
      "2  105.408395  0.892339    1.120651  0.078329  12.766613  0.451367   \n",
      "3  196.706456  0.892339    1.120651 -0.150763  -6.632925  0.451367   \n",
      "4  126.053282  0.892339    1.120651 -0.814480  -1.227777  0.451367   \n",
      "\n",
      "   fcosthmc_v  fsinphi_v    ftanphi  ftanphi_v  ftanthmc  ftanthmc_v  \\\n",
      "0    2.215494   1.104044   2.137295   0.467881  1.976971    0.505824   \n",
      "1    2.215494  -1.315942   1.169049   0.855396  1.976971    0.505824   \n",
      "2    2.215494  -1.003082 -12.727388  -0.078571  1.976971    0.505824   \n",
      "3    2.215494  -1.011562   6.557110   0.152506  1.976971    0.505824   \n",
      "4    2.215494  -1.723568   0.712346   1.403812  1.976971    0.505824   \n",
      "\n",
      "       fttrue  fttrue_v        fttrue2  fttrue2_v  nhitratio  nhitratio_v  \\\n",
      "0 -509.543470 -0.001963  259634.548161   0.000004   6.085714     6.085714   \n",
      "1 -612.399382 -0.001633  375033.002621   0.000003   6.085714     6.085714   \n",
      "2 -506.671068 -0.001974  256715.571210   0.000004   6.085714     6.085714   \n",
      "3 -506.276913 -0.001975  256316.313016   0.000004   6.085714     6.085714   \n",
      "4 -503.025137 -0.001988  253034.288670   0.000004   6.085714     6.085714   \n",
      "\n",
      "   energymc_v    fenergymc2  fenergymc2_v       q_v        q2      q2_v  \\\n",
      "0    0.000021  2.337616e+09  4.277862e-10  0.689527  1.450270  0.689527   \n",
      "1    0.000021  2.337616e+09  4.277862e-10  0.322522  3.100560  0.322522   \n",
      "2    0.000021  2.337616e+09  4.277862e-10  0.910133  1.098740  0.910133   \n",
      "3    0.000021  2.337616e+09  4.277862e-10  1.250098  0.799937  1.250098   \n",
      "4    0.000021  2.337616e+09  4.277862e-10  1.335137  0.748987  1.335137   \n",
      "\n",
      "    fdis_mean   fdis_std  fdis_stdmean  ft_min  ft_max     t_mean      ft_std  \\\n",
      "0  151.290386  82.042181      0.542283 -1034.2  949.79 -78.413058  527.060816   \n",
      "1  151.290386  82.042181      0.542283 -1034.2  949.79 -78.413058  527.060816   \n",
      "2  151.290386  82.042181      0.542283 -1034.2  949.79 -78.413058  527.060816   \n",
      "3  151.290386  82.042181      0.542283 -1034.2  949.79 -78.413058  527.060816   \n",
      "4  151.290386  82.042181      0.542283 -1034.2  949.79 -78.413058  527.060816   \n",
      "\n",
      "   ft_stdmean  ft_mean    ft_rel   ft2_rel  ft_rel_v  ft2_rel_v  ft_1diff  \\\n",
      "0   -6.721595  0.48175 -1.962202  3.850238 -0.509631   0.259724  0.000000   \n",
      "1   -6.721595  0.48175 -1.961671  3.848154 -0.509769   0.259865  0.000531   \n",
      "2   -6.721595  0.48175 -1.951141  3.806951 -0.512521   0.262677  0.010530   \n",
      "3   -6.721595  0.48175 -1.949623  3.801031 -0.512920   0.263087  0.001518   \n",
      "4   -6.721595  0.48175 -1.937101  3.752360 -0.516235   0.266499  0.012522   \n",
      "\n",
      "   ft_5diff  ft_7diff  ft_11diff  \n",
      "0       0.0       0.0        0.0  \n",
      "1       0.0       0.0        0.0  \n",
      "2       0.0       0.0        0.0  \n",
      "3       0.0       0.0        0.0  \n",
      "4       0.0       0.0        0.0  \n",
      "          index    terror             q  event_id    hit_id      nhit  \\\n",
      "0  0.000000e+00  0.463686  4.133248e-07       0.0  0.000023  0.026791   \n",
      "1  7.375281e-08  0.360332  7.727602e-07       0.0  0.000014  0.026791   \n",
      "2  1.475056e-07  0.463686  3.367611e-07       0.0  0.000021  0.026791   \n",
      "3  2.212584e-07  0.463686  2.716814e-07       0.0  0.000009  0.026791   \n",
      "4  2.950112e-07  0.463686  2.605845e-07       0.0  0.000010  0.026791   \n",
      "\n",
      "   energymc   thetamc      fdis  fsinthmc  fsinthmc_v   fcosphi  fcosphi_v  \\\n",
      "0  0.491496  0.989096  0.080255  0.993743    0.000985  0.711894   0.315428   \n",
      "1  0.491496  0.989096  0.244717  0.993743    0.000985  0.174987   0.315428   \n",
      "2  0.491496  0.989096  0.245418  0.993743    0.000985  0.539165   0.315428   \n",
      "3  0.491496  0.989096  0.458048  0.993743    0.000985  0.424618   0.315428   \n",
      "4  0.491496  0.989096  0.293499  0.993743    0.000985  0.092760   0.315428   \n",
      "\n",
      "   fcosthmc  fcosthmc_v  fsinphi_v   ftanphi  ftanphi_v  ftanthmc  ftanthmc_v  \\\n",
      "0  0.017299    0.962054   0.213916  0.684572   0.231896  0.971855    0.002021   \n",
      "1  0.017299    0.962054   0.213916  0.684572   0.231896  0.971855    0.002021   \n",
      "2  0.017299    0.962054   0.213916  0.684572   0.231896  0.971855    0.002021   \n",
      "3  0.017299    0.962054   0.213916  0.684572   0.231896  0.971855    0.002021   \n",
      "4  0.017299    0.962054   0.213916  0.684572   0.231896  0.971855    0.002021   \n",
      "\n",
      "     fttrue  fttrue_v   fttrue2     fttrue2_v  nhitratio  nhitratio_v  \\\n",
      "0  0.183964  0.398003  0.003196  7.851617e-17   0.011146     0.011146   \n",
      "1  0.175149  0.398003  0.004617  5.427911e-17   0.011146     0.011146   \n",
      "2  0.184210  0.398003  0.003160  7.941180e-17   0.011146     0.011146   \n",
      "3  0.184244  0.398003  0.003155  7.953589e-17   0.011146     0.011146   \n",
      "4  0.184522  0.398003  0.003115  8.057079e-17   0.011146     0.011146   \n",
      "\n",
      "   energymc_v  fenergymc2  fenergymc2_v       q_v            q2      q2_v  \\\n",
      "0    0.000233    0.241681  1.591571e-07  0.910630  4.133248e-07  0.910630   \n",
      "1    0.000233    0.241681  1.591571e-07  0.910626  7.727602e-07  0.910626   \n",
      "2    0.000233    0.241681  1.591571e-07  0.910632  3.367611e-07  0.910632   \n",
      "3    0.000233    0.241681  1.591571e-07  0.910635  2.716814e-07  0.910635   \n",
      "4    0.000233    0.241681  1.591571e-07  0.910636  2.605845e-07  0.910636   \n",
      "\n",
      "   fdis_mean  fdis_std  fdis_stdmean    ft_min    ft_max    t_mean    ft_std  \\\n",
      "0   0.424139   0.56866      0.440585  0.464849  0.042349  0.418794  0.799541   \n",
      "1   0.424139   0.56866      0.440585  0.464849  0.042349  0.418794  0.799541   \n",
      "2   0.424139   0.56866      0.440585  0.464849  0.042349  0.418794  0.799541   \n",
      "3   0.424139   0.56866      0.440585  0.464849  0.042349  0.418794  0.799541   \n",
      "4   0.424139   0.56866      0.440585  0.464849  0.042349  0.418794  0.799541   \n",
      "\n",
      "   ft_stdmean   ft_mean    ft_rel   ft2_rel  ft_rel_v     ft2_rel_v  ft_1diff  \\\n",
      "0    0.120869  0.866578  0.079793  0.002978  0.406672  2.725060e-16  0.000000   \n",
      "1    0.120869  0.866578  0.079806  0.002977  0.406672  2.726541e-16  0.000016   \n",
      "2    0.120869  0.866578  0.080062  0.002945  0.406672  2.756138e-16  0.000317   \n",
      "3    0.120869  0.866578  0.080099  0.002940  0.406672  2.760444e-16  0.000046   \n",
      "4    0.120869  0.866578  0.080403  0.002903  0.406672  2.796354e-16  0.000377   \n",
      "\n",
      "   ft_5diff  ft_7diff  ft_11diff  \n",
      "0       0.0       0.0        0.0  \n",
      "1       0.0       0.0        0.0  \n",
      "2       0.0       0.0        0.0  \n",
      "3       0.0       0.0        0.0  \n",
      "4       0.0       0.0        0.0  \n"
     ]
    }
   ],
   "source": [
    "print(trainpd.head())\n",
    "npx = trainpd.values  # returns a numpy array\n",
    "npx[np.isinf(npx)] = 0 # 清洗nan\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "trainnormalpd = pd.DataFrame([[0]*len(trainpd.columns)])\n",
    "trainnormalpd= pd.DataFrame(min_max_scaler.fit_transform(npx))\n",
    "trainnormalpd.columns = trainpd.columns\n",
    "print(trainnormalpd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-e72f925b6ab7>:71: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe462272780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-8-e72f925b6ab7>:92: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fe556811940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:809: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "def batch_iter_list(data_list, batch_size, num_epochs, shuffle=True):\n",
    "    data_size = len(data_list[0])\n",
    "    num_batches_per_epoch = data_size // batch_size  # 每个epoch中包含的batch数量\n",
    "    for epoch in range(num_epochs):\n",
    "        # 每个epoch是否进行shuflle\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data_list = [data[shuffle_indices] for data in data_list]\n",
    "        else:\n",
    "            shuffled_data_list = data_list\n",
    "\n",
    "        for batch_num in range(num_batches_per_epoch + 1):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield [shuffled_data[start_index:end_index] for shuffled_data in shuffled_data_list]\n",
    "\n",
    "class AbstractModeltensor(object):\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config\n",
    "\n",
    "    # You need to override this method.\n",
    "    def buildModel(self):\n",
    "        raise NotImplementedError(\"You need to implement your own model.\")\n",
    "\n",
    "\n",
    "class NeurousNet(AbstractModeltensor):\n",
    "    def __init__(self, xlenth, config=None):\n",
    "        super(NeurousNet, self).__init__(config)\n",
    "        self.graph = tf.Graph()  # 为每个类(实例)单独创建一个graph\n",
    "        self.modeldic = {\n",
    "            \"cnn_dense_less\": self._cnn_dense_less_model,\n",
    "            \"mul4_model\": self._mul4_model,\n",
    "            \"nomul_model\": self._nomul_model,\n",
    "        }\n",
    "        self.ydim = 1\n",
    "        self.keep_prob_ph = config[\"dropout\"]\n",
    "        self.input_dim = xlenth\n",
    "        self.out_dim = 1\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('Inputs'):\n",
    "                self.input_p = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "                self.learn_rate_p = tf.placeholder(dtype=tf.float32, shape=[], name=\"lr\")\n",
    "                self.lr_decay = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "            with tf.name_scope('Outputs'):\n",
    "                self.target_y = tf.placeholder(dtype=tf.float32, shape=[None, self.out_dim])\n",
    "\n",
    "    def buildModel(self):\n",
    "        tf.reset_default_graph()\n",
    "        with self.graph.as_default():\n",
    "            # 不同选择加载\n",
    "            self.modeldic[self.config[\"modelname\"]]()\n",
    "            # 打印打包\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            # 损失目标\n",
    "            tvars = tf.trainable_variables()  # 返回需要训练的variable\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.train_list, tvars), 1e-3)\n",
    "            grads_and_vars = tuple(zip(grads, tvars))\n",
    "            self.train_op = tf.train.AdamOptimizer(self.learn_rate_p).apply_gradients(grads_and_vars)\n",
    "            #             self.train_op = []\n",
    "            #             for i2 in self.train_list:\n",
    "            #                 self.train_op.append(tf.train.AdamOptimizer(self.learn_rate_p).minimize(i2))\n",
    "            # 同一保存加载\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            # [print(n.name) for n in tf.get_default_graph().as_graph_def().node]\n",
    "            # return self.saver\n",
    "\n",
    "    def _mul4_model(self):\n",
    "        with self.graph.as_default():\n",
    "            # 部分1，预测值\n",
    "            base0 = tf.layers.dense(inputs=self.input_p, units=self.input_dim*4, activation=tf.nn.elu,\n",
    "                                     name=\"base0\")\n",
    "            base1 = tf.layers.dense(inputs=self.input_p, units=self.input_dim*8, activation=tf.nn.elu,\n",
    "                                     name=\"base1\")\n",
    "            base2 = tf.layers.dense(inputs=self.input_p, units=self.input_dim*16, activation=tf.nn.elu,\n",
    "                                     name=\"base2\")\n",
    "            base3 = tf.layers.dense(inputs=self.input_p, units=self.input_dim*32, activation=tf.nn.elu,\n",
    "                                     name=\"base3\")\n",
    "            mult0 = tf.layers.dense(inputs=self.input_p, units=self.input_dim*4, activation=tf.nn.elu,\n",
    "                                     name=\"mult0\")\n",
    "            mult_o1 = tf.nn.elu(mult0 * base0, name='mult_o1') # 4 in\n",
    "            mult_e1 = tf.layers.dense(inputs=mult_o1, units=self.input_dim*8, activation=tf.nn.elu,\n",
    "                                     name=\"mult_e1\")\n",
    "            mult_o2 = tf.nn.elu(mult_e1 * base1, name='mult_o2') # 8 in\n",
    "            mult_e2 = tf.layers.dense(inputs=mult_o2, units=self.input_dim*16, activation=tf.nn.elu,\n",
    "                                     name=\"mult_e2\")\n",
    "            mult_o3 = tf.nn.elu(mult_e2 * base2, name='mult_o3') # 16 in\n",
    "            mult_e3 = tf.layers.dense(inputs=mult_o3, units=self.input_dim*32, activation=tf.nn.elu,\n",
    "                                     name=\"mult_e3\")            \n",
    "            mult_o4 = tf.nn.elu(mult_e3 * base3, name='mult_o4') # 32 in\n",
    "            \n",
    "            concat1 = tf.concat([self.input_p, mult_o1, mult_o2, mult_o3, mult_o4], 1, name='concat1')\n",
    "            denseo1 = tf.nn.dropout(concat1, keep_prob=self.keep_prob_ph)\n",
    "            denseo2 = tf.layers.dense(inputs=denseo1, units=self.input_dim * 16, activation=tf.nn.elu, \n",
    "                                      name=\"denseo2\")\n",
    "            denseo2 = tf.nn.dropout(denseo2, keep_prob=self.keep_prob_ph)\n",
    "            denseo3 = tf.layers.dense(inputs=denseo2, units=self.input_dim, activation=tf.nn.elu,\n",
    "                                      name=\"denseo3\")\n",
    "            denseo4 = tf.layers.dense(inputs=denseo3, units=self.input_dim // 8, activation=tf.nn.elu,\n",
    "                                      name=\"denseo4\")\n",
    "            y_res_t = tf.layers.dense(inputs=denseo4, units=self.out_dim, activation=None)\n",
    "            y_res_v = tf.nn.sigmoid(y_res_t, name=\"y_res_v\")\n",
    "            tf.summary.histogram('y_res_v', y_res_v)  # 记录标量的变化\n",
    "            # 损失返回值\n",
    "            y_los = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_res_t, labels=self.target_y, name=\"y_los\")\n",
    "            y_loss_t = tf.reduce_mean(y_los, name=\"y_loss_t\")\n",
    "            y_loss_v = tf.add(y_loss_t, 0, name=\"y_loss_v\")\n",
    "\n",
    "            one = tf.ones_like(y_res_t)\n",
    "            zero = tf.zeros_like(y_res_t)\n",
    "            label_bool = tf.where(y_res_t < 0.5, x=zero, y=one)\n",
    "            self.auc_value, self.auc_op = tf.metrics.auc(self.target_y, label_bool, num_thresholds=4000)\n",
    "            # 猜错的获取 实际盈利值的负数\n",
    "            self.train_list = [y_loss_t]\n",
    "            self.valid_list = [y_loss_v]\n",
    "            self.pred_list = [y_res_v]\n",
    "            # 打印信息\n",
    "            tf.summary.scalar('y_loss_t', y_loss_t)  # 记录标量的变化\n",
    "            tf.summary.scalar('y_loss_v', y_loss_v)  # 记录标量的变化\n",
    "            tf.summary.histogram('concat1', concat1)  # 记录标量的变化\n",
    "            tf.summary.histogram('denseo4', denseo4)  # 记录标量的变化\n",
    "\n",
    "            tf.summary.scalar('lr', self.learn_rate_p)  # 记录标量的变化\n",
    "            return None\n",
    "\n",
    "    def _cnn_dense_less_model(self):\n",
    "        with self.graph.as_default():\n",
    "            # 部分1，预测值\n",
    "            unified = tf.nn.softmax(self.input_p)\n",
    "            dense1 = tf.layers.dense(inputs=unified, units=self.input_dim, activation=tf.nn.softmax,\n",
    "                                     name=\"layer_dense1\")\n",
    "            tf.summary.histogram('dense1', dense1)  # 记录标量的变化\n",
    "            mult_layer1 = tf.nn.softmax(dense1 * self.input_p, name='mult_layer1')\n",
    "            mult_layer2 = tf.nn.softmax(mult_layer1 * self.input_p, name='mult_layer2')\n",
    "            concat1 = tf.concat([self.input_p, dense1, mult_layer1, mult_layer2], 1, name='concat1')\n",
    "            tf.summary.histogram('concat1', concat1)  # 记录标量的变化\n",
    "            denseo1 = tf.nn.dropout(concat1, keep_prob=self.keep_prob_ph)\n",
    "            denseo2 = tf.layers.dense(inputs=denseo1, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense2\")\n",
    "            denseo3 = tf.layers.dense(inputs=denseo2, units=self.input_dim // 4, activation=tf.nn.elu,\n",
    "                                      name=\"layer_dense3\")\n",
    "            y_res_t = tf.layers.dense(inputs=denseo3, units=self.out_dim, activation=None)\n",
    "            y_res_v = tf.nn.sigmoid(y_res_t, name=\"y_res_v\")\n",
    "            tf.summary.histogram('y_res_v', y_res_v)  # 记录标量的变化\n",
    "            # 损失返回值\n",
    "            y_los = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_res_t, labels=self.target_y, name=\"y_los\")\n",
    "            y_loss_t = tf.reduce_mean(y_los, name=\"y_loss_t\")\n",
    "            y_loss_v = tf.add(y_loss_t, 0, name=\"y_loss_v\")\n",
    "\n",
    "            one = tf.ones_like(y_res_t)\n",
    "            zero = tf.zeros_like(y_res_t)\n",
    "            label_bool = tf.where(y_res_t < 0.5, x=zero, y=one)\n",
    "            self.auc_value, self.auc_op = tf.metrics.auc(self.target_y, label_bool, num_thresholds=4000)\n",
    "            # 猜错的获取 实际盈利值的负数\n",
    "            self.train_list = [y_loss_t]\n",
    "            self.valid_list = [y_loss_v]\n",
    "            self.pred_list = [y_res_v]\n",
    "            # 打印信息\n",
    "            tf.summary.scalar('y_loss_t', y_loss_t)  # 记录标量的变化\n",
    "            tf.summary.scalar('y_loss_v', y_loss_v)  # 记录标量的变化\n",
    "            tf.summary.histogram('mult_layer1', mult_layer1)  # 记录标量的变化\n",
    "            tf.summary.histogram('mult_layer2', mult_layer2)  # 记录标量的变化\n",
    "\n",
    "            tf.summary.scalar('lr', self.learn_rate_p)  # 记录标量的变化\n",
    "            return None\n",
    "\n",
    "    def _nomul_model(self):\n",
    "        with self.graph.as_default():\n",
    "            # 部分1，预测值\n",
    "            dense1 = tf.layers.dense(inputs=self.input_p, units=self.input_dim, activation=tf.nn.softmax,\n",
    "                                     name=\"layer_dense1\")\n",
    "            tf.summary.histogram('dense1', dense1)  # 记录标量的变化\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense2\")\n",
    "            dense3 = tf.layers.dense(inputs=dense2, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense3\")\n",
    "            dense4 = tf.layers.dense(inputs=dense3, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense4\")\n",
    "            dense5 = tf.layers.dense(inputs=dense4, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense5\")\n",
    "            dense6 = tf.layers.dense(inputs=dense5, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense6\")\n",
    "            dense7 = tf.layers.dense(inputs=dense6, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense7\")\n",
    "            dense8 = tf.layers.dense(inputs=dense7, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense8\")\n",
    "            concat1 = tf.concat([self.input_p, dense1, dense2, dense3, dense4, dense5, dense6, dense7, dense8], 1,\n",
    "                                name='concat1')\n",
    "            tf.summary.histogram('concat1', concat1)  # 记录标量的变化\n",
    "            denseo1 = tf.nn.dropout(concat1, keep_prob=self.keep_prob_ph)\n",
    "            denseo2 = tf.layers.dense(inputs=denseo1, units=self.input_dim * 4, activation=tf.nn.elu,\n",
    "                                      name=\"layer_denseo2\")\n",
    "            denseo3 = tf.layers.dense(inputs=denseo2, units=self.input_dim, activation=tf.nn.elu,\n",
    "                                      name=\"layer_denseo3\")\n",
    "            denseo4 = tf.layers.dense(inputs=denseo3, units=self.input_dim // 4, activation=tf.nn.elu,\n",
    "                                      name=\"layer_denseo4\")\n",
    "            y_res_t = tf.layers.dense(inputs=denseo4, units=self.out_dim, activation=None)\n",
    "            y_res_v = tf.nn.sigmoid(y_res_t, name=\"y_res_v\")\n",
    "            tf.summary.histogram('y_res_v', y_res_v)  # 记录标量的变化\n",
    "            # 损失返回值\n",
    "            y_los = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_res_t, labels=self.target_y, name=\"y_los\")\n",
    "            y_loss_t = tf.reduce_mean(y_los, name=\"y_loss_t\")\n",
    "            y_loss_v = tf.add(y_loss_t, 0, name=\"y_loss_v\")\n",
    "\n",
    "            one = tf.ones_like(y_res_t)\n",
    "            zero = tf.zeros_like(y_res_t)\n",
    "            label_bool = tf.where(y_res_t < 0.5, x=zero, y=one)\n",
    "            self.auc_value, self.auc_op = tf.metrics.auc(self.target_y, label_bool, num_thresholds=4000)\n",
    "            # 猜错的获取 实际盈利值的负数\n",
    "            self.train_list = [y_loss_t]\n",
    "            self.valid_list = [y_loss_v]\n",
    "            self.pred_list = [y_res_v]\n",
    "            # 打印信息\n",
    "            tf.summary.scalar('y_loss_t', y_loss_t)  # 记录标量的变化\n",
    "            tf.summary.scalar('y_loss_v', y_loss_v)  # 记录标量的变化\n",
    "\n",
    "            tf.summary.scalar('lr', self.learn_rate_p)  # 记录标量的变化\n",
    "            return None\n",
    "\n",
    "    def batch_train(self, trainpd, labels, batch_size=8, num_epochs=1, retrain=True):\n",
    "        # 设置\n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        with sess.as_default():\n",
    "            with self.graph.as_default():\n",
    "                if self.config[\"retrain\"] == 1:\n",
    "                    model_dir = os.path.join(model_path, \"modelevery_%s\" % self.config[\"tailname\"])\n",
    "                    latest_ckpt = tf.train.latest_checkpoint(model_dir)\n",
    "                    if os.path.isfile(\"{}.index\".format(latest_ckpt)):\n",
    "                        self.saver.restore(sess, latest_ckpt)\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        print(\"retraining {}\".format(latest_ckpt))\n",
    "                    else:\n",
    "                        sess.run(tf.global_variables_initializer())\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        print(\"no old model, training new----\")\n",
    "                writer = tf.summary.FileWriter(os.path.join(log_path, \"logsevery_%s\" % self.config[\"tailname\"]),\n",
    "                                               sess.graph)\n",
    "                global_n = 0\n",
    "                stop_n = 0\n",
    "                startt = time.time()\n",
    "                pre_t_base_loss = pre_t_much_loss = pre_v_much_loss = pre_v_base_loss = 100000\n",
    "\n",
    "                n_splits = 5\n",
    "                kf = KFold(n_splits=n_splits, shuffle=True, random_state=4389)\n",
    "                for epoch in range(num_epochs):\n",
    "                    for train_index, valid_index in kf.split(trainpd):\n",
    "                        if self.config[\"learn_rate\"]>1e-9:\n",
    "                            self.config[\"learn_rate\"] *= 0.7\n",
    "                        inputs_t = np.array(trainpd[feature].iloc[train_index])\n",
    "                        output_t = np.expand_dims(np.array(labels[train_index]),-1)\n",
    "                        inputs_v = np.array(trainpd[feature].iloc[valid_index])\n",
    "                        output_v = np.expand_dims(np.array(labels[valid_index]),-1)\n",
    "                        dataiter = batch_iter_list([inputs_t,output_t], batch_size, num_epochs)\n",
    "                        starte = time.time()\n",
    "                        print(\"iter_trainnum\", inputs_t.shape[0] // batch_size + 1)\n",
    "                        redi = inputs_t.shape[0] % batch_size\n",
    "                        lenth = inputs_t.shape[0] // batch_size\n",
    "                        if 0 != redi:\n",
    "                            lenth += 1\n",
    "                        counter = 0\n",
    "                        for batch_num in range(lenth):\n",
    "                            # 获取数据\n",
    "                            r_inputs_t,r_output_t = next(dataiter)\n",
    "                            feed_dict_t = {\n",
    "                                self.input_p: r_inputs_t,\n",
    "                                self.target_y: r_output_t,\n",
    "                                self.learn_rate_p: self.config[\"learn_rate\"],\n",
    "                                self.lr_decay: 1,\n",
    "                            }\n",
    "                            # 更新学习率\n",
    "                            sess.run(self.train_op, feed_dict_t)\n",
    "                            global_n += 1\n",
    "                            losslist_t = sess.run(self.train_list, feed_dict_t)\n",
    "                            sess.run(self.auc_op, feed_dict=feed_dict_t)\n",
    "                            accu = sess.run(self.auc_value)\n",
    "                            result = sess.run(self.merged, feed_dict_t)\n",
    "                            if batch_num % 200 == 0:\n",
    "                                writer.add_summary(result, global_n)\n",
    "                                self.saver.save(sess,\n",
    "                                                os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                             self.config[\"modelfile\"]), global_step=global_n)\n",
    "                                print(\"epocht {}, batch_num {}, step {}, time: {} s, accu: {}, loss_yt: {}\".format(\n",
    "                                    epoch, batch_num, global_n, time.time() - starte, accu, *losslist_t))\n",
    "                        # valid part\n",
    "                        dataiterv = batch_iter_list([inputs_v,output_v], batch_size, num_epochs)\n",
    "                        redi = inputs_v.shape[0] % batch_size\n",
    "                        vnum_iter = inputs_v.shape[0] // batch_size\n",
    "                        if 0 != redi:\n",
    "                            vnum_iter += 1\n",
    "                        counter = 0\n",
    "                        print(\"iter_validnum\", vnum_iter)\n",
    "                        losslist_va = 0\n",
    "                        accu_va = 0\n",
    "                        dataiter = batch_iter_list([inputs_v,output_v], batch_size, num_epochs)\n",
    "                        for batch_num in range(vnum_iter):\n",
    "                            # 获取数据\n",
    "                            r_inputs_v,r_output_v = next(dataiter)\n",
    "                            feed_dict_v = {\n",
    "                                self.input_p: r_inputs_v,\n",
    "                                self.target_y: r_output_v,\n",
    "                                self.learn_rate_p: self.config[\"learn_rate\"],\n",
    "                                self.lr_decay: 1,\n",
    "                            }\n",
    "                            losslist_v = sess.run(self.valid_list, feed_dict_v)\n",
    "                            sess.run(self.auc_op, feed_dict=feed_dict_v)\n",
    "                            accu = sess.run(self.auc_value)\n",
    "                            losslist_va += losslist_v[0]\n",
    "                            accu_va += accu\n",
    "                        losslist_va /= vnum_iter\n",
    "                        accu_va /= vnum_iter\n",
    "                        result = sess.run(self.merged, feed_dict_v)\n",
    "                        writer.add_summary(result, global_n)\n",
    "                        if losslist_t[0] < pre_t_base_loss and losslist_va < pre_v_base_loss:\n",
    "                            stop_n += 1\n",
    "                            if stop_n > self.config[\"early_stop\"]:\n",
    "                                break\n",
    "                            else:\n",
    "                                self.saver.save(sess,\n",
    "                                                os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                             self.config[\"modelfile\"]), global_step=global_n)\n",
    "                        else:\n",
    "                            stop_n = 0\n",
    "                            self.saver.save(sess, os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                               self.config[\"modelfile\"]), global_step=global_n)\n",
    "                        print(\"epochv {}, step {}, stop_n {}, time: {} s, accu_va: {}, loss_yv: {}\".format(\n",
    "                            epoch, global_n, stop_n, time.time() - starte, accu_va, losslist_va))\n",
    "                        pre_t_base_loss = losslist_t[0]\n",
    "                        pre_v_base_loss = losslist_va\n",
    "                writer.close()\n",
    "                print(\"total time: %s s\" % (time.time() - startt))\n",
    "        # 结束\n",
    "        print(\"train finished!\")\n",
    "        return None\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        model_dir = os.path.join(model_path, \"modelevery_%s\" % self.config[\"tailname\"])\n",
    "        print(\"loading model...\")\n",
    "        latest_ckpt = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        with sess.as_default():\n",
    "            with self.graph.as_default():\n",
    "                if os.path.isfile(\"{}.index\".format(latest_ckpt)):\n",
    "                    self.saver.restore(sess, latest_ckpt)\n",
    "                else:\n",
    "                    raise Exception(\"没有找到模型:{}\".format(latest_ckpt))\n",
    "                nplist = []\n",
    "                oneiter = 2000\n",
    "                redi = inputs.shape[0] % oneiter\n",
    "                lenth = inputs.shape[0] // oneiter\n",
    "                if 0 != redi:\n",
    "                    lenth += 1\n",
    "                counter = 0\n",
    "                for num in range(lenth):\n",
    "                    # 获取数据\n",
    "                    startindex = num * oneiter\n",
    "                    if num == lenth - 1 and redi != 0:\n",
    "                        endindex = num * oneiter + redi\n",
    "                    else:\n",
    "                        endindex = (num + 1) * oneiter\n",
    "                    tmppd = inputs.iloc[startindex:endindex][feature]\n",
    "                    r_inputs_v = np.array(tmppd)\n",
    "                    feed_dict = {\n",
    "                        self.input_p: r_inputs_v,\n",
    "                    }\n",
    "                    teslis = sess.run(self.pred_list, feed_dict)\n",
    "                    nplist.append(teslis)\n",
    "                feed_dict = {\n",
    "                    self.input_p: inputs,\n",
    "                }\n",
    "                teslist = np.concatenate(nplist, axis=1)\n",
    "                return teslist\n",
    "\n",
    "\n",
    "trainconfig = {\n",
    "    \"dropout\": 0.5,\n",
    "    \"early_stop\": 100,\n",
    "#     \"tailname\": \"nomul_modeltail\",\n",
    "#     \"modelname\": \"nomul_model\",\n",
    "    \"tailname\": \"mul4_modeltailv2\",\n",
    "    \"modelname\": \"mul4_model\",\n",
    "#     \"tailname\": \"mul_verse\",\n",
    "#     \"modelname\": \"cnn_dense_less\",\n",
    "    \"modelfile\": \"v2\",\n",
    "#     \"learn_rate\": 1e-3,\n",
    "    \"learn_rate\": 5e-4,\n",
    "#     \"learn_rate\": 1e-6,\n",
    "    \"retrain\": 1\n",
    "}\n",
    "modelcrnn = NeurousNet(len(feature), config=trainconfig)\n",
    "modelcrnn.buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../data/particles/model/modelevery_mul4_modeltailv2/v2-7401\n",
      "retraining ../data/particles/model/modelevery_mul4_modeltailv2/v2-7401\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 1, time: 13.664920806884766 s, accu: 0.9719613790512085, loss_yt: 0.08840502053499222\n",
      "epocht 0, batch_num 200, step 201, time: 34.75502419471741 s, accu: 0.966947078704834, loss_yt: 0.07478544861078262\n",
      "epocht 0, batch_num 400, step 401, time: 56.49523305892944 s, accu: 0.9673578143119812, loss_yt: 0.08808233588933945\n",
      "epocht 0, batch_num 600, step 601, time: 78.4117681980133 s, accu: 0.9668822884559631, loss_yt: 0.06902296841144562\n",
      "epocht 0, batch_num 800, step 801, time: 99.84157299995422 s, accu: 0.9669867157936096, loss_yt: 0.08117637783288956\n",
      "WARNING:tensorflow:From /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "epocht 0, batch_num 1000, step 1001, time: 121.35999870300293 s, accu: 0.9669476747512817, loss_yt: 0.07468876242637634\n",
      "epocht 0, batch_num 1200, step 1201, time: 143.21231389045715 s, accu: 0.9667980670928955, loss_yt: 0.07114285975694656\n",
      "epocht 0, batch_num 1400, step 1401, time: 164.89113116264343 s, accu: 0.9669477343559265, loss_yt: 0.07091318070888519\n",
      "epocht 0, batch_num 1600, step 1601, time: 186.77651572227478 s, accu: 0.9669868350028992, loss_yt: 0.12343617528676987\n",
      "epocht 0, batch_num 1800, step 1801, time: 208.33047890663147 s, accu: 0.9669142961502075, loss_yt: 0.06330332159996033\n",
      "epocht 0, batch_num 2000, step 2001, time: 229.8211362361908 s, accu: 0.966942548751831, loss_yt: 0.07293825596570969\n",
      "epocht 0, batch_num 2200, step 2201, time: 251.42282271385193 s, accu: 0.966902494430542, loss_yt: 0.05394154042005539\n",
      "epocht 0, batch_num 2400, step 2401, time: 273.0063445568085 s, accu: 0.9668995141983032, loss_yt: 0.10210643708705902\n",
      "epocht 0, batch_num 2600, step 2601, time: 294.65820503234863 s, accu: 0.9669142365455627, loss_yt: 0.07275158911943436\n",
      "epocht 0, batch_num 2800, step 2801, time: 316.24000096321106 s, accu: 0.967015266418457, loss_yt: 0.08339187502861023\n",
      "epocht 0, batch_num 3000, step 3001, time: 337.78476214408875 s, accu: 0.966964602470398, loss_yt: 0.09498772770166397\n",
      "epocht 0, batch_num 3200, step 3201, time: 359.3535485267639 s, accu: 0.9668938517570496, loss_yt: 0.11437232792377472\n",
      "epocht 0, batch_num 3400, step 3401, time: 380.98171067237854 s, accu: 0.9669409990310669, loss_yt: 0.09373747557401657\n",
      "epocht 0, batch_num 3600, step 3601, time: 402.52171444892883 s, accu: 0.9670029878616333, loss_yt: 0.06792854517698288\n",
      "epocht 0, batch_num 3800, step 3801, time: 424.08229637145996 s, accu: 0.9670028686523438, loss_yt: 0.058577440679073334\n",
      "epocht 0, batch_num 4000, step 4001, time: 445.76813769340515 s, accu: 0.9670650959014893, loss_yt: 0.08069667965173721\n",
      "epocht 0, batch_num 4200, step 4201, time: 467.3280870914459 s, accu: 0.9670445919036865, loss_yt: 0.04938899725675583\n",
      "epocht 0, batch_num 4400, step 4401, time: 488.95440578460693 s, accu: 0.9670878648757935, loss_yt: 0.11485474556684494\n",
      "epocht 0, batch_num 4600, step 4601, time: 510.48604798316956 s, accu: 0.9671107530593872, loss_yt: 0.075350821018219\n",
      "epocht 0, batch_num 4800, step 4801, time: 531.9012808799744 s, accu: 0.9671527147293091, loss_yt: 0.09475455433130264\n",
      "epocht 0, batch_num 5000, step 5001, time: 553.470005273819 s, accu: 0.9671057462692261, loss_yt: 0.09394928812980652\n",
      "epocht 0, batch_num 5200, step 5201, time: 574.9412183761597 s, accu: 0.9671502113342285, loss_yt: 0.0775177851319313\n",
      "epocht 0, batch_num 5400, step 5401, time: 596.4624497890472 s, accu: 0.9671130776405334, loss_yt: 0.12426061183214188\n",
      "epocht 0, batch_num 5600, step 5601, time: 617.8713126182556 s, accu: 0.9671247005462646, loss_yt: 0.06541626900434494\n",
      "epocht 0, batch_num 5800, step 5801, time: 639.2198801040649 s, accu: 0.9671236872673035, loss_yt: 0.061720091849565506\n",
      "epocht 0, batch_num 6000, step 6001, time: 660.8085346221924 s, accu: 0.9671717882156372, loss_yt: 0.055428288877010345\n",
      "epocht 0, batch_num 6200, step 6201, time: 682.5515959262848 s, accu: 0.9671932458877563, loss_yt: 0.09063877910375595\n",
      "epocht 0, batch_num 6400, step 6401, time: 704.1078372001648 s, accu: 0.9672226309776306, loss_yt: 0.09099684655666351\n",
      "epocht 0, batch_num 6600, step 6601, time: 725.9420695304871 s, accu: 0.9672079682350159, loss_yt: 0.0842227041721344\n",
      "epocht 0, batch_num 6800, step 6801, time: 747.5884835720062 s, accu: 0.9672257900238037, loss_yt: 0.07985952496528625\n",
      "epocht 0, batch_num 7000, step 7001, time: 769.0783462524414 s, accu: 0.9672255516052246, loss_yt: 0.08519238233566284\n",
      "epocht 0, batch_num 7200, step 7201, time: 790.1665408611298 s, accu: 0.9672502279281616, loss_yt: 0.0859321728348732\n",
      "epocht 0, batch_num 7400, step 7401, time: 811.2290771007538 s, accu: 0.9672667384147644, loss_yt: 0.10334648936986923\n",
      "epocht 0, batch_num 7600, step 7601, time: 832.4189455509186 s, accu: 0.9672868847846985, loss_yt: 0.11126203089952469\n",
      "epocht 0, batch_num 7800, step 7801, time: 853.4266545772552 s, accu: 0.967299222946167, loss_yt: 0.09888584911823273\n",
      "epocht 0, batch_num 8000, step 8001, time: 874.4934966564178 s, accu: 0.9672873020172119, loss_yt: 0.10921365022659302\n",
      "epocht 0, batch_num 8200, step 8201, time: 895.9679474830627 s, accu: 0.9672871232032776, loss_yt: 0.13254299759864807\n",
      "epocht 0, batch_num 8400, step 8401, time: 917.6320650577545 s, accu: 0.9673176407814026, loss_yt: 0.06393839418888092\n",
      "epocht 0, batch_num 8600, step 8601, time: 938.9985024929047 s, accu: 0.9673143029212952, loss_yt: 0.09584403783082962\n",
      "epocht 0, batch_num 8800, step 8801, time: 960.5520806312561 s, accu: 0.9672843813896179, loss_yt: 0.08245968818664551\n",
      "epocht 0, batch_num 9000, step 9001, time: 982.2569661140442 s, accu: 0.9672783613204956, loss_yt: 0.07150238752365112\n",
      "epocht 0, batch_num 9200, step 9201, time: 1003.6560471057892 s, accu: 0.9672701358795166, loss_yt: 0.09837931394577026\n",
      "epocht 0, batch_num 9400, step 9401, time: 1025.2267565727234 s, accu: 0.9672988653182983, loss_yt: 0.08305321633815765\n",
      "epocht 0, batch_num 9600, step 9601, time: 1046.564726114273 s, accu: 0.9673037528991699, loss_yt: 0.0797584280371666\n",
      "epocht 0, batch_num 9800, step 9801, time: 1068.1055154800415 s, accu: 0.9673232436180115, loss_yt: 0.0737297385931015\n",
      "epocht 0, batch_num 10000, step 10001, time: 1089.5283172130585 s, accu: 0.9673495888710022, loss_yt: 0.06174720823764801\n",
      "epocht 0, batch_num 10200, step 10201, time: 1111.083654165268 s, accu: 0.9673569798469543, loss_yt: 0.07768923044204712\n",
      "epocht 0, batch_num 10400, step 10401, time: 1132.6318957805634 s, accu: 0.9673565626144409, loss_yt: 0.07147079706192017\n",
      "epocht 0, batch_num 10600, step 10601, time: 1153.915428161621 s, accu: 0.9673696160316467, loss_yt: 0.12351355701684952\n",
      "epocht 0, batch_num 10800, step 10801, time: 1175.2449073791504 s, accu: 0.9673653841018677, loss_yt: 0.09591758251190186\n",
      "epocht 0, batch_num 11000, step 11001, time: 1196.6968879699707 s, accu: 0.9673644304275513, loss_yt: 0.0580456517636776\n",
      "epocht 0, batch_num 11200, step 11201, time: 1218.0212202072144 s, accu: 0.9673757553100586, loss_yt: 0.09450383484363556\n",
      "epocht 0, batch_num 11400, step 11401, time: 1239.5406572818756 s, accu: 0.9673796892166138, loss_yt: 0.08560648560523987\n",
      "epocht 0, batch_num 11600, step 11601, time: 1261.0359966754913 s, accu: 0.96739262342453, loss_yt: 0.08843585848808289\n",
      "epocht 0, batch_num 11800, step 11801, time: 1282.549586057663 s, accu: 0.9674047231674194, loss_yt: 0.07881235331296921\n",
      "epocht 0, batch_num 12000, step 12001, time: 1304.0503194332123 s, accu: 0.9674268960952759, loss_yt: 0.11410976946353912\n",
      "epocht 0, batch_num 12200, step 12201, time: 1325.3776361942291 s, accu: 0.9674105644226074, loss_yt: 0.08922383934259415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 12400, step 12401, time: 1346.9222416877747 s, accu: 0.9674159288406372, loss_yt: 0.11610765755176544\n",
      "epocht 0, batch_num 12600, step 12601, time: 1368.1995692253113 s, accu: 0.9674244523048401, loss_yt: 0.10331849753856659\n",
      "epocht 0, batch_num 12800, step 12801, time: 1389.6610238552094 s, accu: 0.9674019813537598, loss_yt: 0.08799419552087784\n",
      "epocht 0, batch_num 13000, step 13001, time: 1411.1642615795135 s, accu: 0.967383861541748, loss_yt: 0.0795389786362648\n",
      "epocht 0, batch_num 13200, step 13201, time: 1432.5853960514069 s, accu: 0.9674171805381775, loss_yt: 0.09140339493751526\n",
      "epocht 0, batch_num 13400, step 13401, time: 1453.9701228141785 s, accu: 0.9674357771873474, loss_yt: 0.08794796466827393\n",
      "epocht 0, batch_num 13600, step 13601, time: 1475.4307816028595 s, accu: 0.9674654603004456, loss_yt: 0.052393339574337006\n",
      "epocht 0, batch_num 13800, step 13801, time: 1496.859807729721 s, accu: 0.9674630165100098, loss_yt: 0.10222358256578445\n",
      "epocht 0, batch_num 14000, step 14001, time: 1518.3052694797516 s, accu: 0.9674925804138184, loss_yt: 0.09002220630645752\n",
      "epocht 0, batch_num 14200, step 14201, time: 1539.699375152588 s, accu: 0.9674980044364929, loss_yt: 0.0825403705239296\n",
      "epocht 0, batch_num 14400, step 14401, time: 1561.2772896289825 s, accu: 0.967499852180481, loss_yt: 0.07513263821601868\n",
      "epocht 0, batch_num 14600, step 14601, time: 1582.765097618103 s, accu: 0.9674964547157288, loss_yt: 0.081663578748703\n",
      "epocht 0, batch_num 14800, step 14801, time: 1604.3725771903992 s, accu: 0.9675036668777466, loss_yt: 0.09610335528850555\n",
      "iter_validnum 3701\n",
      "epochv 0, step 14802, stop_n 1, time: 1644.9245691299438 s, accu_va: 0.9676816628707869, loss_yv: 0.07937692675585523\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 14803, time: 10.415170431137085 s, accu: 0.9678279757499695, loss_yt: 0.06653586775064468\n",
      "epocht 0, batch_num 200, step 15003, time: 31.917688131332397 s, accu: 0.9678381681442261, loss_yt: 0.0727262794971466\n",
      "epocht 0, batch_num 400, step 15203, time: 53.28246355056763 s, accu: 0.9678424000740051, loss_yt: 0.08501142263412476\n",
      "epocht 0, batch_num 600, step 15403, time: 74.7152624130249 s, accu: 0.9678454995155334, loss_yt: 0.13038048148155212\n",
      "epocht 0, batch_num 800, step 15603, time: 96.03445315361023 s, accu: 0.9678459167480469, loss_yt: 0.07741353660821915\n",
      "epocht 0, batch_num 1000, step 15803, time: 117.56453347206116 s, accu: 0.9678632616996765, loss_yt: 0.06904467195272446\n",
      "epocht 0, batch_num 1200, step 16003, time: 139.01091599464417 s, accu: 0.9678677320480347, loss_yt: 0.09026177227497101\n",
      "epocht 0, batch_num 1400, step 16203, time: 160.35745644569397 s, accu: 0.9678789377212524, loss_yt: 0.13449816405773163\n",
      "epocht 0, batch_num 1600, step 16403, time: 182.2925570011139 s, accu: 0.9678870439529419, loss_yt: 0.09920550882816315\n",
      "epocht 0, batch_num 1800, step 16603, time: 203.6466031074524 s, accu: 0.9678905606269836, loss_yt: 0.07230471819639206\n",
      "epocht 0, batch_num 2000, step 16803, time: 225.36297988891602 s, accu: 0.9678940176963806, loss_yt: 0.10057780891656876\n",
      "epocht 0, batch_num 2200, step 17003, time: 246.96901202201843 s, accu: 0.9678983092308044, loss_yt: 0.09050657600164413\n",
      "epocht 0, batch_num 2400, step 17203, time: 268.3477146625519 s, accu: 0.9679038524627686, loss_yt: 0.08621062338352203\n",
      "epocht 0, batch_num 2600, step 17403, time: 289.96021842956543 s, accu: 0.9679160118103027, loss_yt: 0.06766034662723541\n",
      "epocht 0, batch_num 2800, step 17603, time: 311.46495842933655 s, accu: 0.9679204821586609, loss_yt: 0.07812076807022095\n",
      "epocht 0, batch_num 3000, step 17803, time: 333.0200572013855 s, accu: 0.9679273962974548, loss_yt: 0.053791772574186325\n",
      "epocht 0, batch_num 3200, step 18003, time: 354.40267300605774 s, accu: 0.9679329991340637, loss_yt: 0.08822119235992432\n",
      "epocht 0, batch_num 3400, step 18203, time: 375.7531952857971 s, accu: 0.9679413437843323, loss_yt: 0.09868303686380386\n",
      "epocht 0, batch_num 3600, step 18403, time: 397.12180280685425 s, accu: 0.9679473042488098, loss_yt: 0.08330939710140228\n",
      "epocht 0, batch_num 3800, step 18603, time: 418.60773253440857 s, accu: 0.9679492712020874, loss_yt: 0.10740126669406891\n",
      "epocht 0, batch_num 4000, step 18803, time: 439.89334750175476 s, accu: 0.9679535031318665, loss_yt: 0.059738047420978546\n",
      "epocht 0, batch_num 4200, step 19003, time: 461.575838804245 s, accu: 0.9679623246192932, loss_yt: 0.07596167176961899\n",
      "epocht 0, batch_num 4400, step 19203, time: 482.92534017562866 s, accu: 0.967963457107544, loss_yt: 0.08199694752693176\n",
      "epocht 0, batch_num 4600, step 19403, time: 504.408545255661 s, accu: 0.9679675102233887, loss_yt: 0.06192059442400932\n",
      "epocht 0, batch_num 4800, step 19603, time: 526.0022306442261 s, accu: 0.9679672122001648, loss_yt: 0.09827114641666412\n",
      "epocht 0, batch_num 5000, step 19803, time: 547.3637952804565 s, accu: 0.9679814577102661, loss_yt: 0.09873761236667633\n",
      "epocht 0, batch_num 5200, step 20003, time: 568.8557741641998 s, accu: 0.96798175573349, loss_yt: 0.10146341472864151\n",
      "epocht 0, batch_num 5400, step 20203, time: 590.3335671424866 s, accu: 0.9679844975471497, loss_yt: 0.10032123327255249\n",
      "epocht 0, batch_num 5600, step 20403, time: 611.8375043869019 s, accu: 0.9679980278015137, loss_yt: 0.07113566994667053\n",
      "epocht 0, batch_num 5800, step 20603, time: 633.4246349334717 s, accu: 0.9680125713348389, loss_yt: 0.05053798854351044\n",
      "epocht 0, batch_num 6000, step 20803, time: 654.9124755859375 s, accu: 0.9680168628692627, loss_yt: 0.07210475206375122\n",
      "epocht 0, batch_num 6200, step 21003, time: 676.2734699249268 s, accu: 0.9680282473564148, loss_yt: 0.07699806243181229\n",
      "epocht 0, batch_num 6400, step 21203, time: 697.5115368366241 s, accu: 0.9680407047271729, loss_yt: 0.08690233528614044\n",
      "epocht 0, batch_num 6600, step 21403, time: 718.7754118442535 s, accu: 0.9680435657501221, loss_yt: 0.0717034861445427\n",
      "epocht 0, batch_num 6800, step 21603, time: 740.1080598831177 s, accu: 0.9680448174476624, loss_yt: 0.09642747044563293\n",
      "epocht 0, batch_num 7000, step 21803, time: 761.4059138298035 s, accu: 0.9680542349815369, loss_yt: 0.0793524757027626\n",
      "epocht 0, batch_num 7200, step 22003, time: 782.815306186676 s, accu: 0.9680555462837219, loss_yt: 0.0774943083524704\n",
      "epocht 0, batch_num 7400, step 22203, time: 804.4036920070648 s, accu: 0.968063235282898, loss_yt: 0.08327406644821167\n",
      "epocht 0, batch_num 7600, step 22403, time: 825.7656922340393 s, accu: 0.9680676460266113, loss_yt: 0.09539669007062912\n",
      "epocht 0, batch_num 7800, step 22603, time: 847.2347097396851 s, accu: 0.968079149723053, loss_yt: 0.07174282521009445\n",
      "epocht 0, batch_num 8000, step 22803, time: 868.6697781085968 s, accu: 0.9680874943733215, loss_yt: 0.07823250442743301\n",
      "epocht 0, batch_num 8200, step 23003, time: 890.1332190036774 s, accu: 0.9680957198143005, loss_yt: 0.08623217046260834\n",
      "epocht 0, batch_num 8400, step 23203, time: 911.6511926651001 s, accu: 0.968106746673584, loss_yt: 0.10309920459985733\n",
      "epocht 0, batch_num 8600, step 23403, time: 932.9884347915649 s, accu: 0.968101441860199, loss_yt: 0.07876183092594147\n",
      "epocht 0, batch_num 8800, step 23603, time: 954.3023669719696 s, accu: 0.9681053757667542, loss_yt: 0.08137521147727966\n",
      "epocht 0, batch_num 9000, step 23803, time: 975.6802253723145 s, accu: 0.968117892742157, loss_yt: 0.06545382738113403\n",
      "epocht 0, batch_num 9200, step 24003, time: 997.0236899852753 s, accu: 0.9681262373924255, loss_yt: 0.05570271611213684\n",
      "epocht 0, batch_num 9400, step 24203, time: 1018.5856926441193 s, accu: 0.9681337475776672, loss_yt: 0.08402852714061737\n",
      "epocht 0, batch_num 9600, step 24403, time: 1040.0082695484161 s, accu: 0.9681410193443298, loss_yt: 0.0850227028131485\n",
      "epocht 0, batch_num 9800, step 24603, time: 1061.3757548332214 s, accu: 0.9681499600410461, loss_yt: 0.0610831044614315\n",
      "epocht 0, batch_num 10000, step 24803, time: 1082.7792427539825 s, accu: 0.9681504964828491, loss_yt: 0.055587038397789\n",
      "epocht 0, batch_num 10200, step 25003, time: 1104.0008976459503 s, accu: 0.968154788017273, loss_yt: 0.09519072622060776\n",
      "epocht 0, batch_num 10400, step 25203, time: 1125.3594996929169 s, accu: 0.968156099319458, loss_yt: 0.07173217087984085\n",
      "epocht 0, batch_num 10600, step 25403, time: 1146.9372866153717 s, accu: 0.9681593179702759, loss_yt: 0.07859458774328232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 10800, step 25603, time: 1168.4999060630798 s, accu: 0.9681666493415833, loss_yt: 0.079463891685009\n",
      "epocht 0, batch_num 11000, step 25803, time: 1189.795717716217 s, accu: 0.9681711196899414, loss_yt: 0.06081700325012207\n",
      "epocht 0, batch_num 11200, step 26003, time: 1211.317438840866 s, accu: 0.96817946434021, loss_yt: 0.062076278030872345\n",
      "epocht 0, batch_num 11400, step 26203, time: 1232.604011774063 s, accu: 0.9681885838508606, loss_yt: 0.10052545368671417\n",
      "epocht 0, batch_num 11600, step 26403, time: 1254.091302394867 s, accu: 0.9681947231292725, loss_yt: 0.07299789786338806\n",
      "epocht 0, batch_num 11800, step 26603, time: 1275.5028581619263 s, accu: 0.9681949019432068, loss_yt: 0.060634732246398926\n",
      "epocht 0, batch_num 12000, step 26803, time: 1296.9552941322327 s, accu: 0.9682009816169739, loss_yt: 0.06349457055330276\n",
      "epocht 0, batch_num 12200, step 27003, time: 1318.2527792453766 s, accu: 0.9682068824768066, loss_yt: 0.10226769000291824\n",
      "epocht 0, batch_num 12400, step 27203, time: 1339.55540060997 s, accu: 0.9682161808013916, loss_yt: 0.06401225924491882\n",
      "epocht 0, batch_num 12600, step 27403, time: 1360.943011522293 s, accu: 0.9682207703590393, loss_yt: 0.08884716033935547\n",
      "epocht 0, batch_num 12800, step 27603, time: 1382.4556667804718 s, accu: 0.9682275056838989, loss_yt: 0.058086082339286804\n",
      "epocht 0, batch_num 13000, step 27803, time: 1403.7182230949402 s, accu: 0.9682267308235168, loss_yt: 0.09247229993343353\n",
      "epocht 0, batch_num 13200, step 28003, time: 1425.1975984573364 s, accu: 0.9682310223579407, loss_yt: 0.09292647242546082\n",
      "epocht 0, batch_num 13400, step 28203, time: 1446.581666469574 s, accu: 0.9682415723800659, loss_yt: 0.062428638339042664\n",
      "epocht 0, batch_num 13600, step 28403, time: 1467.9795265197754 s, accu: 0.9682407975196838, loss_yt: 0.06400687247514725\n",
      "epocht 0, batch_num 13800, step 28603, time: 1489.3867909908295 s, accu: 0.9682557582855225, loss_yt: 0.04495712369680405\n",
      "epocht 0, batch_num 14000, step 28803, time: 1510.8527727127075 s, accu: 0.9682607054710388, loss_yt: 0.09999600052833557\n",
      "epocht 0, batch_num 14200, step 29003, time: 1532.3658292293549 s, accu: 0.9682681560516357, loss_yt: 0.08511658012866974\n",
      "epocht 0, batch_num 14400, step 29203, time: 1553.6527032852173 s, accu: 0.9682721495628357, loss_yt: 0.07871995866298676\n",
      "epocht 0, batch_num 14600, step 29403, time: 1575.1009061336517 s, accu: 0.9682772159576416, loss_yt: 0.08440476655960083\n",
      "epocht 0, batch_num 14800, step 29603, time: 1596.5083854198456 s, accu: 0.968278706073761, loss_yt: 0.058746106922626495\n",
      "iter_validnum 3701\n",
      "epochv 0, step 29604, stop_n 0, time: 1638.0299634933472 s, accu_va: 0.9683555305181275, loss_yv: 0.07776319062686053\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 29605, time: 10.364041566848755 s, accu: 0.9684236645698547, loss_yt: 0.056002579629421234\n",
      "epocht 0, batch_num 200, step 29805, time: 31.2479305267334 s, accu: 0.9684334993362427, loss_yt: 0.07134704291820526\n",
      "epocht 0, batch_num 400, step 30005, time: 52.66723108291626 s, accu: 0.9684405326843262, loss_yt: 0.06367488950490952\n",
      "epocht 0, batch_num 600, step 30205, time: 73.79343175888062 s, accu: 0.9684479832649231, loss_yt: 0.0929289311170578\n",
      "epocht 0, batch_num 800, step 30405, time: 95.2495493888855 s, accu: 0.9684582352638245, loss_yt: 0.06798987835645676\n",
      "epocht 0, batch_num 1000, step 30605, time: 116.62587976455688 s, accu: 0.9684646129608154, loss_yt: 0.09670833498239517\n",
      "epocht 0, batch_num 1200, step 30805, time: 138.02668356895447 s, accu: 0.968468427658081, loss_yt: 0.0705554187297821\n",
      "epocht 0, batch_num 1400, step 31005, time: 159.5293459892273 s, accu: 0.9684772491455078, loss_yt: 0.08510681241750717\n",
      "epocht 0, batch_num 1600, step 31205, time: 180.879047870636 s, accu: 0.9684786200523376, loss_yt: 0.0917615294456482\n",
      "epocht 0, batch_num 1800, step 31405, time: 202.4112057685852 s, accu: 0.9684841632843018, loss_yt: 0.08139896392822266\n",
      "epocht 0, batch_num 2000, step 31605, time: 223.77987933158875 s, accu: 0.9684858322143555, loss_yt: 0.06577637791633606\n",
      "epocht 0, batch_num 2200, step 31805, time: 245.21512007713318 s, accu: 0.9684932231903076, loss_yt: 0.07928568124771118\n",
      "epocht 0, batch_num 2400, step 32005, time: 266.6650788784027 s, accu: 0.9685039520263672, loss_yt: 0.07022928446531296\n",
      "epocht 0, batch_num 2600, step 32205, time: 288.18778228759766 s, accu: 0.9685101509094238, loss_yt: 0.11230526864528656\n",
      "epocht 0, batch_num 2800, step 32405, time: 309.611629486084 s, accu: 0.968512773513794, loss_yt: 0.06862124055624008\n",
      "epocht 0, batch_num 3000, step 32605, time: 331.0428981781006 s, accu: 0.968514621257782, loss_yt: 0.08606898784637451\n",
      "epocht 0, batch_num 3200, step 32805, time: 352.41176414489746 s, accu: 0.9685215950012207, loss_yt: 0.0748685896396637\n",
      "epocht 0, batch_num 3400, step 33005, time: 373.78348302841187 s, accu: 0.9685264229774475, loss_yt: 0.06554173678159714\n",
      "epocht 0, batch_num 3600, step 33205, time: 395.1598765850067 s, accu: 0.9685351848602295, loss_yt: 0.07590676099061966\n",
      "epocht 0, batch_num 3800, step 33405, time: 416.46677136421204 s, accu: 0.9685399532318115, loss_yt: 0.10395214706659317\n",
      "epocht 0, batch_num 4000, step 33605, time: 437.8442931175232 s, accu: 0.9685506820678711, loss_yt: 0.07114294916391373\n",
      "epocht 0, batch_num 4200, step 33805, time: 459.37613916397095 s, accu: 0.9685560464859009, loss_yt: 0.11675214767456055\n",
      "epocht 0, batch_num 4400, step 34005, time: 480.93604135513306 s, accu: 0.9685627818107605, loss_yt: 0.0710502564907074\n",
      "epocht 0, batch_num 4600, step 34205, time: 502.3706889152527 s, accu: 0.9685648083686829, loss_yt: 0.06744728237390518\n",
      "epocht 0, batch_num 4800, step 34405, time: 523.7978343963623 s, accu: 0.9685727953910828, loss_yt: 0.10823575407266617\n",
      "epocht 0, batch_num 5000, step 34605, time: 545.267828464508 s, accu: 0.9685783386230469, loss_yt: 0.09270244836807251\n",
      "epocht 0, batch_num 5200, step 34805, time: 566.6794769763947 s, accu: 0.9685857892036438, loss_yt: 0.08778609335422516\n",
      "epocht 0, batch_num 5400, step 35005, time: 588.0502555370331 s, accu: 0.9685880541801453, loss_yt: 0.09883514791727066\n",
      "epocht 0, batch_num 5600, step 35205, time: 609.610378742218 s, accu: 0.9685988426208496, loss_yt: 0.04855144023895264\n",
      "epocht 0, batch_num 5800, step 35405, time: 630.8488037586212 s, accu: 0.9686055779457092, loss_yt: 0.07954204082489014\n",
      "epocht 0, batch_num 6000, step 35605, time: 652.1072671413422 s, accu: 0.9686061143875122, loss_yt: 0.09806746989488602\n",
      "epocht 0, batch_num 6200, step 35805, time: 673.5196807384491 s, accu: 0.9686095714569092, loss_yt: 0.08003604412078857\n",
      "epocht 0, batch_num 6400, step 36005, time: 695.0305635929108 s, accu: 0.96861332654953, loss_yt: 0.10772978514432907\n",
      "epocht 0, batch_num 6600, step 36205, time: 716.345675945282 s, accu: 0.9686185717582703, loss_yt: 0.10677743703126907\n",
      "epocht 0, batch_num 6800, step 36405, time: 737.6460192203522 s, accu: 0.9686241745948792, loss_yt: 0.07657676935195923\n",
      "epocht 0, batch_num 7000, step 36605, time: 759.1576435565948 s, accu: 0.9686228036880493, loss_yt: 0.06432607024908066\n",
      "epocht 0, batch_num 7200, step 36805, time: 780.4608962535858 s, accu: 0.9686254262924194, loss_yt: 0.07512456923723221\n",
      "epocht 0, batch_num 7400, step 37005, time: 801.7432882785797 s, accu: 0.9686298370361328, loss_yt: 0.08652253448963165\n",
      "epocht 0, batch_num 7600, step 37205, time: 822.9846880435944 s, accu: 0.9686306118965149, loss_yt: 0.0852559357881546\n",
      "epocht 0, batch_num 7800, step 37405, time: 844.2876904010773 s, accu: 0.9686346054077148, loss_yt: 0.07221579551696777\n",
      "epocht 0, batch_num 8000, step 37605, time: 865.6509740352631 s, accu: 0.9686437249183655, loss_yt: 0.06830879300832748\n",
      "epocht 0, batch_num 8200, step 37805, time: 887.0781531333923 s, accu: 0.9686497449874878, loss_yt: 0.06273352354764938\n",
      "epocht 0, batch_num 8400, step 38005, time: 908.5572154521942 s, accu: 0.968650758266449, loss_yt: 0.07807973027229309\n",
      "epocht 0, batch_num 8600, step 38205, time: 929.935839176178 s, accu: 0.9686577320098877, loss_yt: 0.06626809388399124\n",
      "epocht 0, batch_num 8800, step 38405, time: 951.272444486618 s, accu: 0.9686632752418518, loss_yt: 0.08740275353193283\n",
      "epocht 0, batch_num 9000, step 38605, time: 972.7379460334778 s, accu: 0.9686625003814697, loss_yt: 0.1025865450501442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 9200, step 38805, time: 993.9209911823273 s, accu: 0.9686657786369324, loss_yt: 0.06365431100130081\n",
      "epocht 0, batch_num 9400, step 39005, time: 1015.3027074337006 s, accu: 0.9686669707298279, loss_yt: 0.09164287894964218\n",
      "epocht 0, batch_num 9600, step 39205, time: 1036.7178659439087 s, accu: 0.9686759114265442, loss_yt: 0.06989862024784088\n",
      "epocht 0, batch_num 9800, step 39405, time: 1058.0102016925812 s, accu: 0.968682587146759, loss_yt: 0.0859505906701088\n",
      "epocht 0, batch_num 10000, step 39605, time: 1079.5768620967865 s, accu: 0.9686869382858276, loss_yt: 0.09296546876430511\n",
      "epocht 0, batch_num 10200, step 39805, time: 1101.084318637848 s, accu: 0.9686927795410156, loss_yt: 0.07559355348348618\n",
      "epocht 0, batch_num 10400, step 40005, time: 1122.448638677597 s, accu: 0.9687001705169678, loss_yt: 0.07925371080636978\n",
      "epocht 0, batch_num 10600, step 40205, time: 1143.7558100223541 s, accu: 0.9687078595161438, loss_yt: 0.06876722723245621\n",
      "epocht 0, batch_num 10800, step 40405, time: 1164.9892723560333 s, accu: 0.9687119722366333, loss_yt: 0.07457228004932404\n",
      "epocht 0, batch_num 11000, step 40605, time: 1186.505300283432 s, accu: 0.9687168598175049, loss_yt: 0.09297626465559006\n",
      "epocht 0, batch_num 11200, step 40805, time: 1207.891964673996 s, accu: 0.9687206149101257, loss_yt: 0.0775984451174736\n",
      "epocht 0, batch_num 11400, step 41005, time: 1229.1814668178558 s, accu: 0.9687228798866272, loss_yt: 0.07227472215890884\n",
      "epocht 0, batch_num 11600, step 41205, time: 1250.6885783672333 s, accu: 0.968730628490448, loss_yt: 0.05332605540752411\n",
      "epocht 0, batch_num 11800, step 41405, time: 1272.0067911148071 s, accu: 0.9687363505363464, loss_yt: 0.0958935096859932\n",
      "epocht 0, batch_num 12000, step 41605, time: 1293.360654592514 s, accu: 0.968741238117218, loss_yt: 0.06508243829011917\n",
      "epocht 0, batch_num 12200, step 41805, time: 1314.7184438705444 s, accu: 0.9687444567680359, loss_yt: 0.09936141222715378\n",
      "epocht 0, batch_num 12400, step 42005, time: 1336.255923986435 s, accu: 0.9687466621398926, loss_yt: 0.08882397413253784\n",
      "epocht 0, batch_num 12600, step 42205, time: 1357.7805516719818 s, accu: 0.9687515497207642, loss_yt: 0.0544990636408329\n",
      "epocht 0, batch_num 12800, step 42405, time: 1379.2774448394775 s, accu: 0.9687573313713074, loss_yt: 0.06462033838033676\n",
      "epocht 0, batch_num 13000, step 42605, time: 1400.5593433380127 s, accu: 0.9687627553939819, loss_yt: 0.06901319324970245\n",
      "epocht 0, batch_num 13200, step 42805, time: 1421.8745217323303 s, accu: 0.9687638282775879, loss_yt: 0.07648345082998276\n",
      "epocht 0, batch_num 13400, step 43005, time: 1443.2189333438873 s, accu: 0.9687695503234863, loss_yt: 0.04527192562818527\n",
      "epocht 0, batch_num 13600, step 43205, time: 1464.559455871582 s, accu: 0.968773365020752, loss_yt: 0.0656675472855568\n",
      "epocht 0, batch_num 13800, step 43405, time: 1485.8663275241852 s, accu: 0.9687738418579102, loss_yt: 0.06750452518463135\n",
      "epocht 0, batch_num 14000, step 43605, time: 1507.0678355693817 s, accu: 0.9687776565551758, loss_yt: 0.1030784621834755\n",
      "epocht 0, batch_num 14200, step 43805, time: 1528.4977679252625 s, accu: 0.9687769412994385, loss_yt: 0.08301753550767899\n",
      "epocht 0, batch_num 14400, step 44005, time: 1549.87531208992 s, accu: 0.9687816500663757, loss_yt: 0.06651020050048828\n",
      "epocht 0, batch_num 14600, step 44205, time: 1571.3355176448822 s, accu: 0.9687840342521667, loss_yt: 0.09086914360523224\n",
      "epocht 0, batch_num 14800, step 44405, time: 1592.7703051567078 s, accu: 0.9687848687171936, loss_yt: 0.05847393721342087\n",
      "iter_validnum 3701\n",
      "epochv 0, step 44406, stop_n 1, time: 1633.8681888580322 s, accu_va: 0.9688279151079043, loss_yv: 0.07667308788413423\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 44407, time: 10.974904537200928 s, accu: 0.9688669443130493, loss_yt: 0.07969905436038971\n",
      "epocht 0, batch_num 200, step 44607, time: 32.254661083221436 s, accu: 0.9688743948936462, loss_yt: 0.0639515146613121\n",
      "epocht 0, batch_num 400, step 44807, time: 53.13666892051697 s, accu: 0.9688750505447388, loss_yt: 0.08062202483415604\n",
      "epocht 0, batch_num 600, step 45007, time: 74.26289939880371 s, accu: 0.968883216381073, loss_yt: 0.06491415947675705\n",
      "epocht 0, batch_num 800, step 45207, time: 95.14298009872437 s, accu: 0.9688849449157715, loss_yt: 0.06637398153543472\n",
      "epocht 0, batch_num 1000, step 45407, time: 116.10399222373962 s, accu: 0.9688887000083923, loss_yt: 0.10152994841337204\n",
      "epocht 0, batch_num 1200, step 45607, time: 137.50190997123718 s, accu: 0.9688913822174072, loss_yt: 0.10355459153652191\n",
      "epocht 0, batch_num 1400, step 45807, time: 158.63970685005188 s, accu: 0.9688957333564758, loss_yt: 0.07003076374530792\n",
      "epocht 0, batch_num 1600, step 46007, time: 179.95948028564453 s, accu: 0.9688999056816101, loss_yt: 0.05323228985071182\n",
      "epocht 0, batch_num 1800, step 46207, time: 201.2343394756317 s, accu: 0.9689021110534668, loss_yt: 0.057830989360809326\n",
      "epocht 0, batch_num 2000, step 46407, time: 222.52907729148865 s, accu: 0.9689093232154846, loss_yt: 0.10007749497890472\n",
      "epocht 0, batch_num 2200, step 46607, time: 243.86903309822083 s, accu: 0.9689162969589233, loss_yt: 0.0924220085144043\n",
      "epocht 0, batch_num 2400, step 46807, time: 265.210825920105 s, accu: 0.9689208269119263, loss_yt: 0.06337185204029083\n",
      "epocht 0, batch_num 2600, step 47007, time: 286.53167963027954 s, accu: 0.968925952911377, loss_yt: 0.10008638352155685\n",
      "epocht 0, batch_num 2800, step 47207, time: 307.86163568496704 s, accu: 0.9689314961433411, loss_yt: 0.14273977279663086\n",
      "epocht 0, batch_num 3000, step 47407, time: 329.16793966293335 s, accu: 0.9689345359802246, loss_yt: 0.10207870602607727\n",
      "epocht 0, batch_num 3200, step 47607, time: 350.45608258247375 s, accu: 0.968939483165741, loss_yt: 0.10280170291662216\n",
      "epocht 0, batch_num 3400, step 47807, time: 371.72777581214905 s, accu: 0.9689469337463379, loss_yt: 0.08415938913822174\n",
      "epocht 0, batch_num 3600, step 48007, time: 393.2573757171631 s, accu: 0.9689489603042603, loss_yt: 0.10269103944301605\n",
      "epocht 0, batch_num 3800, step 48207, time: 414.54560351371765 s, accu: 0.9689502716064453, loss_yt: 0.07967398315668106\n",
      "epocht 0, batch_num 4000, step 48407, time: 435.99790143966675 s, accu: 0.9689549207687378, loss_yt: 0.06689635664224625\n",
      "epocht 0, batch_num 4200, step 48607, time: 457.39541816711426 s, accu: 0.9689607620239258, loss_yt: 0.05620092898607254\n",
      "epocht 0, batch_num 4400, step 48807, time: 478.93012166023254 s, accu: 0.9689657092094421, loss_yt: 0.03858008608222008\n",
      "epocht 0, batch_num 4600, step 49007, time: 500.37420892715454 s, accu: 0.9689685702323914, loss_yt: 0.059357624500989914\n",
      "epocht 0, batch_num 4800, step 49207, time: 521.8974313735962 s, accu: 0.9689701795578003, loss_yt: 0.09481273591518402\n",
      "epocht 0, batch_num 5000, step 49407, time: 543.3281617164612 s, accu: 0.9689701795578003, loss_yt: 0.07242985814809799\n",
      "epocht 0, batch_num 5200, step 49607, time: 564.832159280777 s, accu: 0.9689746499061584, loss_yt: 0.07742296904325485\n",
      "epocht 0, batch_num 5400, step 49807, time: 586.1553862094879 s, accu: 0.9689763188362122, loss_yt: 0.10478271543979645\n",
      "epocht 0, batch_num 5600, step 50007, time: 607.5246486663818 s, accu: 0.968978226184845, loss_yt: 0.07396290451288223\n",
      "epocht 0, batch_num 5800, step 50207, time: 628.7891879081726 s, accu: 0.9689815640449524, loss_yt: 0.06241511553525925\n",
      "epocht 0, batch_num 6000, step 50407, time: 650.1484935283661 s, accu: 0.968982458114624, loss_yt: 0.04454544186592102\n",
      "epocht 0, batch_num 6200, step 50607, time: 671.536422252655 s, accu: 0.9689860939979553, loss_yt: 0.05341142416000366\n",
      "epocht 0, batch_num 6400, step 50807, time: 692.7886154651642 s, accu: 0.9689849019050598, loss_yt: 0.08939194679260254\n",
      "epocht 0, batch_num 6600, step 51007, time: 714.2582938671112 s, accu: 0.9689886569976807, loss_yt: 0.1367397904396057\n",
      "epocht 0, batch_num 6800, step 51207, time: 735.5231115818024 s, accu: 0.9689921736717224, loss_yt: 0.08909998089075089\n",
      "epocht 0, batch_num 7000, step 51407, time: 756.9290466308594 s, accu: 0.9689954519271851, loss_yt: 0.053256548941135406\n",
      "epocht 0, batch_num 7200, step 51607, time: 778.375834941864 s, accu: 0.9690011739730835, loss_yt: 0.07473058998584747\n",
      "epocht 0, batch_num 7400, step 51807, time: 799.871143579483 s, accu: 0.9690054655075073, loss_yt: 0.09830456227064133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 7600, step 52007, time: 821.4154300689697 s, accu: 0.969011127948761, loss_yt: 0.07514090090990067\n",
      "epocht 0, batch_num 7800, step 52207, time: 842.8233268260956 s, accu: 0.9690178036689758, loss_yt: 0.058846306055784225\n",
      "epocht 0, batch_num 8000, step 52407, time: 864.1154637336731 s, accu: 0.9690213799476624, loss_yt: 0.07439171522855759\n",
      "epocht 0, batch_num 8200, step 52607, time: 885.3394162654877 s, accu: 0.9690219163894653, loss_yt: 0.064272940158844\n",
      "epocht 0, batch_num 8400, step 52807, time: 906.7970468997955 s, accu: 0.9690254926681519, loss_yt: 0.0929197445511818\n",
      "epocht 0, batch_num 8600, step 53007, time: 928.2297506332397 s, accu: 0.9690307378768921, loss_yt: 0.05221998691558838\n",
      "epocht 0, batch_num 8800, step 53207, time: 949.6616535186768 s, accu: 0.9690297842025757, loss_yt: 0.06548319011926651\n",
      "epocht 0, batch_num 9000, step 53407, time: 970.8731274604797 s, accu: 0.9690313339233398, loss_yt: 0.09423138201236725\n",
      "epocht 0, batch_num 9200, step 53607, time: 992.3832263946533 s, accu: 0.9690356850624084, loss_yt: 0.07834991067647934\n",
      "epocht 0, batch_num 9400, step 53807, time: 1013.8304030895233 s, accu: 0.969039261341095, loss_yt: 0.0742906853556633\n",
      "epocht 0, batch_num 9600, step 54007, time: 1035.2751257419586 s, accu: 0.9690455794334412, loss_yt: 0.06120749190449715\n",
      "epocht 0, batch_num 9800, step 54207, time: 1056.6952266693115 s, accu: 0.9690474271774292, loss_yt: 0.06238522380590439\n",
      "epocht 0, batch_num 10000, step 54407, time: 1078.0397100448608 s, accu: 0.9690524935722351, loss_yt: 0.04916270077228546\n",
      "epocht 0, batch_num 10200, step 54607, time: 1099.3677017688751 s, accu: 0.9690544009208679, loss_yt: 0.09737734496593475\n",
      "epocht 0, batch_num 10400, step 54807, time: 1120.8727734088898 s, accu: 0.9690627455711365, loss_yt: 0.0698775053024292\n",
      "epocht 0, batch_num 10600, step 55007, time: 1142.2375009059906 s, accu: 0.9690679311752319, loss_yt: 0.06349111348390579\n",
      "epocht 0, batch_num 10800, step 55207, time: 1163.6257433891296 s, accu: 0.9690713882446289, loss_yt: 0.0706675797700882\n",
      "epocht 0, batch_num 11000, step 55407, time: 1185.0402240753174 s, accu: 0.9690752029418945, loss_yt: 0.08132509887218475\n",
      "epocht 0, batch_num 11200, step 55607, time: 1206.3760249614716 s, accu: 0.9690781831741333, loss_yt: 0.07655506581068039\n",
      "epocht 0, batch_num 11400, step 55807, time: 1227.804160118103 s, accu: 0.9690799713134766, loss_yt: 0.08706830441951752\n",
      "epocht 0, batch_num 11600, step 56007, time: 1249.235654592514 s, accu: 0.9690839052200317, loss_yt: 0.09474310278892517\n",
      "epocht 0, batch_num 11800, step 56207, time: 1270.6304259300232 s, accu: 0.9690866470336914, loss_yt: 0.08840423822402954\n",
      "epocht 0, batch_num 12000, step 56407, time: 1291.8429400920868 s, accu: 0.9690893292427063, loss_yt: 0.07510524988174438\n",
      "epocht 0, batch_num 12200, step 56607, time: 1313.3062524795532 s, accu: 0.9690946340560913, loss_yt: 0.07328644394874573\n",
      "epocht 0, batch_num 12400, step 56807, time: 1334.7297403812408 s, accu: 0.9690969586372375, loss_yt: 0.06693801283836365\n",
      "epocht 0, batch_num 12600, step 57007, time: 1355.9963643550873 s, accu: 0.969099760055542, loss_yt: 0.07721775770187378\n",
      "epocht 0, batch_num 12800, step 57207, time: 1377.4826793670654 s, accu: 0.9691062569618225, loss_yt: 0.08734589070081711\n",
      "epocht 0, batch_num 13000, step 57407, time: 1398.8629200458527 s, accu: 0.9691078662872314, loss_yt: 0.05940284579992294\n",
      "epocht 0, batch_num 13200, step 57607, time: 1420.2476654052734 s, accu: 0.9691095352172852, loss_yt: 0.06732646375894547\n",
      "epocht 0, batch_num 13400, step 57807, time: 1441.4202806949615 s, accu: 0.9691113829612732, loss_yt: 0.07387849688529968\n",
      "epocht 0, batch_num 13600, step 58007, time: 1462.6178348064423 s, accu: 0.9691174030303955, loss_yt: 0.06177964434027672\n",
      "epocht 0, batch_num 13800, step 58207, time: 1483.8468942642212 s, accu: 0.9691201448440552, loss_yt: 0.07541780918836594\n",
      "epocht 0, batch_num 14000, step 58407, time: 1505.2065424919128 s, accu: 0.9691240191459656, loss_yt: 0.0694427490234375\n",
      "epocht 0, batch_num 14200, step 58607, time: 1526.5826976299286 s, accu: 0.9691248536109924, loss_yt: 0.09578832983970642\n",
      "epocht 0, batch_num 14400, step 58807, time: 1548.023647069931 s, accu: 0.9691286683082581, loss_yt: 0.08887504041194916\n",
      "epocht 0, batch_num 14600, step 59007, time: 1569.304740190506 s, accu: 0.9691323637962341, loss_yt: 0.08460889011621475\n",
      "epocht 0, batch_num 14800, step 59207, time: 1590.7399401664734 s, accu: 0.9691345691680908, loss_yt: 0.073560431599617\n",
      "iter_validnum 3701\n",
      "epochv 0, step 59208, stop_n 0, time: 1631.772311925888 s, accu_va: 0.969128475015919, loss_yv: 0.07930083868290823\n",
      "iter_trainnum 14802\n",
      "epocht 0, batch_num 0, step 59209, time: 11.047889947891235 s, accu: 0.9691270589828491, loss_yt: 0.05583256483078003\n",
      "epocht 0, batch_num 200, step 59409, time: 32.36735939979553 s, accu: 0.9691303968429565, loss_yt: 0.10696123540401459\n",
      "epocht 0, batch_num 400, step 59609, time: 53.720375776290894 s, accu: 0.9691343307495117, loss_yt: 0.07197919487953186\n",
      "epocht 0, batch_num 600, step 59809, time: 74.99036955833435 s, accu: 0.9691372513771057, loss_yt: 0.05930839478969574\n",
      "epocht 0, batch_num 800, step 60009, time: 96.47191596031189 s, accu: 0.9691389203071594, loss_yt: 0.06291820108890533\n",
      "epocht 0, batch_num 1000, step 60209, time: 117.77003622055054 s, accu: 0.9691420197486877, loss_yt: 0.061595283448696136\n",
      "epocht 0, batch_num 1200, step 60409, time: 139.1859073638916 s, accu: 0.9691451787948608, loss_yt: 0.07028822600841522\n",
      "epocht 0, batch_num 1400, step 60609, time: 160.75976753234863 s, accu: 0.9691479206085205, loss_yt: 0.04935075342655182\n",
      "epocht 0, batch_num 1600, step 60809, time: 181.63702821731567 s, accu: 0.9691511392593384, loss_yt: 0.060009296983480453\n",
      "epocht 0, batch_num 1800, step 61009, time: 203.0068497657776 s, accu: 0.9691551923751831, loss_yt: 0.09592894464731216\n",
      "epocht 0, batch_num 2000, step 61209, time: 224.18992447853088 s, accu: 0.9691588282585144, loss_yt: 0.08328215777873993\n",
      "epocht 0, batch_num 2200, step 61409, time: 245.47524094581604 s, accu: 0.9691618084907532, loss_yt: 0.053731463849544525\n",
      "epocht 0, batch_num 2400, step 61609, time: 266.7589647769928 s, accu: 0.9691653251647949, loss_yt: 0.08717837184667587\n",
      "epocht 0, batch_num 2600, step 61809, time: 288.2520844936371 s, accu: 0.9691676497459412, loss_yt: 0.09307192265987396\n",
      "epocht 0, batch_num 2800, step 62009, time: 309.5060935020447 s, accu: 0.9691715240478516, loss_yt: 0.049910057336091995\n",
      "epocht 0, batch_num 3000, step 62209, time: 330.7743353843689 s, accu: 0.9691752195358276, loss_yt: 0.07646813243627548\n",
      "epocht 0, batch_num 3200, step 62409, time: 352.04769253730774 s, accu: 0.9691759347915649, loss_yt: 0.07472702860832214\n",
      "epocht 0, batch_num 3400, step 62609, time: 373.34226655960083 s, accu: 0.9691821336746216, loss_yt: 0.07928671687841415\n",
      "epocht 0, batch_num 3600, step 62809, time: 394.60572934150696 s, accu: 0.9691860675811768, loss_yt: 0.061623167246580124\n",
      "epocht 0, batch_num 3800, step 63009, time: 416.10199069976807 s, accu: 0.969188392162323, loss_yt: 0.05537088215351105\n",
      "epocht 0, batch_num 4000, step 63209, time: 437.5273985862732 s, accu: 0.9691941738128662, loss_yt: 0.06578782200813293\n",
      "epocht 0, batch_num 4200, step 63409, time: 458.7751190662384 s, accu: 0.9692009091377258, loss_yt: 0.06874360144138336\n",
      "epocht 0, batch_num 4400, step 63609, time: 480.031231880188 s, accu: 0.9692066311836243, loss_yt: 0.07884330302476883\n",
      "epocht 0, batch_num 4600, step 63809, time: 501.36278772354126 s, accu: 0.9692104458808899, loss_yt: 0.06402505189180374\n",
      "epocht 0, batch_num 4800, step 64009, time: 522.6641314029694 s, accu: 0.9692131280899048, loss_yt: 0.08678966015577316\n",
      "epocht 0, batch_num 5000, step 64209, time: 544.0324721336365 s, accu: 0.9692147374153137, loss_yt: 0.06830678880214691\n",
      "epocht 0, batch_num 5200, step 64409, time: 565.4409682750702 s, accu: 0.9692203998565674, loss_yt: 0.07435274124145508\n",
      "epocht 0, batch_num 5400, step 64609, time: 586.8664746284485 s, accu: 0.9692261815071106, loss_yt: 0.06481823325157166\n",
      "epocht 0, batch_num 5600, step 64809, time: 608.2963240146637 s, accu: 0.9692305326461792, loss_yt: 0.05543268844485283\n",
      "epocht 0, batch_num 5800, step 65009, time: 629.5676927566528 s, accu: 0.9692348837852478, loss_yt: 0.07486645132303238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 6000, step 65209, time: 650.7863571643829 s, accu: 0.9692387580871582, loss_yt: 0.06553173065185547\n",
      "epocht 0, batch_num 6200, step 65409, time: 671.9779303073883 s, accu: 0.9692401885986328, loss_yt: 0.07946516573429108\n",
      "epocht 0, batch_num 6400, step 65609, time: 693.3258275985718 s, accu: 0.9692429900169373, loss_yt: 0.06514221429824829\n",
      "epocht 0, batch_num 6600, step 65809, time: 714.6647822856903 s, accu: 0.9692486524581909, loss_yt: 0.07129184901714325\n",
      "epocht 0, batch_num 6800, step 66009, time: 735.8335041999817 s, accu: 0.9692521095275879, loss_yt: 0.07217734307050705\n",
      "epocht 0, batch_num 7000, step 66209, time: 757.1304490566254 s, accu: 0.9692581295967102, loss_yt: 0.09046979248523712\n",
      "epocht 0, batch_num 7200, step 66409, time: 778.4139864444733 s, accu: 0.9692624807357788, loss_yt: 0.08052453398704529\n",
      "epocht 0, batch_num 7400, step 66609, time: 799.6801784038544 s, accu: 0.9692646861076355, loss_yt: 0.12078789621591568\n",
      "epocht 0, batch_num 7600, step 66809, time: 821.0047390460968 s, accu: 0.9692679047584534, loss_yt: 0.10718245804309845\n",
      "epocht 0, batch_num 7800, step 67009, time: 842.5180487632751 s, accu: 0.9692726135253906, loss_yt: 0.04939807951450348\n",
      "epocht 0, batch_num 8000, step 67209, time: 863.9421105384827 s, accu: 0.9692772030830383, loss_yt: 0.08745500445365906\n",
      "epocht 0, batch_num 8200, step 67409, time: 885.3420875072479 s, accu: 0.9692796468734741, loss_yt: 0.0710436999797821\n",
      "epocht 0, batch_num 8400, step 67609, time: 906.5129187107086 s, accu: 0.9692814350128174, loss_yt: 0.06560299545526505\n",
      "epocht 0, batch_num 8600, step 67809, time: 927.6743996143341 s, accu: 0.9692805409431458, loss_yt: 0.09618198126554489\n",
      "epocht 0, batch_num 8800, step 68009, time: 948.7497997283936 s, accu: 0.9692826867103577, loss_yt: 0.10258587449789047\n",
      "epocht 0, batch_num 9000, step 68209, time: 970.0588092803955 s, accu: 0.9692872762680054, loss_yt: 0.069358691573143\n",
      "epocht 0, batch_num 9200, step 68409, time: 991.5270667076111 s, accu: 0.9692881107330322, loss_yt: 0.04706992954015732\n",
      "epocht 0, batch_num 9400, step 68609, time: 1012.8005499839783 s, accu: 0.9692919254302979, loss_yt: 0.0678408294916153\n",
      "epocht 0, batch_num 9600, step 68809, time: 1034.3102791309357 s, accu: 0.9692953824996948, loss_yt: 0.10090455412864685\n",
      "epocht 0, batch_num 9800, step 69009, time: 1055.8367428779602 s, accu: 0.9692973494529724, loss_yt: 0.03589070215821266\n",
      "epocht 0, batch_num 10000, step 69209, time: 1077.2842190265656 s, accu: 0.9693008661270142, loss_yt: 0.0553269200026989\n",
      "epocht 0, batch_num 10200, step 69409, time: 1098.6374621391296 s, accu: 0.969302773475647, loss_yt: 0.07379428297281265\n",
      "epocht 0, batch_num 10400, step 69609, time: 1119.9926147460938 s, accu: 0.9693074226379395, loss_yt: 0.04748997092247009\n",
      "epocht 0, batch_num 10600, step 69809, time: 1141.3442373275757 s, accu: 0.9693081378936768, loss_yt: 0.08785214275121689\n",
      "epocht 0, batch_num 10800, step 70009, time: 1162.8685183525085 s, accu: 0.9693099856376648, loss_yt: 0.06568004190921783\n",
      "epocht 0, batch_num 11000, step 70209, time: 1184.2503125667572 s, accu: 0.9693136215209961, loss_yt: 0.077324777841568\n",
      "epocht 0, batch_num 11200, step 70409, time: 1205.6759145259857 s, accu: 0.9693168997764587, loss_yt: 0.07718876749277115\n",
      "epocht 0, batch_num 11400, step 70609, time: 1227.0908885002136 s, accu: 0.9693196415901184, loss_yt: 0.041481997817754745\n",
      "epocht 0, batch_num 11600, step 70809, time: 1248.509598016739 s, accu: 0.9693215489387512, loss_yt: 0.11024904251098633\n",
      "epocht 0, batch_num 11800, step 71009, time: 1270.0235016345978 s, accu: 0.9693242907524109, loss_yt: 0.06362974643707275\n",
      "epocht 0, batch_num 12000, step 71209, time: 1291.3860273361206 s, accu: 0.9693264961242676, loss_yt: 0.07117798179388046\n",
      "epocht 0, batch_num 12200, step 71409, time: 1312.6926872730255 s, accu: 0.9693282842636108, loss_yt: 0.09863601624965668\n",
      "epocht 0, batch_num 12400, step 71609, time: 1334.1425173282623 s, accu: 0.9693326354026794, loss_yt: 0.06829117238521576\n",
      "epocht 0, batch_num 12600, step 71809, time: 1355.3685178756714 s, accu: 0.9693375825881958, loss_yt: 0.080161452293396\n",
      "epocht 0, batch_num 12800, step 72009, time: 1376.908861398697 s, accu: 0.9693382382392883, loss_yt: 0.07606722414493561\n",
      "epocht 0, batch_num 13000, step 72209, time: 1398.2845706939697 s, accu: 0.9693409204483032, loss_yt: 0.060056619346141815\n",
      "epocht 0, batch_num 13200, step 72409, time: 1419.7186026573181 s, accu: 0.9693436622619629, loss_yt: 0.044206276535987854\n",
      "epocht 0, batch_num 13400, step 72609, time: 1441.124496936798 s, accu: 0.9693458080291748, loss_yt: 0.07619468867778778\n",
      "epocht 0, batch_num 13600, step 72809, time: 1462.3103413581848 s, accu: 0.9693484306335449, loss_yt: 0.08087322860956192\n",
      "epocht 0, batch_num 13800, step 73009, time: 1483.65917634964 s, accu: 0.9693526029586792, loss_yt: 0.10533781349658966\n",
      "epocht 0, batch_num 14000, step 73209, time: 1505.1248574256897 s, accu: 0.9693567156791687, loss_yt: 0.06293436884880066\n",
      "epocht 0, batch_num 14200, step 73409, time: 1526.568238735199 s, accu: 0.9693618416786194, loss_yt: 0.044640809297561646\n",
      "epocht 0, batch_num 14400, step 73609, time: 1547.8967475891113 s, accu: 0.9693626761436462, loss_yt: 0.07150030136108398\n",
      "epocht 0, batch_num 14600, step 73809, time: 1569.321295261383 s, accu: 0.9693650603294373, loss_yt: 0.08871039003133774\n",
      "epocht 0, batch_num 14800, step 74009, time: 1590.797613143921 s, accu: 0.9693672060966492, loss_yt: 0.12019875645637512\n",
      "iter_validnum 3701\n",
      "epochv 0, step 74010, stop_n 1, time: 1631.721708536148 s, accu_va: 0.9693999691320027, loss_yv: 0.07476811827092726\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 74011, time: 11.098491430282593 s, accu: 0.9694264531135559, loss_yt: 0.07779043912887573\n",
      "epocht 1, batch_num 200, step 74211, time: 32.18287944793701 s, accu: 0.9694307446479797, loss_yt: 0.08870160579681396\n",
      "epocht 1, batch_num 400, step 74411, time: 53.45639729499817 s, accu: 0.9694344997406006, loss_yt: 0.10825991630554199\n",
      "epocht 1, batch_num 600, step 74611, time: 74.58989667892456 s, accu: 0.9694360494613647, loss_yt: 0.07648209482431412\n",
      "epocht 1, batch_num 800, step 74811, time: 95.91107535362244 s, accu: 0.9694364666938782, loss_yt: 0.08292561769485474\n",
      "epocht 1, batch_num 1000, step 75011, time: 117.179452419281 s, accu: 0.9694390892982483, loss_yt: 0.1149146631360054\n",
      "epocht 1, batch_num 1200, step 75211, time: 138.41968894004822 s, accu: 0.9694393873214722, loss_yt: 0.09276962280273438\n",
      "epocht 1, batch_num 1400, step 75411, time: 159.7413649559021 s, accu: 0.9694420099258423, loss_yt: 0.12745153903961182\n",
      "epocht 1, batch_num 1600, step 75611, time: 181.00309348106384 s, accu: 0.9694433212280273, loss_yt: 0.0858016088604927\n",
      "epocht 1, batch_num 1800, step 75811, time: 202.4197919368744 s, accu: 0.9694468975067139, loss_yt: 0.09004592150449753\n",
      "epocht 1, batch_num 2000, step 76011, time: 223.66053867340088 s, accu: 0.9694485664367676, loss_yt: 0.1019744798541069\n",
      "epocht 1, batch_num 2200, step 76211, time: 245.0661005973816 s, accu: 0.9694501161575317, loss_yt: 0.0887117087841034\n",
      "epocht 1, batch_num 2400, step 76411, time: 266.5019733905792 s, accu: 0.9694533348083496, loss_yt: 0.0757402777671814\n",
      "epocht 1, batch_num 2600, step 76611, time: 287.86467814445496 s, accu: 0.9694564938545227, loss_yt: 0.08592245727777481\n",
      "epocht 1, batch_num 2800, step 76811, time: 309.0906431674957 s, accu: 0.9694611430168152, loss_yt: 0.11933273077011108\n",
      "epocht 1, batch_num 3000, step 77011, time: 330.2828423976898 s, accu: 0.9694639444351196, loss_yt: 0.06866549700498581\n",
      "epocht 1, batch_num 3200, step 77211, time: 351.672012090683 s, accu: 0.9694656133651733, loss_yt: 0.062322817742824554\n",
      "epocht 1, batch_num 3400, step 77411, time: 372.9067575931549 s, accu: 0.9694687724113464, loss_yt: 0.07117059826850891\n",
      "epocht 1, batch_num 3600, step 77611, time: 394.1217579841614 s, accu: 0.9694709181785583, loss_yt: 0.11461570113897324\n",
      "epocht 1, batch_num 3800, step 77811, time: 415.5514543056488 s, accu: 0.9694735407829285, loss_yt: 0.08886163681745529\n",
      "epocht 1, batch_num 4000, step 78011, time: 436.7950689792633 s, accu: 0.9694762229919434, loss_yt: 0.059443749487400055\n",
      "epocht 1, batch_num 4200, step 78211, time: 458.15796160697937 s, accu: 0.9694775938987732, loss_yt: 0.08911614865064621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 4400, step 78411, time: 479.4650330543518 s, accu: 0.9694818258285522, loss_yt: 0.08494489639997482\n",
      "epocht 1, batch_num 4600, step 78611, time: 500.8010940551758 s, accu: 0.9694856405258179, loss_yt: 0.0753852128982544\n",
      "epocht 1, batch_num 4800, step 78811, time: 522.1472065448761 s, accu: 0.9694899320602417, loss_yt: 0.0645832046866417\n",
      "epocht 1, batch_num 5000, step 79011, time: 543.6200969219208 s, accu: 0.9694927334785461, loss_yt: 0.06969063729047775\n",
      "epocht 1, batch_num 5200, step 79211, time: 564.8051416873932 s, accu: 0.9694957733154297, loss_yt: 0.08356338739395142\n",
      "epocht 1, batch_num 5400, step 79411, time: 586.3106949329376 s, accu: 0.9694993495941162, loss_yt: 0.059580300003290176\n",
      "epocht 1, batch_num 5600, step 79611, time: 607.5312304496765 s, accu: 0.9695007801055908, loss_yt: 0.054485343396663666\n",
      "epocht 1, batch_num 5800, step 79811, time: 628.7691965103149 s, accu: 0.9695018529891968, loss_yt: 0.0679406002163887\n",
      "epocht 1, batch_num 6000, step 80011, time: 650.0502064228058 s, accu: 0.9695034027099609, loss_yt: 0.05205920711159706\n",
      "epocht 1, batch_num 6200, step 80211, time: 671.3700089454651 s, accu: 0.9695084691047668, loss_yt: 0.07272186875343323\n",
      "epocht 1, batch_num 6400, step 80411, time: 692.8184208869934 s, accu: 0.9695111513137817, loss_yt: 0.07135893404483795\n",
      "epocht 1, batch_num 6600, step 80611, time: 714.5050754547119 s, accu: 0.9695138931274414, loss_yt: 0.06164192035794258\n",
      "epocht 1, batch_num 6800, step 80811, time: 735.9398486614227 s, accu: 0.9695153832435608, loss_yt: 0.07769659906625748\n",
      "epocht 1, batch_num 7000, step 81011, time: 757.2415187358856 s, accu: 0.9695159196853638, loss_yt: 0.09972386062145233\n",
      "epocht 1, batch_num 7200, step 81211, time: 778.3912763595581 s, accu: 0.9695190787315369, loss_yt: 0.07118687778711319\n",
      "epocht 1, batch_num 7400, step 81411, time: 799.5896010398865 s, accu: 0.9695233106613159, loss_yt: 0.05642611160874367\n",
      "epocht 1, batch_num 7600, step 81611, time: 820.9932019710541 s, accu: 0.9695281386375427, loss_yt: 0.07753193378448486\n",
      "epocht 1, batch_num 7800, step 81811, time: 842.2045197486877 s, accu: 0.9695312976837158, loss_yt: 0.10738266259431839\n",
      "epocht 1, batch_num 8000, step 82011, time: 863.4533452987671 s, accu: 0.9695330858230591, loss_yt: 0.06319896131753922\n",
      "epocht 1, batch_num 8200, step 82211, time: 884.8978662490845 s, accu: 0.9695373177528381, loss_yt: 0.06729745864868164\n",
      "epocht 1, batch_num 8400, step 82411, time: 906.4538402557373 s, accu: 0.9695395231246948, loss_yt: 0.06632105261087418\n",
      "epocht 1, batch_num 8600, step 82611, time: 927.8139498233795 s, accu: 0.9695417284965515, loss_yt: 0.06755834072828293\n",
      "epocht 1, batch_num 8800, step 82811, time: 949.1806766986847 s, accu: 0.969543993473053, loss_yt: 0.06121734529733658\n",
      "epocht 1, batch_num 9000, step 83011, time: 970.4058575630188 s, accu: 0.9695457220077515, loss_yt: 0.0765417292714119\n",
      "epocht 1, batch_num 9200, step 83211, time: 991.8071339130402 s, accu: 0.9695479273796082, loss_yt: 0.09830066561698914\n",
      "epocht 1, batch_num 9400, step 83411, time: 1012.9986114501953 s, accu: 0.9695502519607544, loss_yt: 0.06873852759599686\n",
      "epocht 1, batch_num 9600, step 83611, time: 1034.2108256816864 s, accu: 0.9695525169372559, loss_yt: 0.08827218413352966\n",
      "epocht 1, batch_num 9800, step 83811, time: 1055.7006030082703 s, accu: 0.9695543646812439, loss_yt: 0.06070064753293991\n",
      "epocht 1, batch_num 10000, step 84011, time: 1076.9847691059113 s, accu: 0.9695566892623901, loss_yt: 0.06613752990961075\n",
      "epocht 1, batch_num 10200, step 84211, time: 1098.306500196457 s, accu: 0.9695618748664856, loss_yt: 0.09382452070713043\n",
      "epocht 1, batch_num 10400, step 84411, time: 1119.5645062923431 s, accu: 0.9695640802383423, loss_yt: 0.1098526120185852\n",
      "epocht 1, batch_num 10600, step 84611, time: 1140.8044564723969 s, accu: 0.9695649743080139, loss_yt: 0.08006934821605682\n",
      "epocht 1, batch_num 10800, step 84811, time: 1162.1765694618225 s, accu: 0.969565749168396, loss_yt: 0.09240251779556274\n",
      "epocht 1, batch_num 11000, step 85011, time: 1183.4840369224548 s, accu: 0.9695677161216736, loss_yt: 0.08555053174495697\n",
      "epocht 1, batch_num 11200, step 85211, time: 1204.9770708084106 s, accu: 0.9695701003074646, loss_yt: 0.04143548756837845\n",
      "epocht 1, batch_num 11400, step 85411, time: 1226.3205389976501 s, accu: 0.9695706367492676, loss_yt: 0.08051864057779312\n",
      "epocht 1, batch_num 11600, step 85611, time: 1247.6698107719421 s, accu: 0.9695748090744019, loss_yt: 0.07281304150819778\n",
      "epocht 1, batch_num 11800, step 85811, time: 1268.92636013031 s, accu: 0.9695769548416138, loss_yt: 0.09498587250709534\n",
      "epocht 1, batch_num 12000, step 86011, time: 1290.258324623108 s, accu: 0.9695777297019958, loss_yt: 0.08380838483572006\n",
      "epocht 1, batch_num 12200, step 86211, time: 1311.6247425079346 s, accu: 0.9695789813995361, loss_yt: 0.07562841475009918\n",
      "epocht 1, batch_num 12400, step 86411, time: 1332.9931881427765 s, accu: 0.9695802330970764, loss_yt: 0.07900222390890121\n",
      "epocht 1, batch_num 12600, step 86611, time: 1354.350185394287 s, accu: 0.9695835709571838, loss_yt: 0.0781213790178299\n",
      "epocht 1, batch_num 12800, step 86811, time: 1375.5239131450653 s, accu: 0.9695872664451599, loss_yt: 0.07404887676239014\n",
      "epocht 1, batch_num 13000, step 87011, time: 1396.9523310661316 s, accu: 0.9695910215377808, loss_yt: 0.055159732699394226\n",
      "epocht 1, batch_num 13200, step 87211, time: 1418.2864305973053 s, accu: 0.9695942401885986, loss_yt: 0.04806658253073692\n",
      "epocht 1, batch_num 13400, step 87411, time: 1439.5279092788696 s, accu: 0.9695950746536255, loss_yt: 0.08514051139354706\n",
      "epocht 1, batch_num 13600, step 87611, time: 1460.891901731491 s, accu: 0.969595730304718, loss_yt: 0.059769194573163986\n",
      "epocht 1, batch_num 13800, step 87811, time: 1482.383953332901 s, accu: 0.9695960283279419, loss_yt: 0.08045272529125214\n",
      "epocht 1, batch_num 14000, step 88011, time: 1503.7804589271545 s, accu: 0.9695999026298523, loss_yt: 0.07475391030311584\n",
      "epocht 1, batch_num 14200, step 88211, time: 1525.1156961917877 s, accu: 0.9696016311645508, loss_yt: 0.09092431515455246\n",
      "epocht 1, batch_num 14400, step 88411, time: 1546.5651330947876 s, accu: 0.9696044325828552, loss_yt: 0.07105021178722382\n",
      "epocht 1, batch_num 14600, step 88611, time: 1568.0861730575562 s, accu: 0.9696074724197388, loss_yt: 0.07792770862579346\n",
      "epocht 1, batch_num 14800, step 88811, time: 1589.4455993175507 s, accu: 0.9696098566055298, loss_yt: 0.07077548652887344\n",
      "iter_validnum 3701\n",
      "epochv 1, step 88812, stop_n 0, time: 1629.7850770950317 s, accu_va: 0.9696335675458075, loss_yv: 0.07472191094522765\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 88813, time: 11.02400517463684 s, accu: 0.96965491771698, loss_yt: 0.07786861062049866\n",
      "epocht 1, batch_num 200, step 89013, time: 32.43179988861084 s, accu: 0.9696582555770874, loss_yt: 0.0845700353384018\n",
      "epocht 1, batch_num 400, step 89213, time: 53.81082272529602 s, accu: 0.9696608781814575, loss_yt: 0.05975368618965149\n",
      "epocht 1, batch_num 600, step 89413, time: 75.0497841835022 s, accu: 0.9696643948554993, loss_yt: 0.0798758864402771\n",
      "epocht 1, batch_num 800, step 89613, time: 96.38925337791443 s, accu: 0.9696676135063171, loss_yt: 0.06974023580551147\n",
      "epocht 1, batch_num 1000, step 89813, time: 117.6298508644104 s, accu: 0.9696702361106873, loss_yt: 0.11219016462564468\n",
      "epocht 1, batch_num 1200, step 90013, time: 138.89315223693848 s, accu: 0.9696726202964783, loss_yt: 0.08582206070423126\n",
      "epocht 1, batch_num 1400, step 90213, time: 160.1296992301941 s, accu: 0.9696750044822693, loss_yt: 0.0746380016207695\n",
      "epocht 1, batch_num 1600, step 90413, time: 181.6148328781128 s, accu: 0.9696776866912842, loss_yt: 0.08488213270902634\n",
      "epocht 1, batch_num 1800, step 90613, time: 203.0401258468628 s, accu: 0.9696806073188782, loss_yt: 0.09284798800945282\n",
      "epocht 1, batch_num 2000, step 90813, time: 224.49609065055847 s, accu: 0.9696820974349976, loss_yt: 0.03844590485095978\n",
      "epocht 1, batch_num 2200, step 91013, time: 245.83874940872192 s, accu: 0.9696836471557617, loss_yt: 0.09955612570047379\n",
      "epocht 1, batch_num 2400, step 91213, time: 267.1755542755127 s, accu: 0.9696862101554871, loss_yt: 0.10172945261001587\n",
      "epocht 1, batch_num 2600, step 91413, time: 288.59669184684753 s, accu: 0.9696880578994751, loss_yt: 0.06252995133399963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 2800, step 91613, time: 309.80898785591125 s, accu: 0.9696901440620422, loss_yt: 0.06389661878347397\n",
      "epocht 1, batch_num 3000, step 91813, time: 331.1180944442749 s, accu: 0.9696928858757019, loss_yt: 0.061488524079322815\n",
      "epocht 1, batch_num 3200, step 92013, time: 352.34618735313416 s, accu: 0.9696969389915466, loss_yt: 0.08725467324256897\n",
      "epocht 1, batch_num 3400, step 92213, time: 373.53611159324646 s, accu: 0.9696992635726929, loss_yt: 0.05616200715303421\n",
      "epocht 1, batch_num 3600, step 92413, time: 394.67464232444763 s, accu: 0.9697026014328003, loss_yt: 0.13188454508781433\n",
      "epocht 1, batch_num 3800, step 92613, time: 415.85232400894165 s, accu: 0.9697052836418152, loss_yt: 0.08958476036787033\n",
      "epocht 1, batch_num 4000, step 92813, time: 437.24120140075684 s, accu: 0.969706118106842, loss_yt: 0.07258918881416321\n",
      "epocht 1, batch_num 4200, step 93013, time: 458.5704185962677 s, accu: 0.9697082042694092, loss_yt: 0.044482871890068054\n",
      "epocht 1, batch_num 4400, step 93213, time: 479.96671748161316 s, accu: 0.9697108864784241, loss_yt: 0.06283295899629593\n",
      "epocht 1, batch_num 4600, step 93413, time: 501.3064708709717 s, accu: 0.969712495803833, loss_yt: 0.07455369830131531\n",
      "epocht 1, batch_num 4800, step 93613, time: 522.497407913208 s, accu: 0.9697147011756897, loss_yt: 0.08361195027828217\n",
      "epocht 1, batch_num 5000, step 93813, time: 543.8830623626709 s, accu: 0.9697167277336121, loss_yt: 0.07170718908309937\n",
      "epocht 1, batch_num 5200, step 94013, time: 565.2151799201965 s, accu: 0.9697173833847046, loss_yt: 0.08269266039133072\n",
      "epocht 1, batch_num 5400, step 94213, time: 586.4000461101532 s, accu: 0.9697191715240479, loss_yt: 0.07405778765678406\n",
      "epocht 1, batch_num 5600, step 94413, time: 607.6410949230194 s, accu: 0.9697214365005493, loss_yt: 0.06960658729076385\n",
      "epocht 1, batch_num 5800, step 94613, time: 628.8592159748077 s, accu: 0.9697233438491821, loss_yt: 0.05848192051053047\n",
      "epocht 1, batch_num 6000, step 94813, time: 650.0789818763733 s, accu: 0.9697270393371582, loss_yt: 0.06950138509273529\n",
      "epocht 1, batch_num 6200, step 95013, time: 671.3701910972595 s, accu: 0.9697288274765015, loss_yt: 0.07346419990062714\n",
      "epocht 1, batch_num 6400, step 95213, time: 692.5092394351959 s, accu: 0.9697302579879761, loss_yt: 0.04187293350696564\n",
      "epocht 1, batch_num 6600, step 95413, time: 713.767457485199 s, accu: 0.969733476638794, loss_yt: 0.057519227266311646\n",
      "epocht 1, batch_num 6800, step 95613, time: 734.9107139110565 s, accu: 0.9697348475456238, loss_yt: 0.06654179096221924\n",
      "epocht 1, batch_num 7000, step 95813, time: 756.24578332901 s, accu: 0.9697368144989014, loss_yt: 0.1267469823360443\n",
      "epocht 1, batch_num 7200, step 96013, time: 777.5305788516998 s, accu: 0.9697386622428894, loss_yt: 0.07365383207798004\n",
      "epocht 1, batch_num 7400, step 96213, time: 798.893194437027 s, accu: 0.9697403907775879, loss_yt: 0.0669756680727005\n",
      "epocht 1, batch_num 7600, step 96413, time: 820.2758719921112 s, accu: 0.9697430729866028, loss_yt: 0.09392181783914566\n",
      "epocht 1, batch_num 7800, step 96613, time: 841.4879817962646 s, accu: 0.9697445034980774, loss_yt: 0.08420858532190323\n",
      "epocht 1, batch_num 8000, step 96813, time: 862.6059453487396 s, accu: 0.9697450995445251, loss_yt: 0.07983316481113434\n",
      "epocht 1, batch_num 8200, step 97013, time: 884.0004336833954 s, accu: 0.9697484374046326, loss_yt: 0.06874695420265198\n",
      "epocht 1, batch_num 8400, step 97213, time: 905.2117376327515 s, accu: 0.9697515964508057, loss_yt: 0.06758254766464233\n",
      "epocht 1, batch_num 8600, step 97413, time: 926.4592962265015 s, accu: 0.9697533249855042, loss_yt: 0.08489502966403961\n",
      "epocht 1, batch_num 8800, step 97613, time: 947.751702785492 s, accu: 0.9697551727294922, loss_yt: 0.08651426434516907\n",
      "epocht 1, batch_num 9000, step 97813, time: 969.1830699443817 s, accu: 0.9697540402412415, loss_yt: 0.08162911981344223\n",
      "epocht 1, batch_num 9200, step 98013, time: 990.5199127197266 s, accu: 0.9697558283805847, loss_yt: 0.0816042423248291\n",
      "epocht 1, batch_num 9400, step 98213, time: 1011.6873559951782 s, accu: 0.9697574377059937, loss_yt: 0.04989973083138466\n",
      "epocht 1, batch_num 9600, step 98413, time: 1032.920562028885 s, accu: 0.9697589874267578, loss_yt: 0.07201548665761948\n",
      "epocht 1, batch_num 9800, step 98613, time: 1054.1842079162598 s, accu: 0.9697612524032593, loss_yt: 0.08005121350288391\n",
      "epocht 1, batch_num 10000, step 98813, time: 1075.5214176177979 s, accu: 0.9697638154029846, loss_yt: 0.045477695763111115\n",
      "epocht 1, batch_num 10200, step 99013, time: 1096.935833454132 s, accu: 0.9697644114494324, loss_yt: 0.08628923445940018\n",
      "epocht 1, batch_num 10400, step 99213, time: 1118.3197407722473 s, accu: 0.9697641730308533, loss_yt: 0.0712171196937561\n",
      "epocht 1, batch_num 10600, step 99413, time: 1139.4779801368713 s, accu: 0.9697660207748413, loss_yt: 0.07666086405515671\n",
      "epocht 1, batch_num 10800, step 99613, time: 1160.7144620418549 s, accu: 0.9697681665420532, loss_yt: 0.08118492364883423\n",
      "epocht 1, batch_num 11000, step 99813, time: 1182.0835537910461 s, accu: 0.9697701334953308, loss_yt: 0.09477599710226059\n",
      "epocht 1, batch_num 11200, step 100013, time: 1203.4191269874573 s, accu: 0.9697723388671875, loss_yt: 0.08160417526960373\n",
      "epocht 1, batch_num 11400, step 100213, time: 1224.6266117095947 s, accu: 0.9697760343551636, loss_yt: 0.04025690257549286\n",
      "epocht 1, batch_num 11600, step 100413, time: 1245.8135001659393 s, accu: 0.9697774648666382, loss_yt: 0.09251990169286728\n",
      "epocht 1, batch_num 11800, step 100613, time: 1267.1295132637024 s, accu: 0.9697790741920471, loss_yt: 0.1250108927488327\n",
      "epocht 1, batch_num 12000, step 100813, time: 1288.283227443695 s, accu: 0.9697819948196411, loss_yt: 0.09703744947910309\n",
      "epocht 1, batch_num 12200, step 101013, time: 1309.534565448761 s, accu: 0.9697840809822083, loss_yt: 0.08007975667715073\n",
      "epocht 1, batch_num 12400, step 101213, time: 1330.8892986774445 s, accu: 0.969785213470459, loss_yt: 0.07299131155014038\n",
      "epocht 1, batch_num 12600, step 101413, time: 1352.4386036396027 s, accu: 0.9697864651679993, loss_yt: 0.09417793899774551\n",
      "epocht 1, batch_num 12800, step 101613, time: 1373.6550233364105 s, accu: 0.9697892665863037, loss_yt: 0.08923421800136566\n",
      "epocht 1, batch_num 13000, step 101813, time: 1395.0071339607239 s, accu: 0.9697897434234619, loss_yt: 0.051438331604003906\n",
      "epocht 1, batch_num 13200, step 102013, time: 1416.3553733825684 s, accu: 0.9697926640510559, loss_yt: 0.11694933474063873\n",
      "epocht 1, batch_num 13400, step 102213, time: 1437.7185077667236 s, accu: 0.9697955846786499, loss_yt: 0.098579540848732\n",
      "epocht 1, batch_num 13600, step 102413, time: 1458.973242521286 s, accu: 0.9697962999343872, loss_yt: 0.07074678689241409\n",
      "epocht 1, batch_num 13800, step 102613, time: 1480.3378944396973 s, accu: 0.9697974920272827, loss_yt: 0.06779946386814117\n",
      "epocht 1, batch_num 14000, step 102813, time: 1501.5539212226868 s, accu: 0.9698010087013245, loss_yt: 0.08010639995336533\n",
      "epocht 1, batch_num 14200, step 103013, time: 1522.7152948379517 s, accu: 0.9698024392127991, loss_yt: 0.06598014384508133\n",
      "epocht 1, batch_num 14400, step 103213, time: 1544.1637337207794 s, accu: 0.9698057770729065, loss_yt: 0.06774813681840897\n",
      "epocht 1, batch_num 14600, step 103413, time: 1565.5123612880707 s, accu: 0.9698059558868408, loss_yt: 0.08079160749912262\n",
      "epocht 1, batch_num 14800, step 103613, time: 1586.875412940979 s, accu: 0.9698066711425781, loss_yt: 0.0777188390493393\n",
      "iter_validnum 3701\n",
      "epochv 1, step 103614, stop_n 0, time: 1627.9943680763245 s, accu_va: 0.9698206271812163, loss_yv: 0.07541161485950437\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 103615, time: 11.043845176696777 s, accu: 0.969829261302948, loss_yt: 0.08066115528345108\n",
      "epocht 1, batch_num 200, step 103815, time: 32.232524394989014 s, accu: 0.9698315858840942, loss_yt: 0.06524863094091415\n",
      "epocht 1, batch_num 400, step 104015, time: 53.4746515750885 s, accu: 0.9698336124420166, loss_yt: 0.0754571184515953\n",
      "epocht 1, batch_num 600, step 104215, time: 74.9183521270752 s, accu: 0.9698375463485718, loss_yt: 0.11700054258108139\n",
      "epocht 1, batch_num 800, step 104415, time: 96.30283975601196 s, accu: 0.9698390364646912, loss_yt: 0.06782828271389008\n",
      "epocht 1, batch_num 1000, step 104615, time: 117.6263632774353 s, accu: 0.9698403477668762, loss_yt: 0.08258567750453949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 1200, step 104815, time: 138.8865988254547 s, accu: 0.9698426723480225, loss_yt: 0.0835166722536087\n",
      "epocht 1, batch_num 1400, step 105015, time: 160.16549849510193 s, accu: 0.9698438048362732, loss_yt: 0.0946819931268692\n",
      "epocht 1, batch_num 1600, step 105215, time: 181.2698950767517 s, accu: 0.9698436260223389, loss_yt: 0.058861009776592255\n",
      "epocht 1, batch_num 1800, step 105415, time: 202.3261661529541 s, accu: 0.9698455333709717, loss_yt: 0.07622409611940384\n",
      "epocht 1, batch_num 2000, step 105615, time: 223.6328582763672 s, accu: 0.9698461294174194, loss_yt: 0.055323828011751175\n",
      "epocht 1, batch_num 2200, step 105815, time: 244.93664383888245 s, accu: 0.9698503613471985, loss_yt: 0.10266454517841339\n",
      "epocht 1, batch_num 2400, step 106015, time: 266.36053371429443 s, accu: 0.9698532223701477, loss_yt: 0.10543252527713776\n",
      "epocht 1, batch_num 2600, step 106215, time: 287.6030042171478 s, accu: 0.969856321811676, loss_yt: 0.06078913062810898\n",
      "epocht 1, batch_num 2800, step 106415, time: 308.89122462272644 s, accu: 0.9698588252067566, loss_yt: 0.06727685779333115\n",
      "epocht 1, batch_num 3000, step 106615, time: 330.0552079677582 s, accu: 0.9698598384857178, loss_yt: 0.10646598041057587\n",
      "epocht 1, batch_num 3200, step 106815, time: 351.38901233673096 s, accu: 0.9698610305786133, loss_yt: 0.06690330803394318\n",
      "epocht 1, batch_num 3400, step 107015, time: 372.5839834213257 s, accu: 0.96986323595047, loss_yt: 0.08561059087514877\n",
      "epocht 1, batch_num 3600, step 107215, time: 393.7644913196564 s, accu: 0.9698647260665894, loss_yt: 0.08590606600046158\n",
      "epocht 1, batch_num 3800, step 107415, time: 415.31052827835083 s, accu: 0.9698682427406311, loss_yt: 0.06944464892148972\n",
      "epocht 1, batch_num 4000, step 107615, time: 436.50623393058777 s, accu: 0.9698702692985535, loss_yt: 0.058733269572257996\n",
      "epocht 1, batch_num 4200, step 107815, time: 457.78000140190125 s, accu: 0.9698718786239624, loss_yt: 0.07477454841136932\n",
      "epocht 1, batch_num 4400, step 108015, time: 479.11036562919617 s, accu: 0.969872772693634, loss_yt: 0.08172775059938431\n",
      "epocht 1, batch_num 4600, step 108215, time: 500.61775279045105 s, accu: 0.9698748588562012, loss_yt: 0.09282631427049637\n",
      "epocht 1, batch_num 4800, step 108415, time: 521.9126465320587 s, accu: 0.9698778986930847, loss_yt: 0.06576520204544067\n",
      "epocht 1, batch_num 5000, step 108615, time: 543.0979526042938 s, accu: 0.9698795080184937, loss_yt: 0.06929126381874084\n",
      "epocht 1, batch_num 5200, step 108815, time: 564.3918130397797 s, accu: 0.9698798060417175, loss_yt: 0.08878566324710846\n",
      "epocht 1, batch_num 5400, step 109015, time: 585.6124312877655 s, accu: 0.9698808789253235, loss_yt: 0.08039626479148865\n",
      "epocht 1, batch_num 5600, step 109215, time: 607.1328649520874 s, accu: 0.9698839783668518, loss_yt: 0.07000739872455597\n",
      "epocht 1, batch_num 5800, step 109415, time: 628.4931085109711 s, accu: 0.9698859453201294, loss_yt: 0.05894939973950386\n",
      "epocht 1, batch_num 6000, step 109615, time: 649.9103829860687 s, accu: 0.9698874950408936, loss_yt: 0.08710509538650513\n",
      "epocht 1, batch_num 6200, step 109815, time: 670.4060096740723 s, accu: 0.9698892831802368, loss_yt: 0.09268394112586975\n",
      "epocht 1, batch_num 6400, step 110015, time: 691.4475882053375 s, accu: 0.9698918461799622, loss_yt: 0.04554322361946106\n",
      "epocht 1, batch_num 6600, step 110215, time: 712.7103128433228 s, accu: 0.9698920249938965, loss_yt: 0.0562993586063385\n",
      "epocht 1, batch_num 6800, step 110415, time: 733.8907387256622 s, accu: 0.9698935747146606, loss_yt: 0.08441995829343796\n",
      "epocht 1, batch_num 7000, step 110615, time: 755.2176768779755 s, accu: 0.9698958992958069, loss_yt: 0.048338424414396286\n",
      "epocht 1, batch_num 7200, step 110815, time: 776.5025260448456 s, accu: 0.9698984622955322, loss_yt: 0.07641523331403732\n",
      "epocht 1, batch_num 7400, step 111015, time: 797.9186625480652 s, accu: 0.9699005484580994, loss_yt: 0.09028232097625732\n",
      "epocht 1, batch_num 7600, step 111215, time: 819.125899553299 s, accu: 0.9699026346206665, loss_yt: 0.07757018506526947\n",
      "epocht 1, batch_num 7800, step 111415, time: 840.6340487003326 s, accu: 0.9699031114578247, loss_yt: 0.07751129567623138\n",
      "epocht 1, batch_num 8000, step 111615, time: 861.9375095367432 s, accu: 0.9699054956436157, loss_yt: 0.09637260437011719\n",
      "epocht 1, batch_num 8200, step 111815, time: 883.2332172393799 s, accu: 0.9699074029922485, loss_yt: 0.09538073092699051\n",
      "epocht 1, batch_num 8400, step 112015, time: 904.6416118144989 s, accu: 0.9699081778526306, loss_yt: 0.06930162012577057\n",
      "epocht 1, batch_num 8600, step 112215, time: 925.7960481643677 s, accu: 0.9699094295501709, loss_yt: 0.0639907568693161\n",
      "epocht 1, batch_num 8800, step 112415, time: 947.0932989120483 s, accu: 0.9699112176895142, loss_yt: 0.05156425014138222\n",
      "epocht 1, batch_num 9000, step 112615, time: 968.4039444923401 s, accu: 0.9699123501777649, loss_yt: 0.09990762174129486\n",
      "epocht 1, batch_num 9200, step 112815, time: 989.7342429161072 s, accu: 0.9699127078056335, loss_yt: 0.07243421673774719\n",
      "epocht 1, batch_num 9400, step 113015, time: 1010.983993768692 s, accu: 0.9699137806892395, loss_yt: 0.09371928125619888\n",
      "epocht 1, batch_num 9600, step 113215, time: 1032.2102191448212 s, accu: 0.9699150919914246, loss_yt: 0.07235854864120483\n",
      "epocht 1, batch_num 9800, step 113415, time: 1053.3049404621124 s, accu: 0.9699167609214783, loss_yt: 0.10300871729850769\n",
      "epocht 1, batch_num 10000, step 113615, time: 1074.5177278518677 s, accu: 0.9699168801307678, loss_yt: 0.06996101886034012\n",
      "epocht 1, batch_num 10200, step 113815, time: 1095.8713858127594 s, accu: 0.9699181914329529, loss_yt: 0.08549875020980835\n",
      "epocht 1, batch_num 10400, step 114015, time: 1117.104521036148 s, accu: 0.9699192643165588, loss_yt: 0.08432839810848236\n",
      "epocht 1, batch_num 10600, step 114215, time: 1138.3291401863098 s, accu: 0.9699210524559021, loss_yt: 0.06453374028205872\n",
      "epocht 1, batch_num 10800, step 114415, time: 1159.441883802414 s, accu: 0.969923198223114, loss_yt: 0.09345049411058426\n",
      "epocht 1, batch_num 11000, step 114615, time: 1180.584504365921 s, accu: 0.9699247479438782, loss_yt: 0.08891652524471283\n",
      "epocht 1, batch_num 11200, step 114815, time: 1202.0803444385529 s, accu: 0.969926655292511, loss_yt: 0.057382527738809586\n",
      "epocht 1, batch_num 11400, step 115015, time: 1223.4772474765778 s, accu: 0.9699276685714722, loss_yt: 0.08200843632221222\n",
      "epocht 1, batch_num 11600, step 115215, time: 1244.6541693210602 s, accu: 0.969930112361908, loss_yt: 0.0712268054485321\n",
      "epocht 1, batch_num 11800, step 115415, time: 1265.9421319961548 s, accu: 0.9699329733848572, loss_yt: 0.0633934885263443\n",
      "epocht 1, batch_num 12000, step 115615, time: 1287.091180562973 s, accu: 0.9699349403381348, loss_yt: 0.05426272749900818\n",
      "epocht 1, batch_num 12200, step 115815, time: 1308.4001359939575 s, accu: 0.969936728477478, loss_yt: 0.050407569855451584\n",
      "epocht 1, batch_num 12400, step 116015, time: 1329.6866047382355 s, accu: 0.9699382781982422, loss_yt: 0.07096457481384277\n",
      "epocht 1, batch_num 12600, step 116215, time: 1351.058715581894 s, accu: 0.9699392914772034, loss_yt: 0.08631846308708191\n",
      "epocht 1, batch_num 12800, step 116415, time: 1372.3472378253937 s, accu: 0.9699404835700989, loss_yt: 0.07970689982175827\n",
      "epocht 1, batch_num 13000, step 116615, time: 1393.6987135410309 s, accu: 0.9699434041976929, loss_yt: 0.08322892338037491\n",
      "epocht 1, batch_num 13200, step 116815, time: 1414.7786266803741 s, accu: 0.9699446558952332, loss_yt: 0.07940293848514557\n",
      "epocht 1, batch_num 13400, step 117015, time: 1435.9899303913116 s, accu: 0.9699465036392212, loss_yt: 0.06695092469453812\n",
      "epocht 1, batch_num 13600, step 117215, time: 1457.1775493621826 s, accu: 0.9699490070343018, loss_yt: 0.08921796828508377\n",
      "epocht 1, batch_num 13800, step 117415, time: 1478.3162970542908 s, accu: 0.9699500203132629, loss_yt: 0.08508478105068207\n",
      "epocht 1, batch_num 14000, step 117615, time: 1499.5828232765198 s, accu: 0.9699516892433167, loss_yt: 0.07857691496610641\n",
      "epocht 1, batch_num 14200, step 117815, time: 1520.9220077991486 s, accu: 0.9699526429176331, loss_yt: 0.07766523957252502\n",
      "epocht 1, batch_num 14400, step 118015, time: 1542.3266088962555 s, accu: 0.9699550867080688, loss_yt: 0.10736610740423203\n",
      "epocht 1, batch_num 14600, step 118215, time: 1563.7215976715088 s, accu: 0.9699557423591614, loss_yt: 0.08543679118156433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 14800, step 118415, time: 1585.087466955185 s, accu: 0.969956636428833, loss_yt: 0.047116879373788834\n",
      "iter_validnum 3701\n",
      "epochv 1, step 118416, stop_n 0, time: 1625.90198636055 s, accu_va: 0.9699699153806093, loss_yv: 0.07410349741012413\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 118417, time: 11.027524471282959 s, accu: 0.9699834585189819, loss_yt: 0.06270038336515427\n",
      "epocht 1, batch_num 200, step 118617, time: 32.22470211982727 s, accu: 0.9699841141700745, loss_yt: 0.07754909247159958\n",
      "epocht 1, batch_num 400, step 118817, time: 53.52359342575073 s, accu: 0.9699851274490356, loss_yt: 0.07500527799129486\n",
      "epocht 1, batch_num 600, step 119017, time: 74.84120965003967 s, accu: 0.9699857831001282, loss_yt: 0.07022995501756668\n",
      "epocht 1, batch_num 800, step 119217, time: 96.22008776664734 s, accu: 0.9699886441230774, loss_yt: 0.10445628315210342\n",
      "epocht 1, batch_num 1000, step 119417, time: 117.48157811164856 s, accu: 0.969990074634552, loss_yt: 0.1162131130695343\n",
      "epocht 1, batch_num 1200, step 119617, time: 138.81874775886536 s, accu: 0.9699905514717102, loss_yt: 0.05057765543460846\n",
      "epocht 1, batch_num 1400, step 119817, time: 160.00208067893982 s, accu: 0.9699928164482117, loss_yt: 0.08262287080287933\n",
      "epocht 1, batch_num 1600, step 120017, time: 181.34338760375977 s, accu: 0.9699951410293579, loss_yt: 0.055774934589862823\n",
      "epocht 1, batch_num 1800, step 120217, time: 202.62285661697388 s, accu: 0.9699971675872803, loss_yt: 0.061764951795339584\n",
      "epocht 1, batch_num 2000, step 120417, time: 223.9288456439972 s, accu: 0.9699979424476624, loss_yt: 0.05970041826367378\n",
      "epocht 1, batch_num 2200, step 120617, time: 245.1074776649475 s, accu: 0.9700009226799011, loss_yt: 0.10473380982875824\n",
      "epocht 1, batch_num 2400, step 120817, time: 266.3095963001251 s, accu: 0.9700026512145996, loss_yt: 0.07850943505764008\n",
      "epocht 1, batch_num 2600, step 121017, time: 287.7273004055023 s, accu: 0.9700043201446533, loss_yt: 0.06274613738059998\n",
      "epocht 1, batch_num 2800, step 121217, time: 309.0182237625122 s, accu: 0.970005214214325, loss_yt: 0.05805489420890808\n",
      "epocht 1, batch_num 3000, step 121417, time: 330.3094699382782 s, accu: 0.9700074791908264, loss_yt: 0.07758544385433197\n",
      "epocht 1, batch_num 3200, step 121617, time: 351.6022894382477 s, accu: 0.9700104594230652, loss_yt: 0.08334576338529587\n",
      "epocht 1, batch_num 3400, step 121817, time: 372.97725009918213 s, accu: 0.9700124859809875, loss_yt: 0.06388910859823227\n",
      "epocht 1, batch_num 3600, step 122017, time: 394.0907258987427 s, accu: 0.9700141549110413, loss_yt: 0.064983069896698\n",
      "epocht 1, batch_num 3800, step 122217, time: 415.55121207237244 s, accu: 0.9700157642364502, loss_yt: 0.06916116923093796\n",
      "epocht 1, batch_num 4000, step 122417, time: 436.91979598999023 s, accu: 0.9700173139572144, loss_yt: 0.07179445773363113\n",
      "epocht 1, batch_num 4200, step 122617, time: 458.08355927467346 s, accu: 0.9700183868408203, loss_yt: 0.07700961083173752\n",
      "epocht 1, batch_num 4400, step 122817, time: 479.2393054962158 s, accu: 0.9700207710266113, loss_yt: 0.07421655207872391\n",
      "epocht 1, batch_num 4600, step 123017, time: 500.46707701683044 s, accu: 0.9700209498405457, loss_yt: 0.08013242483139038\n",
      "epocht 1, batch_num 4800, step 123217, time: 521.6380560398102 s, accu: 0.9700226783752441, loss_yt: 0.07054946571588516\n",
      "epocht 1, batch_num 5000, step 123417, time: 542.9124767780304 s, accu: 0.9700250029563904, loss_yt: 0.04159951210021973\n",
      "epocht 1, batch_num 5200, step 123617, time: 564.3092565536499 s, accu: 0.9700263142585754, loss_yt: 0.06402479112148285\n",
      "epocht 1, batch_num 5400, step 123817, time: 585.621077299118 s, accu: 0.9700279831886292, loss_yt: 0.05516861379146576\n",
      "epocht 1, batch_num 5600, step 124017, time: 606.9681503772736 s, accu: 0.9700295925140381, loss_yt: 0.07023842632770538\n",
      "epocht 1, batch_num 5800, step 124217, time: 628.4435386657715 s, accu: 0.9700309634208679, loss_yt: 0.0760231763124466\n",
      "epocht 1, batch_num 6000, step 124417, time: 649.7620470523834 s, accu: 0.9700331091880798, loss_yt: 0.0706709623336792\n",
      "epocht 1, batch_num 6200, step 124617, time: 671.1245684623718 s, accu: 0.9700337052345276, loss_yt: 0.05872548371553421\n",
      "epocht 1, batch_num 6400, step 124817, time: 692.5295729637146 s, accu: 0.9700339436531067, loss_yt: 0.07279892265796661\n",
      "epocht 1, batch_num 6600, step 125017, time: 713.8188045024872 s, accu: 0.97003573179245, loss_yt: 0.06854287534952164\n",
      "epocht 1, batch_num 6800, step 125217, time: 735.2556798458099 s, accu: 0.9700372219085693, loss_yt: 0.05829635262489319\n",
      "epocht 1, batch_num 7000, step 125417, time: 756.5219964981079 s, accu: 0.9700379371643066, loss_yt: 0.06922861188650131\n",
      "epocht 1, batch_num 7200, step 125617, time: 777.917332649231 s, accu: 0.9700391888618469, loss_yt: 0.06266243755817413\n",
      "epocht 1, batch_num 7400, step 125817, time: 799.3049273490906 s, accu: 0.9700406789779663, loss_yt: 0.0771632120013237\n",
      "epocht 1, batch_num 7600, step 126017, time: 820.666720867157 s, accu: 0.9700415134429932, loss_yt: 0.07107219845056534\n",
      "epocht 1, batch_num 7800, step 126217, time: 841.9466853141785 s, accu: 0.9700434803962708, loss_yt: 0.07618828117847443\n",
      "epocht 1, batch_num 8000, step 126417, time: 863.2177016735077 s, accu: 0.9700449705123901, loss_yt: 0.05711602047085762\n",
      "epocht 1, batch_num 8200, step 126617, time: 884.5154461860657 s, accu: 0.9700464010238647, loss_yt: 0.08808628469705582\n",
      "epocht 1, batch_num 8400, step 126817, time: 905.8919885158539 s, accu: 0.9700478911399841, loss_yt: 0.06912755966186523\n",
      "epocht 1, batch_num 8600, step 127017, time: 927.1140379905701 s, accu: 0.9700501561164856, loss_yt: 0.0732717365026474\n",
      "epocht 1, batch_num 8800, step 127217, time: 948.2629318237305 s, accu: 0.9700523614883423, loss_yt: 0.06695602089166641\n",
      "epocht 1, batch_num 9000, step 127417, time: 969.5487859249115 s, accu: 0.9700542092323303, loss_yt: 0.0747203603386879\n",
      "epocht 1, batch_num 9200, step 127617, time: 990.820315361023 s, accu: 0.9700557589530945, loss_yt: 0.039645709097385406\n",
      "epocht 1, batch_num 9400, step 127817, time: 1012.0968770980835 s, accu: 0.9700570106506348, loss_yt: 0.07551512867212296\n",
      "epocht 1, batch_num 9600, step 128017, time: 1033.515941143036 s, accu: 0.9700589776039124, loss_yt: 0.10951264202594757\n",
      "epocht 1, batch_num 9800, step 128217, time: 1054.6974029541016 s, accu: 0.9700590372085571, loss_yt: 0.08512407541275024\n",
      "epocht 1, batch_num 10000, step 128417, time: 1076.0862872600555 s, accu: 0.9700620770454407, loss_yt: 0.050528958439826965\n",
      "epocht 1, batch_num 10200, step 128617, time: 1097.2570729255676 s, accu: 0.9700626134872437, loss_yt: 0.07060343772172928\n",
      "epocht 1, batch_num 10400, step 128817, time: 1118.4539725780487 s, accu: 0.9700632691383362, loss_yt: 0.05421009659767151\n",
      "epocht 1, batch_num 10600, step 129017, time: 1139.6720542907715 s, accu: 0.970064103603363, loss_yt: 0.04779095947742462\n",
      "epocht 1, batch_num 10800, step 129217, time: 1161.0193610191345 s, accu: 0.9700639247894287, loss_yt: 0.12920886278152466\n",
      "epocht 1, batch_num 11000, step 129417, time: 1182.2431802749634 s, accu: 0.9700655341148376, loss_yt: 0.04852651059627533\n",
      "epocht 1, batch_num 11200, step 129617, time: 1203.5503523349762 s, accu: 0.9700676202774048, loss_yt: 0.08437928557395935\n",
      "epocht 1, batch_num 11400, step 129817, time: 1224.9272978305817 s, accu: 0.9700697064399719, loss_yt: 0.05991321802139282\n",
      "epocht 1, batch_num 11600, step 130017, time: 1246.0951595306396 s, accu: 0.9700710773468018, loss_yt: 0.050276439636945724\n",
      "epocht 1, batch_num 11800, step 130217, time: 1267.2792036533356 s, accu: 0.9700706005096436, loss_yt: 0.0804651752114296\n",
      "epocht 1, batch_num 12000, step 130417, time: 1288.6876089572906 s, accu: 0.9700714945793152, loss_yt: 0.09046586602926254\n",
      "epocht 1, batch_num 12200, step 130617, time: 1309.995144367218 s, accu: 0.9700714945793152, loss_yt: 0.120375856757164\n",
      "epocht 1, batch_num 12400, step 130817, time: 1331.2802493572235 s, accu: 0.9700734615325928, loss_yt: 0.07666736841201782\n",
      "epocht 1, batch_num 12600, step 131017, time: 1352.6093018054962 s, accu: 0.970073938369751, loss_yt: 0.11055365949869156\n",
      "epocht 1, batch_num 12800, step 131217, time: 1373.8491327762604 s, accu: 0.9700759053230286, loss_yt: 0.08135025948286057\n",
      "epocht 1, batch_num 13000, step 131417, time: 1395.092458486557 s, accu: 0.9700759649276733, loss_yt: 0.04699204862117767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 13200, step 131617, time: 1416.3823075294495 s, accu: 0.9700771570205688, loss_yt: 0.06308543682098389\n",
      "epocht 1, batch_num 13400, step 131817, time: 1437.503888130188 s, accu: 0.9700778126716614, loss_yt: 0.0615948811173439\n",
      "epocht 1, batch_num 13600, step 132017, time: 1458.7364177703857 s, accu: 0.9700784683227539, loss_yt: 0.06896355748176575\n",
      "epocht 1, batch_num 13800, step 132217, time: 1480.1422607898712 s, accu: 0.9700790643692017, loss_yt: 0.061184290796518326\n",
      "epocht 1, batch_num 14000, step 132417, time: 1501.5565197467804 s, accu: 0.9700798988342285, loss_yt: 0.06678321212530136\n",
      "epocht 1, batch_num 14200, step 132617, time: 1523.0823788642883 s, accu: 0.9700815081596375, loss_yt: 0.056143324822187424\n",
      "epocht 1, batch_num 14400, step 132817, time: 1544.6030368804932 s, accu: 0.9700834155082703, loss_yt: 0.07298373430967331\n",
      "epocht 1, batch_num 14600, step 133017, time: 1565.7845964431763 s, accu: 0.970085859298706, loss_yt: 0.08010836690664291\n",
      "epocht 1, batch_num 14800, step 133217, time: 1587.1948773860931 s, accu: 0.9700886607170105, loss_yt: 0.06966520100831985\n",
      "iter_validnum 3701\n",
      "epochv 1, step 133218, stop_n 1, time: 1628.6558582782745 s, accu_va: 0.9701010472740363, loss_yv: 0.07387367106915745\n",
      "iter_trainnum 14802\n",
      "epocht 1, batch_num 0, step 133219, time: 11.126409530639648 s, accu: 0.970113217830658, loss_yt: 0.07514618337154388\n",
      "epocht 1, batch_num 200, step 133419, time: 32.58006954193115 s, accu: 0.9701166152954102, loss_yt: 0.06493666023015976\n",
      "epocht 1, batch_num 400, step 133619, time: 53.82425260543823 s, accu: 0.9701173901557922, loss_yt: 0.06907733529806137\n",
      "epocht 1, batch_num 600, step 133819, time: 74.92339301109314 s, accu: 0.9701180458068848, loss_yt: 0.07859743386507034\n",
      "epocht 1, batch_num 800, step 134019, time: 96.32347345352173 s, accu: 0.9701183438301086, loss_yt: 0.09618905186653137\n",
      "epocht 1, batch_num 1000, step 134219, time: 117.69141101837158 s, accu: 0.9701198935508728, loss_yt: 0.06006848067045212\n",
      "epocht 1, batch_num 1200, step 134419, time: 139.05633068084717 s, accu: 0.9701209664344788, loss_yt: 0.06343653798103333\n",
      "epocht 1, batch_num 1400, step 134619, time: 160.40113306045532 s, accu: 0.9701228141784668, loss_yt: 0.09028364717960358\n",
      "epocht 1, batch_num 1600, step 134819, time: 181.9169750213623 s, accu: 0.9701237678527832, loss_yt: 0.08292242884635925\n",
      "epocht 1, batch_num 1800, step 135019, time: 203.24364471435547 s, accu: 0.9701240062713623, loss_yt: 0.07655531913042068\n",
      "epocht 1, batch_num 2000, step 135219, time: 224.65537476539612 s, accu: 0.9701252579689026, loss_yt: 0.08787797391414642\n",
      "epocht 1, batch_num 2200, step 135419, time: 245.99520015716553 s, accu: 0.9701272249221802, loss_yt: 0.08976555615663528\n",
      "epocht 1, batch_num 2400, step 135619, time: 267.35141944885254 s, accu: 0.9701277017593384, loss_yt: 0.09214989840984344\n",
      "epocht 1, batch_num 2600, step 135819, time: 288.61745500564575 s, accu: 0.9701294302940369, loss_yt: 0.04398512840270996\n",
      "epocht 1, batch_num 2800, step 136019, time: 310.1108191013336 s, accu: 0.970131516456604, loss_yt: 0.0736137256026268\n",
      "epocht 1, batch_num 3000, step 136219, time: 331.4068806171417 s, accu: 0.9701330065727234, loss_yt: 0.08364977687597275\n",
      "epocht 1, batch_num 3200, step 136419, time: 352.8445131778717 s, accu: 0.9701340198516846, loss_yt: 0.06618030369281769\n",
      "epocht 1, batch_num 3400, step 136619, time: 374.1227488517761 s, accu: 0.9701352715492249, loss_yt: 0.04406813904643059\n",
      "epocht 1, batch_num 3600, step 136819, time: 395.31971549987793 s, accu: 0.970136284828186, loss_yt: 0.07178319245576859\n",
      "epocht 1, batch_num 3800, step 137019, time: 416.6291706562042 s, accu: 0.9701364636421204, loss_yt: 0.08316194266080856\n",
      "epocht 1, batch_num 4000, step 137219, time: 437.73842000961304 s, accu: 0.9701384902000427, loss_yt: 0.06160205975174904\n",
      "epocht 1, batch_num 4200, step 137419, time: 458.8563959598541 s, accu: 0.9701400399208069, loss_yt: 0.05677428096532822\n",
      "epocht 1, batch_num 4400, step 137619, time: 480.1674807071686 s, accu: 0.9701411724090576, loss_yt: 0.03598176687955856\n",
      "epocht 1, batch_num 4600, step 137819, time: 501.2957606315613 s, accu: 0.9701421856880188, loss_yt: 0.09777503460645676\n",
      "epocht 1, batch_num 4800, step 138019, time: 522.455084323883 s, accu: 0.9701434373855591, loss_yt: 0.06493864953517914\n",
      "epocht 1, batch_num 5000, step 138219, time: 543.688967704773 s, accu: 0.9701452851295471, loss_yt: 0.07377903163433075\n",
      "epocht 1, batch_num 5200, step 138419, time: 564.8121597766876 s, accu: 0.9701464772224426, loss_yt: 0.0850125253200531\n",
      "epocht 1, batch_num 5400, step 138619, time: 586.1725375652313 s, accu: 0.9701470136642456, loss_yt: 0.05784858763217926\n",
      "epocht 1, batch_num 5600, step 138819, time: 607.4364287853241 s, accu: 0.9701480269432068, loss_yt: 0.10704547166824341\n",
      "epocht 1, batch_num 5800, step 139019, time: 628.794513463974 s, accu: 0.9701490998268127, loss_yt: 0.09839563071727753\n",
      "epocht 1, batch_num 6000, step 139219, time: 650.1849784851074 s, accu: 0.9701489210128784, loss_yt: 0.07084003835916519\n",
      "epocht 1, batch_num 6200, step 139419, time: 671.3263218402863 s, accu: 0.9701501131057739, loss_yt: 0.07248982787132263\n",
      "epocht 1, batch_num 6400, step 139619, time: 692.5605947971344 s, accu: 0.97014981508255, loss_yt: 0.06407436728477478\n",
      "epocht 1, batch_num 6600, step 139819, time: 713.7001354694366 s, accu: 0.9701501727104187, loss_yt: 0.06538534164428711\n",
      "epocht 1, batch_num 6800, step 140019, time: 733.5038461685181 s, accu: 0.9701511859893799, loss_yt: 0.06650736927986145\n",
      "epocht 1, batch_num 7000, step 140219, time: 754.7551157474518 s, accu: 0.9701517224311829, loss_yt: 0.06815743446350098\n",
      "epocht 1, batch_num 7200, step 140419, time: 775.9594757556915 s, accu: 0.970152735710144, loss_yt: 0.08804665505886078\n",
      "epocht 1, batch_num 7400, step 140619, time: 797.1096324920654 s, accu: 0.9701547622680664, loss_yt: 0.04281320422887802\n",
      "epocht 1, batch_num 7600, step 140819, time: 818.4511117935181 s, accu: 0.9701560735702515, loss_yt: 0.04827526956796646\n",
      "epocht 1, batch_num 7800, step 141019, time: 839.5777766704559 s, accu: 0.9701566696166992, loss_yt: 0.07732029259204865\n",
      "epocht 1, batch_num 8000, step 141219, time: 861.01637840271 s, accu: 0.9701583385467529, loss_yt: 0.06559102982282639\n",
      "epocht 1, batch_num 8200, step 141419, time: 882.2283406257629 s, accu: 0.9701597094535828, loss_yt: 0.06714676320552826\n",
      "epocht 1, batch_num 8400, step 141619, time: 903.6291072368622 s, accu: 0.9701604247093201, loss_yt: 0.07557126879692078\n",
      "epocht 1, batch_num 8600, step 141819, time: 924.9289848804474 s, accu: 0.9701610803604126, loss_yt: 0.08740989863872528\n",
      "epocht 1, batch_num 8800, step 142019, time: 946.2040054798126 s, accu: 0.9701623320579529, loss_yt: 0.08280165493488312\n",
      "epocht 1, batch_num 9000, step 142219, time: 967.5308594703674 s, accu: 0.9701635241508484, loss_yt: 0.0634276270866394\n",
      "epocht 1, batch_num 9200, step 142419, time: 988.7615427970886 s, accu: 0.9701659083366394, loss_yt: 0.07039892673492432\n",
      "epocht 1, batch_num 9400, step 142619, time: 1009.9905090332031 s, accu: 0.9701676368713379, loss_yt: 0.07823014259338379\n",
      "epocht 1, batch_num 9600, step 142819, time: 1031.073302268982 s, accu: 0.9701687693595886, loss_yt: 0.08147846162319183\n",
      "epocht 1, batch_num 9800, step 143019, time: 1052.4146785736084 s, accu: 0.9701697826385498, loss_yt: 0.050335660576820374\n",
      "epocht 1, batch_num 10000, step 143219, time: 1073.566433429718 s, accu: 0.9701710343360901, loss_yt: 0.03781881555914879\n",
      "epocht 1, batch_num 10200, step 143419, time: 1094.7287383079529 s, accu: 0.9701710343360901, loss_yt: 0.07453924417495728\n",
      "epocht 1, batch_num 10400, step 143619, time: 1115.9002647399902 s, accu: 0.9701720476150513, loss_yt: 0.09025870263576508\n",
      "epocht 1, batch_num 10600, step 143819, time: 1137.0694675445557 s, accu: 0.9701728820800781, loss_yt: 0.11343062669038773\n",
      "epocht 1, batch_num 10800, step 144019, time: 1158.4300801753998 s, accu: 0.9701735377311707, loss_yt: 0.08341763913631439\n",
      "epocht 1, batch_num 11000, step 144219, time: 1179.8379056453705 s, accu: 0.9701743721961975, loss_yt: 0.09273254871368408\n",
      "epocht 1, batch_num 11200, step 144419, time: 1201.1852116584778 s, accu: 0.9701759219169617, loss_yt: 0.08082261681556702\n",
      "epocht 1, batch_num 11400, step 144619, time: 1222.3382160663605 s, accu: 0.970177173614502, loss_yt: 0.08505886793136597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 1, batch_num 11600, step 144819, time: 1243.6522369384766 s, accu: 0.9701790809631348, loss_yt: 0.05501525104045868\n",
      "epocht 1, batch_num 11800, step 145019, time: 1264.7895946502686 s, accu: 0.9701811075210571, loss_yt: 0.08382631838321686\n",
      "epocht 1, batch_num 12000, step 145219, time: 1285.8093509674072 s, accu: 0.9701812267303467, loss_yt: 0.06747663766145706\n",
      "epocht 1, batch_num 12200, step 145419, time: 1306.9070460796356 s, accu: 0.9701839685440063, loss_yt: 0.052137166261672974\n",
      "epocht 1, batch_num 12400, step 145619, time: 1328.1438856124878 s, accu: 0.9701861143112183, loss_yt: 0.07754957675933838\n",
      "epocht 1, batch_num 12600, step 145819, time: 1349.4341177940369 s, accu: 0.9701868891716003, loss_yt: 0.08376637101173401\n",
      "epocht 1, batch_num 12800, step 146019, time: 1370.6841804981232 s, accu: 0.9701884984970093, loss_yt: 0.03389900177717209\n",
      "epocht 1, batch_num 13000, step 146219, time: 1391.9287991523743 s, accu: 0.9701896905899048, loss_yt: 0.06843189150094986\n",
      "epocht 1, batch_num 13200, step 146419, time: 1413.2994606494904 s, accu: 0.9701911807060242, loss_yt: 0.075246661901474\n",
      "epocht 1, batch_num 13400, step 146619, time: 1434.7080228328705 s, accu: 0.970192551612854, loss_yt: 0.08942791819572449\n",
      "epocht 1, batch_num 13600, step 146819, time: 1456.1352944374084 s, accu: 0.9701945781707764, loss_yt: 0.103850819170475\n",
      "epocht 1, batch_num 13800, step 147019, time: 1477.3681755065918 s, accu: 0.9701957702636719, loss_yt: 0.07258424162864685\n",
      "epocht 1, batch_num 14000, step 147219, time: 1498.5139677524567 s, accu: 0.9701967835426331, loss_yt: 0.06344214826822281\n",
      "epocht 1, batch_num 14200, step 147419, time: 1519.6638960838318 s, accu: 0.9701985120773315, loss_yt: 0.07756120711565018\n",
      "epocht 1, batch_num 14400, step 147619, time: 1540.6245276927948 s, accu: 0.9702006578445435, loss_yt: 0.10613100230693817\n",
      "epocht 1, batch_num 14600, step 147819, time: 1561.1836729049683 s, accu: 0.9702014327049255, loss_yt: 0.07753772288560867\n",
      "epocht 1, batch_num 14800, step 148019, time: 1582.0063879489899 s, accu: 0.9702025055885315, loss_yt: 0.07256600260734558\n",
      "iter_validnum 3701\n",
      "epochv 1, step 148020, stop_n 0, time: 1621.9552512168884 s, accu_va: 0.9702137762454677, loss_yv: 0.07366894275831777\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 148021, time: 11.019725799560547 s, accu: 0.9702239632606506, loss_yt: 0.08384696394205093\n",
      "epocht 2, batch_num 200, step 148221, time: 32.2764253616333 s, accu: 0.97022545337677, loss_yt: 0.05689210072159767\n",
      "epocht 2, batch_num 400, step 148421, time: 53.52246809005737 s, accu: 0.9702257513999939, loss_yt: 0.08170434832572937\n",
      "epocht 2, batch_num 600, step 148621, time: 74.82398247718811 s, accu: 0.9702264666557312, loss_yt: 0.05322479456663132\n",
      "epocht 2, batch_num 800, step 148821, time: 96.07257151603699 s, accu: 0.970228374004364, loss_yt: 0.10769128799438477\n",
      "epocht 2, batch_num 1000, step 149021, time: 117.27254486083984 s, accu: 0.9702289700508118, loss_yt: 0.09851332753896713\n",
      "epocht 2, batch_num 1200, step 149221, time: 138.51518535614014 s, accu: 0.9702301621437073, loss_yt: 0.11682292819023132\n",
      "epocht 2, batch_num 1400, step 149421, time: 159.74463391304016 s, accu: 0.9702311158180237, loss_yt: 0.06573192775249481\n",
      "epocht 2, batch_num 1600, step 149621, time: 181.15938353538513 s, accu: 0.9702326655387878, loss_yt: 0.056911565363407135\n",
      "epocht 2, batch_num 1800, step 149821, time: 202.22083044052124 s, accu: 0.9702330231666565, loss_yt: 0.08430112153291702\n",
      "epocht 2, batch_num 2000, step 150021, time: 223.45845580101013 s, accu: 0.9702336192131042, loss_yt: 0.07219299674034119\n",
      "epocht 2, batch_num 2200, step 150221, time: 244.73894810676575 s, accu: 0.9702341556549072, loss_yt: 0.07723171263933182\n",
      "epocht 2, batch_num 2400, step 150421, time: 265.9786305427551 s, accu: 0.9702349901199341, loss_yt: 0.059716951102018356\n",
      "epocht 2, batch_num 2600, step 150621, time: 287.17380142211914 s, accu: 0.9702353477478027, loss_yt: 0.06912249326705933\n",
      "epocht 2, batch_num 2800, step 150821, time: 308.2315092086792 s, accu: 0.9702367186546326, loss_yt: 0.1170574426651001\n",
      "epocht 2, batch_num 3000, step 151021, time: 329.4519157409668 s, accu: 0.9702385067939758, loss_yt: 0.10936760902404785\n",
      "epocht 2, batch_num 3200, step 151221, time: 350.6322293281555 s, accu: 0.970239520072937, loss_yt: 0.06358595937490463\n",
      "epocht 2, batch_num 3400, step 151421, time: 371.97075295448303 s, accu: 0.9702397584915161, loss_yt: 0.08583929389715195\n",
      "epocht 2, batch_num 3600, step 151621, time: 393.1441857814789 s, accu: 0.9702395796775818, loss_yt: 0.07912589609622955\n",
      "epocht 2, batch_num 3800, step 151821, time: 414.47976899147034 s, accu: 0.9702405333518982, loss_yt: 0.07551383227109909\n",
      "epocht 2, batch_num 4000, step 152021, time: 435.5806612968445 s, accu: 0.9702421426773071, loss_yt: 0.0737796276807785\n",
      "epocht 2, batch_num 4200, step 152221, time: 456.7845711708069 s, accu: 0.9702427387237549, loss_yt: 0.08561427146196365\n",
      "epocht 2, batch_num 4400, step 152421, time: 478.03682112693787 s, accu: 0.9702437520027161, loss_yt: 0.06966067105531693\n",
      "epocht 2, batch_num 4600, step 152621, time: 499.11643147468567 s, accu: 0.9702455997467041, loss_yt: 0.07634873688220978\n",
      "epocht 2, batch_num 4800, step 152821, time: 520.4443283081055 s, accu: 0.9702470898628235, loss_yt: 0.06642259657382965\n",
      "epocht 2, batch_num 5000, step 153021, time: 541.8373854160309 s, accu: 0.970249593257904, loss_yt: 0.04861822351813316\n",
      "epocht 2, batch_num 5200, step 153221, time: 563.0161645412445 s, accu: 0.9702509641647339, loss_yt: 0.08613903820514679\n",
      "epocht 2, batch_num 5400, step 153421, time: 584.2107827663422 s, accu: 0.9702516794204712, loss_yt: 0.06947395205497742\n",
      "epocht 2, batch_num 5600, step 153621, time: 605.329995393753 s, accu: 0.9702519178390503, loss_yt: 0.054161664098501205\n",
      "epocht 2, batch_num 5800, step 153821, time: 626.7329630851746 s, accu: 0.9702528119087219, loss_yt: 0.08868535608053207\n",
      "epocht 2, batch_num 6000, step 154021, time: 648.0352220535278 s, accu: 0.9702538847923279, loss_yt: 0.07169787585735321\n",
      "epocht 2, batch_num 6200, step 154221, time: 669.3729779720306 s, accu: 0.9702546000480652, loss_yt: 0.07685912400484085\n",
      "epocht 2, batch_num 6400, step 154421, time: 690.7329733371735 s, accu: 0.9702553153038025, loss_yt: 0.09467592090368271\n",
      "epocht 2, batch_num 6600, step 154621, time: 712.0592803955078 s, accu: 0.9702567458152771, loss_yt: 0.06836898624897003\n",
      "epocht 2, batch_num 6800, step 154821, time: 733.2505342960358 s, accu: 0.9702567458152771, loss_yt: 0.10103841871023178\n",
      "epocht 2, batch_num 7000, step 155021, time: 754.6271483898163 s, accu: 0.970258355140686, loss_yt: 0.07094405591487885\n",
      "epocht 2, batch_num 7200, step 155221, time: 775.9995911121368 s, accu: 0.9702592492103577, loss_yt: 0.0787939503788948\n",
      "epocht 2, batch_num 7400, step 155421, time: 797.3412156105042 s, accu: 0.9702594876289368, loss_yt: 0.07037976384162903\n",
      "epocht 2, batch_num 7600, step 155621, time: 818.505026102066 s, accu: 0.9702607989311218, loss_yt: 0.10127142816781998\n",
      "epocht 2, batch_num 7800, step 155821, time: 839.7833201885223 s, accu: 0.9702617526054382, loss_yt: 0.052240241318941116\n",
      "epocht 2, batch_num 8000, step 156021, time: 861.1552946567535 s, accu: 0.9702630043029785, loss_yt: 0.0679960772395134\n",
      "epocht 2, batch_num 8200, step 156221, time: 882.2750542163849 s, accu: 0.9702630043029785, loss_yt: 0.08694804459810257\n",
      "epocht 2, batch_num 8400, step 156421, time: 903.5492119789124 s, accu: 0.9702640175819397, loss_yt: 0.05665925517678261\n",
      "epocht 2, batch_num 8600, step 156621, time: 924.8753297328949 s, accu: 0.9702652096748352, loss_yt: 0.09117082506418228\n",
      "epocht 2, batch_num 8800, step 156821, time: 946.1529059410095 s, accu: 0.9702661037445068, loss_yt: 0.1092928797006607\n",
      "epocht 2, batch_num 9000, step 157021, time: 967.3641438484192 s, accu: 0.9702683687210083, loss_yt: 0.08448188006877899\n",
      "epocht 2, batch_num 9200, step 157221, time: 988.755891084671 s, accu: 0.9702694416046143, loss_yt: 0.05795491859316826\n",
      "epocht 2, batch_num 9400, step 157421, time: 1009.8727116584778 s, accu: 0.9702710509300232, loss_yt: 0.06669013202190399\n",
      "epocht 2, batch_num 9600, step 157621, time: 1031.098141670227 s, accu: 0.9702711701393127, loss_yt: 0.09667489677667618\n",
      "epocht 2, batch_num 9800, step 157821, time: 1052.4663190841675 s, accu: 0.9702721834182739, loss_yt: 0.060189224779605865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 10000, step 158021, time: 1073.6738226413727 s, accu: 0.9702717065811157, loss_yt: 0.06909438967704773\n",
      "epocht 2, batch_num 10200, step 158221, time: 1094.9202771186829 s, accu: 0.970273494720459, loss_yt: 0.06829313933849335\n",
      "epocht 2, batch_num 10400, step 158421, time: 1116.248225927353 s, accu: 0.9702740907669067, loss_yt: 0.0777767151594162\n",
      "epocht 2, batch_num 10600, step 158621, time: 1137.5270686149597 s, accu: 0.9702756404876709, loss_yt: 0.1123211607336998\n",
      "epocht 2, batch_num 10800, step 158821, time: 1158.7885794639587 s, accu: 0.9702773690223694, loss_yt: 0.0634738951921463\n",
      "epocht 2, batch_num 11000, step 159021, time: 1180.1201446056366 s, accu: 0.9702789783477783, loss_yt: 0.07382756471633911\n",
      "epocht 2, batch_num 11200, step 159221, time: 1201.190043926239 s, accu: 0.9702803492546082, loss_yt: 0.07941778749227524\n",
      "epocht 2, batch_num 11400, step 159421, time: 1222.2577903270721 s, accu: 0.9702808260917664, loss_yt: 0.06609735637903214\n",
      "epocht 2, batch_num 11600, step 159621, time: 1243.3763823509216 s, accu: 0.9702819585800171, loss_yt: 0.05559734255075455\n",
      "epocht 2, batch_num 11800, step 159821, time: 1264.7657573223114 s, accu: 0.9702828526496887, loss_yt: 0.07855851203203201\n",
      "epocht 2, batch_num 12000, step 160021, time: 1286.0160670280457 s, accu: 0.9702840447425842, loss_yt: 0.054559215903282166\n",
      "epocht 2, batch_num 12200, step 160221, time: 1307.2949635982513 s, accu: 0.9702842831611633, loss_yt: 0.04980768263339996\n",
      "epocht 2, batch_num 12400, step 160421, time: 1328.4891319274902 s, accu: 0.9702860713005066, loss_yt: 0.0701780617237091\n",
      "epocht 2, batch_num 12600, step 160621, time: 1349.795815229416 s, accu: 0.970287024974823, loss_yt: 0.050051476806402206\n",
      "epocht 2, batch_num 12800, step 160821, time: 1371.0292732715607 s, accu: 0.9702869653701782, loss_yt: 0.13343116641044617\n",
      "epocht 2, batch_num 13000, step 161021, time: 1392.1650593280792 s, accu: 0.9702896475791931, loss_yt: 0.05694218724966049\n",
      "epocht 2, batch_num 13200, step 161221, time: 1413.4769639968872 s, accu: 0.970291018486023, loss_yt: 0.06616547703742981\n",
      "epocht 2, batch_num 13400, step 161421, time: 1434.7224395275116 s, accu: 0.9702929258346558, loss_yt: 0.07411061227321625\n",
      "epocht 2, batch_num 13600, step 161621, time: 1455.914871931076 s, accu: 0.9702937602996826, loss_yt: 0.08391803503036499\n",
      "epocht 2, batch_num 13800, step 161821, time: 1477.238354921341 s, accu: 0.9702945351600647, loss_yt: 0.05263950303196907\n",
      "epocht 2, batch_num 14000, step 162021, time: 1498.6130411624908 s, accu: 0.9702948927879333, loss_yt: 0.07464005053043365\n",
      "epocht 2, batch_num 14200, step 162221, time: 1519.9044530391693 s, accu: 0.9702962636947632, loss_yt: 0.09446639567613602\n",
      "epocht 2, batch_num 14400, step 162421, time: 1541.3325216770172 s, accu: 0.9702966809272766, loss_yt: 0.060407545417547226\n",
      "epocht 2, batch_num 14600, step 162621, time: 1562.5908613204956 s, accu: 0.9702987670898438, loss_yt: 0.058522067964076996\n",
      "epocht 2, batch_num 14800, step 162821, time: 1583.8404793739319 s, accu: 0.9702997207641602, loss_yt: 0.06902571022510529\n",
      "iter_validnum 3701\n",
      "epochv 2, step 162822, stop_n 0, time: 1624.7733204364777 s, accu_va: 0.9703083365100746, loss_yv: 0.07382204043695297\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 162823, time: 10.982874631881714 s, accu: 0.970317006111145, loss_yt: 0.0670417845249176\n",
      "epocht 2, batch_num 200, step 163023, time: 32.221535205841064 s, accu: 0.9703174233436584, loss_yt: 0.08003085851669312\n",
      "epocht 2, batch_num 400, step 163223, time: 53.45551156997681 s, accu: 0.9703177213668823, loss_yt: 0.07847534120082855\n",
      "epocht 2, batch_num 600, step 163423, time: 74.66894841194153 s, accu: 0.9703192114830017, loss_yt: 0.04777698218822479\n",
      "epocht 2, batch_num 800, step 163623, time: 95.81723976135254 s, accu: 0.9703197479248047, loss_yt: 0.045063164085149765\n",
      "epocht 2, batch_num 1000, step 163823, time: 117.06674695014954 s, accu: 0.9703214168548584, loss_yt: 0.06475003808736801\n",
      "epocht 2, batch_num 1200, step 164023, time: 138.4556565284729 s, accu: 0.9703225493431091, loss_yt: 0.10948996245861053\n",
      "epocht 2, batch_num 1400, step 164223, time: 159.66479635238647 s, accu: 0.9703233242034912, loss_yt: 0.04109314829111099\n",
      "epocht 2, batch_num 1600, step 164423, time: 180.95394492149353 s, accu: 0.9703243374824524, loss_yt: 0.06678889691829681\n",
      "epocht 2, batch_num 1800, step 164623, time: 202.34370398521423 s, accu: 0.970325231552124, loss_yt: 0.08923106640577316\n",
      "epocht 2, batch_num 2000, step 164823, time: 223.58800792694092 s, accu: 0.9703261852264404, loss_yt: 0.05694401636719704\n",
      "epocht 2, batch_num 2200, step 165023, time: 244.58237838745117 s, accu: 0.9703266620635986, loss_yt: 0.05102870613336563\n",
      "epocht 2, batch_num 2400, step 165223, time: 265.97364926338196 s, accu: 0.9703271985054016, loss_yt: 0.053179338574409485\n",
      "epocht 2, batch_num 2600, step 165423, time: 287.08241510391235 s, accu: 0.9703282117843628, loss_yt: 0.07807088643312454\n",
      "epocht 2, batch_num 2800, step 165623, time: 308.3824601173401 s, accu: 0.9703295230865479, loss_yt: 0.0807378739118576\n",
      "epocht 2, batch_num 3000, step 165823, time: 329.46253848075867 s, accu: 0.9703294038772583, loss_yt: 0.07486246526241302\n",
      "epocht 2, batch_num 3200, step 166023, time: 350.83833861351013 s, accu: 0.970329999923706, loss_yt: 0.13526132702827454\n",
      "epocht 2, batch_num 3400, step 166223, time: 372.17171573638916 s, accu: 0.9703304767608643, loss_yt: 0.09109380096197128\n",
      "epocht 2, batch_num 3600, step 166423, time: 393.50595259666443 s, accu: 0.9703312516212463, loss_yt: 0.07456734776496887\n",
      "epocht 2, batch_num 3800, step 166623, time: 414.7225420475006 s, accu: 0.9703319072723389, loss_yt: 0.040485356003046036\n",
      "epocht 2, batch_num 4000, step 166823, time: 436.0004234313965 s, accu: 0.9703335762023926, loss_yt: 0.06194638833403587\n",
      "epocht 2, batch_num 4200, step 167023, time: 457.3139555454254 s, accu: 0.9703342318534851, loss_yt: 0.07448985427618027\n",
      "epocht 2, batch_num 4400, step 167223, time: 478.64678597450256 s, accu: 0.9703351259231567, loss_yt: 0.08132317662239075\n",
      "epocht 2, batch_num 4600, step 167423, time: 499.8922972679138 s, accu: 0.9703366756439209, loss_yt: 0.0908227190375328\n",
      "epocht 2, batch_num 4800, step 167623, time: 521.065845489502 s, accu: 0.9703370332717896, loss_yt: 0.04858323186635971\n",
      "epocht 2, batch_num 5000, step 167823, time: 542.331862449646 s, accu: 0.9703370332717896, loss_yt: 0.07230710983276367\n",
      "epocht 2, batch_num 5200, step 168023, time: 563.4704830646515 s, accu: 0.9703373312950134, loss_yt: 0.0727393701672554\n",
      "epocht 2, batch_num 5400, step 168223, time: 584.9350082874298 s, accu: 0.9703370928764343, loss_yt: 0.059388015419244766\n",
      "epocht 2, batch_num 5600, step 168423, time: 606.1715443134308 s, accu: 0.9703386425971985, loss_yt: 0.09126821905374527\n",
      "epocht 2, batch_num 5800, step 168623, time: 627.4875447750092 s, accu: 0.9703398942947388, loss_yt: 0.07308121025562286\n",
      "epocht 2, batch_num 6000, step 168823, time: 648.7332038879395 s, accu: 0.970341145992279, loss_yt: 0.0874614417552948\n",
      "epocht 2, batch_num 6200, step 169023, time: 670.0834593772888 s, accu: 0.9703419804573059, loss_yt: 0.06368646770715714\n",
      "epocht 2, batch_num 6400, step 169223, time: 691.2218708992004 s, accu: 0.9703433513641357, loss_yt: 0.0869964063167572\n",
      "epocht 2, batch_num 6600, step 169423, time: 712.6205852031708 s, accu: 0.9703436493873596, loss_yt: 0.053815171122550964\n",
      "epocht 2, batch_num 6800, step 169623, time: 733.8415880203247 s, accu: 0.9703449010848999, loss_yt: 0.06042595952749252\n",
      "epocht 2, batch_num 7000, step 169823, time: 755.1289772987366 s, accu: 0.9703458547592163, loss_yt: 0.06434321403503418\n",
      "epocht 2, batch_num 7200, step 170023, time: 776.3243763446808 s, accu: 0.9703460931777954, loss_yt: 0.0984545648097992\n",
      "epocht 2, batch_num 7400, step 170223, time: 797.4724428653717 s, accu: 0.9703468084335327, loss_yt: 0.06910892575979233\n",
      "epocht 2, batch_num 7600, step 170423, time: 818.6213414669037 s, accu: 0.9703481793403625, loss_yt: 0.07597219198942184\n",
      "epocht 2, batch_num 7800, step 170623, time: 839.8312904834747 s, accu: 0.9703490138053894, loss_yt: 0.06893843412399292\n",
      "epocht 2, batch_num 8000, step 170823, time: 861.2796499729156 s, accu: 0.970350980758667, loss_yt: 0.08636493980884552\n",
      "epocht 2, batch_num 8200, step 171023, time: 882.7099859714508 s, accu: 0.9703524112701416, loss_yt: 0.05669097602367401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 8400, step 171223, time: 903.8231353759766 s, accu: 0.9703521728515625, loss_yt: 0.08703535050153732\n",
      "epocht 2, batch_num 8600, step 171423, time: 925.1891098022461 s, accu: 0.970352828502655, loss_yt: 0.08571911603212357\n",
      "epocht 2, batch_num 8800, step 171623, time: 946.5318832397461 s, accu: 0.9703543186187744, loss_yt: 0.06731557101011276\n",
      "epocht 2, batch_num 9000, step 171823, time: 967.9711747169495 s, accu: 0.9703556299209595, loss_yt: 0.07716158032417297\n",
      "epocht 2, batch_num 9200, step 172023, time: 989.164671421051 s, accu: 0.970356285572052, loss_yt: 0.0556790791451931\n",
      "epocht 2, batch_num 9400, step 172223, time: 1010.3444368839264 s, accu: 0.9703572392463684, loss_yt: 0.07186122238636017\n",
      "epocht 2, batch_num 9600, step 172423, time: 1031.6747553348541 s, accu: 0.9703578352928162, loss_yt: 0.10050245374441147\n",
      "epocht 2, batch_num 9800, step 172623, time: 1053.0044615268707 s, accu: 0.97035813331604, loss_yt: 0.07543358951807022\n",
      "epocht 2, batch_num 10000, step 172823, time: 1074.1224675178528 s, accu: 0.9703590273857117, loss_yt: 0.0802571102976799\n",
      "epocht 2, batch_num 10200, step 173023, time: 1095.411464691162 s, accu: 0.9703596830368042, loss_yt: 0.07425378262996674\n",
      "epocht 2, batch_num 10400, step 173223, time: 1116.7648401260376 s, accu: 0.970361590385437, loss_yt: 0.035875000059604645\n",
      "epocht 2, batch_num 10600, step 173423, time: 1137.9133033752441 s, accu: 0.9703627228736877, loss_yt: 0.09547360986471176\n",
      "epocht 2, batch_num 10800, step 173623, time: 1158.9674763679504 s, accu: 0.9703634977340698, loss_yt: 0.05392211675643921\n",
      "epocht 2, batch_num 11000, step 173823, time: 1180.1822669506073 s, accu: 0.970364511013031, loss_yt: 0.0677000880241394\n",
      "epocht 2, batch_num 11200, step 174023, time: 1201.4319579601288 s, accu: 0.9703651070594788, loss_yt: 0.08374010771512985\n",
      "epocht 2, batch_num 11400, step 174223, time: 1222.9103724956512 s, accu: 0.9703652262687683, loss_yt: 0.0908103808760643\n",
      "epocht 2, batch_num 11600, step 174423, time: 1244.0209968090057 s, accu: 0.970365583896637, loss_yt: 0.0774892047047615\n",
      "epocht 2, batch_num 11800, step 174623, time: 1265.273618221283 s, accu: 0.9703666567802429, loss_yt: 0.09476838260889053\n",
      "epocht 2, batch_num 12000, step 174823, time: 1286.3964421749115 s, accu: 0.9703682065010071, loss_yt: 0.0813865140080452\n",
      "epocht 2, batch_num 12200, step 175023, time: 1307.7091298103333 s, accu: 0.9703692197799683, loss_yt: 0.07427676767110825\n",
      "epocht 2, batch_num 12400, step 175223, time: 1328.9474852085114 s, accu: 0.9703700542449951, loss_yt: 0.11275205761194229\n",
      "epocht 2, batch_num 12600, step 175423, time: 1350.3317408561707 s, accu: 0.9703713655471802, loss_yt: 0.07802709192037582\n",
      "epocht 2, batch_num 12800, step 175623, time: 1371.2542562484741 s, accu: 0.9703726768493652, loss_yt: 0.03943929076194763\n",
      "epocht 2, batch_num 13000, step 175823, time: 1392.619714975357 s, accu: 0.9703734517097473, loss_yt: 0.09221051633358002\n",
      "epocht 2, batch_num 13200, step 176023, time: 1413.3592340946198 s, accu: 0.9703742861747742, loss_yt: 0.08951930701732635\n",
      "epocht 2, batch_num 13400, step 176223, time: 1434.0346262454987 s, accu: 0.9703747630119324, loss_yt: 0.06831448525190353\n",
      "epocht 2, batch_num 13600, step 176423, time: 1454.6270079612732 s, accu: 0.9703751802444458, loss_yt: 0.05712055414915085\n",
      "epocht 2, batch_num 13800, step 176623, time: 1475.3301329612732 s, accu: 0.9703763723373413, loss_yt: 0.06505635380744934\n",
      "epocht 2, batch_num 14000, step 176823, time: 1496.005089521408 s, accu: 0.9703773260116577, loss_yt: 0.07560496777296066\n",
      "epocht 2, batch_num 14200, step 177023, time: 1516.4985115528107 s, accu: 0.9703786373138428, loss_yt: 0.07642032951116562\n",
      "epocht 2, batch_num 14400, step 177223, time: 1537.2647478580475 s, accu: 0.9703794717788696, loss_yt: 0.10232792794704437\n",
      "epocht 2, batch_num 14600, step 177423, time: 1557.8554317951202 s, accu: 0.9703804850578308, loss_yt: 0.09191416949033737\n",
      "epocht 2, batch_num 14800, step 177623, time: 1578.614760875702 s, accu: 0.970381498336792, loss_yt: 0.09373563528060913\n",
      "iter_validnum 3701\n",
      "epochv 2, step 177624, stop_n 1, time: 1619.5969502925873 s, accu_va: 0.9703895622123032, loss_yv: 0.07378785270051082\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 177625, time: 11.700331449508667 s, accu: 0.9703980088233948, loss_yt: 0.07274934649467468\n",
      "epocht 2, batch_num 200, step 177825, time: 32.734312534332275 s, accu: 0.9703989028930664, loss_yt: 0.126205176115036\n",
      "epocht 2, batch_num 400, step 178025, time: 54.13749074935913 s, accu: 0.9704000949859619, loss_yt: 0.06851270794868469\n",
      "epocht 2, batch_num 600, step 178225, time: 75.33070206642151 s, accu: 0.9704018831253052, loss_yt: 0.07539564371109009\n",
      "epocht 2, batch_num 800, step 178425, time: 96.69748759269714 s, accu: 0.9704029560089111, loss_yt: 0.0595337450504303\n",
      "epocht 2, batch_num 1000, step 178625, time: 118.11509680747986 s, accu: 0.9704039096832275, loss_yt: 0.054033827036619186\n",
      "epocht 2, batch_num 1200, step 178825, time: 139.34079957008362 s, accu: 0.9704048037528992, loss_yt: 0.09864642471075058\n",
      "epocht 2, batch_num 1400, step 179025, time: 160.50450038909912 s, accu: 0.9704054594039917, loss_yt: 0.061743494123220444\n",
      "epocht 2, batch_num 1600, step 179225, time: 181.64766311645508 s, accu: 0.9704064726829529, loss_yt: 0.07103655487298965\n",
      "epocht 2, batch_num 1800, step 179425, time: 202.86959719657898 s, accu: 0.9704073071479797, loss_yt: 0.0489204041659832\n",
      "epocht 2, batch_num 2000, step 179625, time: 224.13595914840698 s, accu: 0.9704078435897827, loss_yt: 0.07228543609380722\n",
      "epocht 2, batch_num 2200, step 179825, time: 245.25641298294067 s, accu: 0.9704094529151917, loss_yt: 0.0766695961356163\n",
      "epocht 2, batch_num 2400, step 180025, time: 266.70322942733765 s, accu: 0.9704101085662842, loss_yt: 0.05544290691614151\n",
      "epocht 2, batch_num 2600, step 180225, time: 288.08152985572815 s, accu: 0.9704121947288513, loss_yt: 0.06787039339542389\n",
      "epocht 2, batch_num 2800, step 180425, time: 309.25782561302185 s, accu: 0.9704130291938782, loss_yt: 0.09001889824867249\n",
      "epocht 2, batch_num 3000, step 180625, time: 330.4838683605194 s, accu: 0.9704132080078125, loss_yt: 0.06723964214324951\n",
      "epocht 2, batch_num 3200, step 180825, time: 351.8201336860657 s, accu: 0.9704141020774841, loss_yt: 0.06380457431077957\n",
      "epocht 2, batch_num 3400, step 181025, time: 373.12208247184753 s, accu: 0.9704145193099976, loss_yt: 0.08954876661300659\n",
      "epocht 2, batch_num 3600, step 181225, time: 394.5064823627472 s, accu: 0.9704151153564453, loss_yt: 0.055085282772779465\n",
      "epocht 2, batch_num 3800, step 181425, time: 415.6271402835846 s, accu: 0.970415472984314, loss_yt: 0.07351446151733398\n",
      "epocht 2, batch_num 4000, step 181625, time: 436.83337807655334 s, accu: 0.970417320728302, loss_yt: 0.07796242833137512\n",
      "epocht 2, batch_num 4200, step 181825, time: 457.97548151016235 s, accu: 0.970418393611908, loss_yt: 0.09130431711673737\n",
      "epocht 2, batch_num 4400, step 182025, time: 479.31070947647095 s, accu: 0.9704194068908691, loss_yt: 0.0594177171587944\n",
      "epocht 2, batch_num 4600, step 182225, time: 500.51983737945557 s, accu: 0.9704200029373169, loss_yt: 0.11336681991815567\n",
      "epocht 2, batch_num 4800, step 182425, time: 521.7173669338226 s, accu: 0.9704214334487915, loss_yt: 0.05083388462662697\n",
      "epocht 2, batch_num 5000, step 182625, time: 542.8289768695831 s, accu: 0.9704223871231079, loss_yt: 0.061683688312768936\n",
      "epocht 2, batch_num 5200, step 182825, time: 564.0484502315521 s, accu: 0.9704228043556213, loss_yt: 0.04434193670749664\n",
      "epocht 2, batch_num 5400, step 183025, time: 585.3507633209229 s, accu: 0.9704238772392273, loss_yt: 0.06149567291140556\n",
      "epocht 2, batch_num 5600, step 183225, time: 606.5492272377014 s, accu: 0.970424234867096, loss_yt: 0.04872514307498932\n",
      "epocht 2, batch_num 5800, step 183425, time: 627.8304681777954 s, accu: 0.9704250693321228, loss_yt: 0.08491331338882446\n",
      "epocht 2, batch_num 6000, step 183625, time: 648.9873869419098 s, accu: 0.9704257845878601, loss_yt: 0.07274873554706573\n",
      "epocht 2, batch_num 6200, step 183825, time: 670.3276124000549 s, accu: 0.9704261422157288, loss_yt: 0.08204849809408188\n",
      "epocht 2, batch_num 6400, step 184025, time: 691.6117486953735 s, accu: 0.970427393913269, loss_yt: 0.0769062489271164\n",
      "epocht 2, batch_num 6600, step 184225, time: 712.7012526988983 s, accu: 0.9704270362854004, loss_yt: 0.11372363567352295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 6800, step 184425, time: 733.752382516861 s, accu: 0.9704272747039795, loss_yt: 0.09208856523036957\n",
      "epocht 2, batch_num 7000, step 184625, time: 755.0957338809967 s, accu: 0.9704275727272034, loss_yt: 0.09795158356428146\n",
      "epocht 2, batch_num 7200, step 184825, time: 776.3386652469635 s, accu: 0.9704287648200989, loss_yt: 0.06828872859477997\n",
      "epocht 2, batch_num 7400, step 185025, time: 797.7160921096802 s, accu: 0.9704290628433228, loss_yt: 0.09781847894191742\n",
      "epocht 2, batch_num 7600, step 185225, time: 819.0723807811737 s, accu: 0.9704298973083496, loss_yt: 0.08119669556617737\n",
      "epocht 2, batch_num 7800, step 185425, time: 840.4771223068237 s, accu: 0.970430314540863, loss_yt: 0.04620988667011261\n",
      "epocht 2, batch_num 8000, step 185625, time: 861.7298202514648 s, accu: 0.9704304337501526, loss_yt: 0.06320695579051971\n",
      "epocht 2, batch_num 8200, step 185825, time: 882.7217075824738 s, accu: 0.970431923866272, loss_yt: 0.0603850893676281\n",
      "epocht 2, batch_num 8400, step 186025, time: 903.7708270549774 s, accu: 0.9704323410987854, loss_yt: 0.07543695718050003\n",
      "epocht 2, batch_num 8600, step 186225, time: 925.059063911438 s, accu: 0.9704340696334839, loss_yt: 0.08771093934774399\n",
      "epocht 2, batch_num 8800, step 186425, time: 946.2779531478882 s, accu: 0.9704341888427734, loss_yt: 0.08306814730167389\n",
      "epocht 2, batch_num 9000, step 186625, time: 967.5668637752533 s, accu: 0.9704349040985107, loss_yt: 0.08000490069389343\n",
      "epocht 2, batch_num 9200, step 186825, time: 988.6572041511536 s, accu: 0.9704352021217346, loss_yt: 0.08665154129266739\n",
      "epocht 2, batch_num 9400, step 187025, time: 1009.9793727397919 s, accu: 0.970435380935669, loss_yt: 0.08952450007200241\n",
      "epocht 2, batch_num 9600, step 187225, time: 1031.1431937217712 s, accu: 0.9704364538192749, loss_yt: 0.08316045254468918\n",
      "epocht 2, batch_num 9800, step 187425, time: 1052.3362243175507 s, accu: 0.9704360365867615, loss_yt: 0.09603926539421082\n",
      "epocht 2, batch_num 10000, step 187625, time: 1073.633956670761 s, accu: 0.9704370498657227, loss_yt: 0.07026335597038269\n",
      "epocht 2, batch_num 10200, step 187825, time: 1094.9535493850708 s, accu: 0.9704373478889465, loss_yt: 0.06571134924888611\n",
      "epocht 2, batch_num 10400, step 188025, time: 1116.156212091446 s, accu: 0.9704381227493286, loss_yt: 0.06923896074295044\n",
      "epocht 2, batch_num 10600, step 188225, time: 1137.3574573993683 s, accu: 0.9704387784004211, loss_yt: 0.07644818723201752\n",
      "epocht 2, batch_num 10800, step 188425, time: 1158.5594189167023 s, accu: 0.9704400897026062, loss_yt: 0.08160319179296494\n",
      "epocht 2, batch_num 11000, step 188625, time: 1179.6963877677917 s, accu: 0.9704413414001465, loss_yt: 0.08825842291116714\n",
      "epocht 2, batch_num 11200, step 188825, time: 1201.0181171894073 s, accu: 0.9704421758651733, loss_yt: 0.07733745872974396\n",
      "epocht 2, batch_num 11400, step 189025, time: 1222.2818076610565 s, accu: 0.9704427719116211, loss_yt: 0.06801249086856842\n",
      "epocht 2, batch_num 11600, step 189225, time: 1243.6164846420288 s, accu: 0.9704428315162659, loss_yt: 0.08361275494098663\n",
      "epocht 2, batch_num 11800, step 189425, time: 1264.9494171142578 s, accu: 0.9704429507255554, loss_yt: 0.061689868569374084\n",
      "epocht 2, batch_num 12000, step 189625, time: 1286.0242743492126 s, accu: 0.970443844795227, loss_yt: 0.052950356155633926\n",
      "epocht 2, batch_num 12200, step 189825, time: 1307.2567193508148 s, accu: 0.9704445004463196, loss_yt: 0.08790025115013123\n",
      "epocht 2, batch_num 12400, step 190025, time: 1328.5803446769714 s, accu: 0.9704453349113464, loss_yt: 0.09367331117391586\n",
      "epocht 2, batch_num 12600, step 190225, time: 1349.6810929775238 s, accu: 0.9704450964927673, loss_yt: 0.0721864178776741\n",
      "epocht 2, batch_num 12800, step 190425, time: 1370.9253215789795 s, accu: 0.9704461097717285, loss_yt: 0.04061242938041687\n",
      "epocht 2, batch_num 13000, step 190625, time: 1392.0958154201508 s, accu: 0.970446765422821, loss_yt: 0.08325903117656708\n",
      "epocht 2, batch_num 13200, step 190825, time: 1413.3644881248474 s, accu: 0.9704476594924927, loss_yt: 0.07881341874599457\n",
      "epocht 2, batch_num 13400, step 191025, time: 1434.4648878574371 s, accu: 0.9704486727714539, loss_yt: 0.052260324358940125\n",
      "epocht 2, batch_num 13600, step 191225, time: 1455.5525455474854 s, accu: 0.9704488515853882, loss_yt: 0.09383106231689453\n",
      "epocht 2, batch_num 13800, step 191425, time: 1476.686181306839 s, accu: 0.9704498648643494, loss_yt: 0.08264216035604477\n",
      "epocht 2, batch_num 14000, step 191625, time: 1497.9570662975311 s, accu: 0.9704503417015076, loss_yt: 0.07263200730085373\n",
      "epocht 2, batch_num 14200, step 191825, time: 1519.0952830314636 s, accu: 0.9704515933990479, loss_yt: 0.10129448771476746\n",
      "epocht 2, batch_num 14400, step 192025, time: 1540.3949496746063 s, accu: 0.9704532027244568, loss_yt: 0.06511521339416504\n",
      "epocht 2, batch_num 14600, step 192225, time: 1561.4563312530518 s, accu: 0.9704537391662598, loss_yt: 0.07489040493965149\n",
      "epocht 2, batch_num 14800, step 192425, time: 1582.796451807022 s, accu: 0.9704543948173523, loss_yt: 0.07873068749904633\n",
      "iter_validnum 3701\n",
      "epochv 2, step 192426, stop_n 0, time: 1623.85719871521 s, accu_va: 0.9704607942722128, loss_yv: 0.07350013160892385\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 192427, time: 11.17997670173645 s, accu: 0.9704669713973999, loss_yt: 0.07407381385564804\n",
      "epocht 2, batch_num 200, step 192627, time: 32.33400344848633 s, accu: 0.9704670310020447, loss_yt: 0.04989341273903847\n",
      "epocht 2, batch_num 400, step 192827, time: 53.0571768283844 s, accu: 0.9704673290252686, loss_yt: 0.09652789682149887\n",
      "epocht 2, batch_num 600, step 193027, time: 74.03345465660095 s, accu: 0.9704680442810059, loss_yt: 0.05748405680060387\n",
      "epocht 2, batch_num 800, step 193227, time: 95.04491972923279 s, accu: 0.9704691767692566, loss_yt: 0.06593498587608337\n",
      "epocht 2, batch_num 1000, step 193427, time: 116.24764084815979 s, accu: 0.9704710841178894, loss_yt: 0.038698773831129074\n",
      "epocht 2, batch_num 1200, step 193627, time: 137.2300751209259 s, accu: 0.9704715609550476, loss_yt: 0.06416058540344238\n",
      "epocht 2, batch_num 1400, step 193827, time: 158.375470161438 s, accu: 0.9704723954200745, loss_yt: 0.10078742355108261\n",
      "epocht 2, batch_num 1600, step 194027, time: 179.55966806411743 s, accu: 0.9704736471176147, loss_yt: 0.053340062499046326\n",
      "epocht 2, batch_num 1800, step 194227, time: 200.7215976715088 s, accu: 0.9704751968383789, loss_yt: 0.09653308242559433\n",
      "epocht 2, batch_num 2000, step 194427, time: 221.91006517410278 s, accu: 0.9704763293266296, loss_yt: 0.07114303112030029\n",
      "epocht 2, batch_num 2200, step 194627, time: 242.99480843544006 s, accu: 0.970476508140564, loss_yt: 0.07029706239700317\n",
      "epocht 2, batch_num 2400, step 194827, time: 264.2897720336914 s, accu: 0.9704769849777222, loss_yt: 0.0576012060046196\n",
      "epocht 2, batch_num 2600, step 195027, time: 285.59595108032227 s, accu: 0.9704775214195251, loss_yt: 0.0684337392449379\n",
      "epocht 2, batch_num 2800, step 195227, time: 306.77175211906433 s, accu: 0.9704777002334595, loss_yt: 0.060862548649311066\n",
      "epocht 2, batch_num 3000, step 195427, time: 327.9787805080414 s, accu: 0.9704776406288147, loss_yt: 0.06988206505775452\n",
      "epocht 2, batch_num 3200, step 195627, time: 349.27046394348145 s, accu: 0.9704786539077759, loss_yt: 0.07962445169687271\n",
      "epocht 2, batch_num 3400, step 195827, time: 370.4941506385803 s, accu: 0.9704793691635132, loss_yt: 0.06948231905698776\n",
      "epocht 2, batch_num 3600, step 196027, time: 391.67388010025024 s, accu: 0.9704799652099609, loss_yt: 0.07494306564331055\n",
      "epocht 2, batch_num 3800, step 196227, time: 412.8196506500244 s, accu: 0.9704806208610535, loss_yt: 0.05859704688191414\n",
      "epocht 2, batch_num 4000, step 196427, time: 434.00934314727783 s, accu: 0.9704819917678833, loss_yt: 0.09301776438951492\n",
      "epocht 2, batch_num 4200, step 196627, time: 455.1821811199188 s, accu: 0.9704824090003967, loss_yt: 0.08221222460269928\n",
      "epocht 2, batch_num 4400, step 196827, time: 476.6039731502533 s, accu: 0.9704831838607788, loss_yt: 0.07738245278596878\n",
      "epocht 2, batch_num 4600, step 197027, time: 497.80751752853394 s, accu: 0.9704830646514893, loss_yt: 0.06462978571653366\n",
      "epocht 2, batch_num 4800, step 197227, time: 519.1705176830292 s, accu: 0.9704838991165161, loss_yt: 0.0697527527809143\n",
      "epocht 2, batch_num 5000, step 197427, time: 540.4190170764923 s, accu: 0.9704848527908325, loss_yt: 0.0683014988899231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 5200, step 197627, time: 561.7321989536285 s, accu: 0.9704859256744385, loss_yt: 0.08047989010810852\n",
      "epocht 2, batch_num 5400, step 197827, time: 583.0422575473785 s, accu: 0.9704862236976624, loss_yt: 0.07191810011863708\n",
      "epocht 2, batch_num 5600, step 198027, time: 604.2724707126617 s, accu: 0.970487117767334, loss_yt: 0.071172796189785\n",
      "epocht 2, batch_num 5800, step 198227, time: 625.6172611713409 s, accu: 0.9704878926277161, loss_yt: 0.06529871374368668\n",
      "epocht 2, batch_num 6000, step 198427, time: 646.8885440826416 s, accu: 0.9704889059066772, loss_yt: 0.08016204833984375\n",
      "epocht 2, batch_num 6200, step 198627, time: 668.191260099411 s, accu: 0.970489501953125, loss_yt: 0.0599854402244091\n",
      "epocht 2, batch_num 6400, step 198827, time: 689.4471867084503 s, accu: 0.9704894423484802, loss_yt: 0.09888694435358047\n",
      "epocht 2, batch_num 6600, step 199027, time: 710.6959934234619 s, accu: 0.9704897403717041, loss_yt: 0.09685821086168289\n",
      "epocht 2, batch_num 6800, step 199227, time: 732.0094544887543 s, accu: 0.9704904556274414, loss_yt: 0.08494745194911957\n",
      "epocht 2, batch_num 7000, step 199427, time: 753.2111005783081 s, accu: 0.9704911708831787, loss_yt: 0.08599888533353806\n",
      "epocht 2, batch_num 7200, step 199627, time: 774.44433426857 s, accu: 0.9704916477203369, loss_yt: 0.06196785345673561\n",
      "epocht 2, batch_num 7400, step 199827, time: 795.587418794632 s, accu: 0.9704916477203369, loss_yt: 0.08086717128753662\n",
      "epocht 2, batch_num 7600, step 200027, time: 816.8936786651611 s, accu: 0.9704923033714294, loss_yt: 0.05400281399488449\n",
      "epocht 2, batch_num 7800, step 200227, time: 838.2956538200378 s, accu: 0.9704931974411011, loss_yt: 0.0576101578772068\n",
      "epocht 2, batch_num 8000, step 200427, time: 859.5305652618408 s, accu: 0.9704940915107727, loss_yt: 0.08575174957513809\n",
      "epocht 2, batch_num 8200, step 200627, time: 880.8490104675293 s, accu: 0.9704947471618652, loss_yt: 0.07627538591623306\n",
      "epocht 2, batch_num 8400, step 200827, time: 902.2736177444458 s, accu: 0.9704951643943787, loss_yt: 0.06893866509199142\n",
      "epocht 2, batch_num 8600, step 201027, time: 923.3802020549774 s, accu: 0.9704957008361816, loss_yt: 0.07579033821821213\n",
      "epocht 2, batch_num 8800, step 201227, time: 944.7433276176453 s, accu: 0.9704964756965637, loss_yt: 0.057774484157562256\n",
      "epocht 2, batch_num 9000, step 201427, time: 965.9770367145538 s, accu: 0.9704972505569458, loss_yt: 0.05570773407816887\n",
      "epocht 2, batch_num 9200, step 201627, time: 986.9057326316833 s, accu: 0.9704979658126831, loss_yt: 0.06671857088804245\n",
      "epocht 2, batch_num 9400, step 201827, time: 1006.8007855415344 s, accu: 0.9704981446266174, loss_yt: 0.10559415817260742\n",
      "epocht 2, batch_num 9600, step 202027, time: 1027.8432140350342 s, accu: 0.9704993963241577, loss_yt: 0.057870183140039444\n",
      "epocht 2, batch_num 9800, step 202227, time: 1049.049160003662 s, accu: 0.9705004692077637, loss_yt: 0.0717620700597763\n",
      "epocht 2, batch_num 10000, step 202427, time: 1070.2487289905548 s, accu: 0.9705013036727905, loss_yt: 0.0666714534163475\n",
      "epocht 2, batch_num 10200, step 202627, time: 1091.3074686527252 s, accu: 0.9705008864402771, loss_yt: 0.09698165953159332\n",
      "epocht 2, batch_num 10400, step 202827, time: 1112.6716933250427 s, accu: 0.9705013632774353, loss_yt: 0.08716145902872086\n",
      "epocht 2, batch_num 10600, step 203027, time: 1134.0197360515594 s, accu: 0.9705021977424622, loss_yt: 0.05816994979977608\n",
      "epocht 2, batch_num 10800, step 203227, time: 1155.4036066532135 s, accu: 0.9705028533935547, loss_yt: 0.09451805055141449\n",
      "epocht 2, batch_num 11000, step 203427, time: 1176.6223404407501 s, accu: 0.9705032110214233, loss_yt: 0.056738972663879395\n",
      "epocht 2, batch_num 11200, step 203627, time: 1197.9973771572113 s, accu: 0.9705032110214233, loss_yt: 0.08573446422815323\n",
      "epocht 2, batch_num 11400, step 203827, time: 1219.318523645401 s, accu: 0.9705041646957397, loss_yt: 0.06276421248912811\n",
      "epocht 2, batch_num 11600, step 204027, time: 1240.6583290100098 s, accu: 0.970504879951477, loss_yt: 0.06840834021568298\n",
      "epocht 2, batch_num 11800, step 204227, time: 1261.915965795517 s, accu: 0.9705055356025696, loss_yt: 0.06651680916547775\n",
      "epocht 2, batch_num 12000, step 204427, time: 1283.1837594509125 s, accu: 0.9705061316490173, loss_yt: 0.06028652563691139\n",
      "epocht 2, batch_num 12200, step 204627, time: 1304.3464722633362 s, accu: 0.9705076217651367, loss_yt: 0.06651512533426285\n",
      "epocht 2, batch_num 12400, step 204827, time: 1325.3007264137268 s, accu: 0.9705082178115845, loss_yt: 0.06493303924798965\n",
      "epocht 2, batch_num 12600, step 205027, time: 1346.0935072898865 s, accu: 0.9705091714859009, loss_yt: 0.1013115644454956\n",
      "epocht 2, batch_num 12800, step 205227, time: 1366.7231793403625 s, accu: 0.970509946346283, loss_yt: 0.043327637016773224\n",
      "epocht 2, batch_num 13000, step 205427, time: 1387.172913312912 s, accu: 0.9705103039741516, loss_yt: 0.0367918387055397\n",
      "epocht 2, batch_num 13200, step 205627, time: 1407.5959541797638 s, accu: 0.9705104827880859, loss_yt: 0.05509718134999275\n",
      "epocht 2, batch_num 13400, step 205827, time: 1428.4060294628143 s, accu: 0.9705101847648621, loss_yt: 0.09051523357629776\n",
      "epocht 2, batch_num 13600, step 206027, time: 1448.9488713741302 s, accu: 0.970509946346283, loss_yt: 0.05195597931742668\n",
      "epocht 2, batch_num 13800, step 206227, time: 1469.6376826763153 s, accu: 0.9705111980438232, loss_yt: 0.09112486988306046\n",
      "epocht 2, batch_num 14000, step 206427, time: 1490.331841468811 s, accu: 0.9705122113227844, loss_yt: 0.058470550924539566\n",
      "epocht 2, batch_num 14200, step 206627, time: 1510.957043647766 s, accu: 0.9705123901367188, loss_yt: 0.05077402666211128\n",
      "epocht 2, batch_num 14400, step 206827, time: 1531.6736760139465 s, accu: 0.9705125093460083, loss_yt: 0.0797707736492157\n",
      "epocht 2, batch_num 14600, step 207027, time: 1552.010192155838 s, accu: 0.9705132246017456, loss_yt: 0.0917663648724556\n",
      "epocht 2, batch_num 14800, step 207227, time: 1572.5828366279602 s, accu: 0.9705142974853516, loss_yt: 0.06391654163599014\n",
      "iter_validnum 3701\n",
      "epochv 2, step 207228, stop_n 0, time: 1614.0339334011078 s, accu_va: 0.9705199982404258, loss_yv: 0.07358197705612347\n",
      "iter_trainnum 14802\n",
      "epocht 2, batch_num 0, step 207229, time: 10.754374027252197 s, accu: 0.9705258011817932, loss_yt: 0.05488140881061554\n",
      "epocht 2, batch_num 200, step 207429, time: 31.99738383293152 s, accu: 0.9705264568328857, loss_yt: 0.09053405374288559\n",
      "epocht 2, batch_num 400, step 207629, time: 53.1955029964447 s, accu: 0.9705272912979126, loss_yt: 0.055000826716423035\n",
      "epocht 2, batch_num 600, step 207829, time: 74.41754746437073 s, accu: 0.9705284237861633, loss_yt: 0.06420309096574783\n",
      "epocht 2, batch_num 800, step 208029, time: 95.73858165740967 s, accu: 0.9705292582511902, loss_yt: 0.07821717858314514\n",
      "epocht 2, batch_num 1000, step 208229, time: 116.98926568031311 s, accu: 0.9705293774604797, loss_yt: 0.07489761710166931\n",
      "epocht 2, batch_num 1200, step 208429, time: 138.23658299446106 s, accu: 0.9705301523208618, loss_yt: 0.053590476512908936\n",
      "epocht 2, batch_num 1400, step 208629, time: 159.6540629863739 s, accu: 0.970531165599823, loss_yt: 0.07408294081687927\n",
      "epocht 2, batch_num 1600, step 208829, time: 180.89564514160156 s, accu: 0.9705318212509155, loss_yt: 0.050556618720293045\n",
      "epocht 2, batch_num 1800, step 209029, time: 202.05075573921204 s, accu: 0.9705333113670349, loss_yt: 0.0808345228433609\n",
      "epocht 2, batch_num 2000, step 209229, time: 223.32407569885254 s, accu: 0.9705337882041931, loss_yt: 0.05226646736264229\n",
      "epocht 2, batch_num 2200, step 209429, time: 244.676260471344 s, accu: 0.9705345630645752, loss_yt: 0.08123280853033066\n",
      "epocht 2, batch_num 2400, step 209629, time: 265.9893250465393 s, accu: 0.9705348610877991, loss_yt: 0.07859279215335846\n",
      "epocht 2, batch_num 2600, step 209829, time: 287.3482925891876 s, accu: 0.9705348014831543, loss_yt: 0.06008562073111534\n",
      "epocht 2, batch_num 2800, step 210029, time: 308.69946360588074 s, accu: 0.9705357551574707, loss_yt: 0.09988278895616531\n",
      "epocht 2, batch_num 3000, step 210229, time: 330.00376319885254 s, accu: 0.9705366492271423, loss_yt: 0.043799202889204025\n",
      "epocht 2, batch_num 3200, step 210429, time: 351.32151675224304 s, accu: 0.9705368876457214, loss_yt: 0.05660659447312355\n",
      "epocht 2, batch_num 3400, step 210629, time: 372.5421543121338 s, accu: 0.9705371260643005, loss_yt: 0.07571631669998169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 2, batch_num 3600, step 210829, time: 393.82785987854004 s, accu: 0.9705373644828796, loss_yt: 0.07367337495088577\n",
      "epocht 2, batch_num 3800, step 211029, time: 415.0008804798126 s, accu: 0.9705382585525513, loss_yt: 0.07879021018743515\n",
      "epocht 2, batch_num 4000, step 211229, time: 436.3664240837097 s, accu: 0.9705383777618408, loss_yt: 0.05903554707765579\n",
      "epocht 2, batch_num 4200, step 211429, time: 457.4941062927246 s, accu: 0.9705395102500916, loss_yt: 0.10157760232686996\n",
      "epocht 2, batch_num 4400, step 211629, time: 478.6628062725067 s, accu: 0.9705394506454468, loss_yt: 0.05659033730626106\n",
      "epocht 2, batch_num 4600, step 211829, time: 499.89226245880127 s, accu: 0.9705392122268677, loss_yt: 0.08456055074930191\n",
      "epocht 2, batch_num 4800, step 212029, time: 521.306973695755 s, accu: 0.9705396294593811, loss_yt: 0.09023748338222504\n",
      "epocht 2, batch_num 5000, step 212229, time: 542.6317825317383 s, accu: 0.9705398678779602, loss_yt: 0.051388029009103775\n",
      "epocht 2, batch_num 5200, step 212429, time: 563.8732380867004 s, accu: 0.9705403447151184, loss_yt: 0.10040659457445145\n",
      "epocht 2, batch_num 5400, step 212629, time: 584.9611284732819 s, accu: 0.9705409407615662, loss_yt: 0.09387797862291336\n",
      "epocht 2, batch_num 5600, step 212829, time: 606.1714396476746 s, accu: 0.9705415964126587, loss_yt: 0.0743216723203659\n",
      "epocht 2, batch_num 5800, step 213029, time: 627.4161491394043 s, accu: 0.9705419540405273, loss_yt: 0.10619094967842102\n",
      "epocht 2, batch_num 6000, step 213229, time: 648.7531180381775 s, accu: 0.9705427885055542, loss_yt: 0.0745849683880806\n",
      "epocht 2, batch_num 6200, step 213429, time: 670.1043968200684 s, accu: 0.9705432057380676, loss_yt: 0.05506022274494171\n",
      "epocht 2, batch_num 6400, step 213629, time: 691.451785326004 s, accu: 0.970544159412384, loss_yt: 0.12687569856643677\n",
      "epocht 2, batch_num 6600, step 213829, time: 712.6751141548157 s, accu: 0.9705447554588318, loss_yt: 0.07265608757734299\n",
      "epocht 2, batch_num 6800, step 214029, time: 733.9700305461884 s, accu: 0.97054523229599, loss_yt: 0.08069364726543427\n",
      "epocht 2, batch_num 7000, step 214229, time: 755.0522689819336 s, accu: 0.970546305179596, loss_yt: 0.05887366458773613\n",
      "epocht 2, batch_num 7200, step 214429, time: 776.2873859405518 s, accu: 0.9705470204353333, loss_yt: 0.06675229966640472\n",
      "epocht 2, batch_num 7400, step 214629, time: 797.6071586608887 s, accu: 0.9705479145050049, loss_yt: 0.056264497339725494\n",
      "epocht 2, batch_num 7600, step 214829, time: 818.7870910167694 s, accu: 0.9705479145050049, loss_yt: 0.07742459326982498\n",
      "epocht 2, batch_num 7800, step 215029, time: 840.1611161231995 s, accu: 0.9705492258071899, loss_yt: 0.03669089451432228\n",
      "epocht 2, batch_num 8000, step 215229, time: 861.498265504837 s, accu: 0.9705495834350586, loss_yt: 0.0888470709323883\n",
      "epocht 2, batch_num 8200, step 215429, time: 882.8234994411469 s, accu: 0.9705504179000854, loss_yt: 0.07675190269947052\n",
      "epocht 2, batch_num 8400, step 215629, time: 903.7808861732483 s, accu: 0.9705520868301392, loss_yt: 0.06293044239282608\n",
      "epocht 2, batch_num 8600, step 215829, time: 924.9548606872559 s, accu: 0.9705522060394287, loss_yt: 0.11061359941959381\n",
      "epocht 2, batch_num 8800, step 216029, time: 946.2987995147705 s, accu: 0.9705529808998108, loss_yt: 0.057154566049575806\n",
      "epocht 2, batch_num 9000, step 216229, time: 967.5757939815521 s, accu: 0.970553457736969, loss_yt: 0.09106512367725372\n",
      "epocht 2, batch_num 9200, step 216429, time: 988.9125862121582 s, accu: 0.9705533385276794, loss_yt: 0.07103811949491501\n",
      "epocht 2, batch_num 9400, step 216629, time: 1010.2192010879517 s, accu: 0.9705538153648376, loss_yt: 0.065986268222332\n",
      "epocht 2, batch_num 9600, step 216829, time: 1031.5478100776672 s, accu: 0.9705536365509033, loss_yt: 0.12445762753486633\n",
      "epocht 2, batch_num 9800, step 217029, time: 1052.859855413437 s, accu: 0.9705546498298645, loss_yt: 0.0645662471652031\n",
      "epocht 2, batch_num 10000, step 217229, time: 1073.9414134025574 s, accu: 0.9705551266670227, loss_yt: 0.042116355150938034\n",
      "epocht 2, batch_num 10200, step 217429, time: 1095.002342224121 s, accu: 0.9705557823181152, loss_yt: 0.039944302290678024\n",
      "epocht 2, batch_num 10400, step 217629, time: 1116.128446817398 s, accu: 0.9705561995506287, loss_yt: 0.07171584665775299\n",
      "epocht 2, batch_num 10600, step 217829, time: 1137.1732213497162 s, accu: 0.9705571532249451, loss_yt: 0.07250162214040756\n",
      "epocht 2, batch_num 10800, step 218029, time: 1158.4062747955322 s, accu: 0.9705579876899719, loss_yt: 0.05952219292521477\n",
      "epocht 2, batch_num 11000, step 218229, time: 1179.6678457260132 s, accu: 0.9705584645271301, loss_yt: 0.04438285529613495\n",
      "epocht 2, batch_num 11200, step 218429, time: 1200.9672055244446 s, accu: 0.9705590009689331, loss_yt: 0.06882227957248688\n",
      "epocht 2, batch_num 11400, step 218629, time: 1222.2792236804962 s, accu: 0.9705603122711182, loss_yt: 0.03880903869867325\n",
      "epocht 2, batch_num 11600, step 218829, time: 1243.6597604751587 s, accu: 0.9705606698989868, loss_yt: 0.1089547723531723\n",
      "epocht 2, batch_num 11800, step 219029, time: 1264.9445695877075 s, accu: 0.9705619215965271, loss_yt: 0.06955380737781525\n",
      "epocht 2, batch_num 12000, step 219229, time: 1286.2950747013092 s, accu: 0.9705619812011719, loss_yt: 0.06425701826810837\n",
      "epocht 2, batch_num 12200, step 219429, time: 1307.6390764713287 s, accu: 0.9705626964569092, loss_yt: 0.10491342842578888\n",
      "epocht 2, batch_num 12400, step 219629, time: 1328.9529044628143 s, accu: 0.9705633521080017, loss_yt: 0.07528965175151825\n",
      "epocht 2, batch_num 12600, step 219829, time: 1350.1699471473694 s, accu: 0.9705642461776733, loss_yt: 0.08030666410923004\n",
      "epocht 2, batch_num 12800, step 220029, time: 1371.4303386211395 s, accu: 0.9705644845962524, loss_yt: 0.06153412163257599\n",
      "epocht 2, batch_num 13000, step 220229, time: 1392.6915247440338 s, accu: 0.9705649018287659, loss_yt: 0.059229932725429535\n",
      "epocht 2, batch_num 13200, step 220429, time: 1413.98632979393 s, accu: 0.9705651998519897, loss_yt: 0.07806235551834106\n",
      "epocht 2, batch_num 13400, step 220629, time: 1435.4152460098267 s, accu: 0.9705653786659241, loss_yt: 0.08326790481805801\n",
      "epocht 2, batch_num 13600, step 220829, time: 1456.769956111908 s, accu: 0.9705659747123718, loss_yt: 0.06666718423366547\n",
      "epocht 2, batch_num 13800, step 221029, time: 1478.1904594898224 s, accu: 0.9705665707588196, loss_yt: 0.08006320893764496\n",
      "epocht 2, batch_num 14000, step 221229, time: 1499.4929177761078 s, accu: 0.9705668687820435, loss_yt: 0.06229623779654503\n",
      "epocht 2, batch_num 14200, step 221429, time: 1520.9193680286407 s, accu: 0.9705675840377808, loss_yt: 0.08159960806369781\n",
      "epocht 2, batch_num 14400, step 221629, time: 1542.3658292293549 s, accu: 0.9705675840377808, loss_yt: 0.06527403742074966\n",
      "epocht 2, batch_num 14600, step 221829, time: 1563.758313179016 s, accu: 0.9705684185028076, loss_yt: 0.062137454748153687\n",
      "epocht 2, batch_num 14800, step 222029, time: 1585.1870346069336 s, accu: 0.9705689549446106, loss_yt: 0.07621905207633972\n",
      "iter_validnum 3701\n",
      "epochv 2, step 222030, stop_n 0, time: 1625.2578511238098 s, accu_va: 0.9705731205700604, loss_yv: 0.07349540246008893\n",
      "iter_trainnum 14802\n",
      "epocht 3, batch_num 0, step 222031, time: 11.81675410270691 s, accu: 0.9705789089202881, loss_yt: 0.07059717178344727\n",
      "epocht 3, batch_num 200, step 222231, time: 33.19425964355469 s, accu: 0.9705801606178284, loss_yt: 0.05043325573205948\n",
      "epocht 3, batch_num 400, step 222431, time: 54.26197528839111 s, accu: 0.9705800414085388, loss_yt: 0.08727753162384033\n",
      "epocht 3, batch_num 600, step 222631, time: 75.4465765953064 s, accu: 0.970580518245697, loss_yt: 0.1221630722284317\n",
      "epocht 3, batch_num 800, step 222831, time: 96.57646822929382 s, accu: 0.9705809950828552, loss_yt: 0.07081595063209534\n",
      "epocht 3, batch_num 1000, step 223031, time: 117.64290833473206 s, accu: 0.9705820083618164, loss_yt: 0.08904585242271423\n",
      "epocht 3, batch_num 1200, step 223231, time: 139.08365392684937 s, accu: 0.9705817699432373, loss_yt: 0.05236094444990158\n",
      "epocht 3, batch_num 1400, step 223431, time: 160.33399724960327 s, accu: 0.9705826640129089, loss_yt: 0.057440612465143204\n",
      "epocht 3, batch_num 1600, step 223631, time: 181.4259843826294 s, accu: 0.9705830216407776, loss_yt: 0.07879125326871872\n",
      "epocht 3, batch_num 1800, step 223831, time: 202.72089004516602 s, accu: 0.970583975315094, loss_yt: 0.08143102377653122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 3, batch_num 2000, step 224031, time: 223.8474268913269 s, accu: 0.9705846309661865, loss_yt: 0.05761588737368584\n",
      "epocht 3, batch_num 2200, step 224231, time: 245.0422830581665 s, accu: 0.9705851674079895, loss_yt: 0.05864004045724869\n",
      "epocht 3, batch_num 2400, step 224431, time: 266.2434401512146 s, accu: 0.9705854654312134, loss_yt: 0.08983203023672104\n",
      "epocht 3, batch_num 2600, step 224631, time: 287.5436305999756 s, accu: 0.9705860018730164, loss_yt: 0.0359339565038681\n",
      "epocht 3, batch_num 2800, step 224831, time: 308.7733063697815 s, accu: 0.9705865979194641, loss_yt: 0.0727292150259018\n",
      "epocht 3, batch_num 3000, step 225031, time: 330.1507761478424 s, accu: 0.9705871939659119, loss_yt: 0.067542165517807\n",
      "epocht 3, batch_num 3200, step 225231, time: 351.47486901283264 s, accu: 0.9705875515937805, loss_yt: 0.06496710330247879\n",
      "epocht 3, batch_num 3400, step 225431, time: 372.55325746536255 s, accu: 0.9705880880355835, loss_yt: 0.06763254106044769\n",
      "epocht 3, batch_num 3600, step 225631, time: 393.7366166114807 s, accu: 0.9705893397331238, loss_yt: 0.053574517369270325\n",
      "epocht 3, batch_num 3800, step 225831, time: 415.0637619495392 s, accu: 0.9705900549888611, loss_yt: 0.09119146317243576\n",
      "epocht 3, batch_num 4000, step 226031, time: 436.3319413661957 s, accu: 0.970590353012085, loss_yt: 0.07454749196767807\n",
      "epocht 3, batch_num 4200, step 226231, time: 457.4501678943634 s, accu: 0.9705906510353088, loss_yt: 0.09599743038415909\n",
      "epocht 3, batch_num 4400, step 226431, time: 478.7056279182434 s, accu: 0.9705917835235596, loss_yt: 0.0857488140463829\n",
      "epocht 3, batch_num 4600, step 226631, time: 500.10479617118835 s, accu: 0.9705924391746521, loss_yt: 0.08040887117385864\n",
      "epocht 3, batch_num 4800, step 226831, time: 521.4776027202606 s, accu: 0.9705930352210999, loss_yt: 0.09375660866498947\n",
      "epocht 3, batch_num 5000, step 227031, time: 542.6567239761353 s, accu: 0.9705934524536133, loss_yt: 0.04926338791847229\n",
      "epocht 3, batch_num 5200, step 227231, time: 563.8827135562897 s, accu: 0.9705944061279297, loss_yt: 0.060771018266677856\n",
      "epocht 3, batch_num 5400, step 227431, time: 585.0868630409241 s, accu: 0.9705944657325745, loss_yt: 0.06669840216636658\n",
      "epocht 3, batch_num 5600, step 227631, time: 606.299435377121 s, accu: 0.9705953001976013, loss_yt: 0.06020185723900795\n",
      "epocht 3, batch_num 5800, step 227831, time: 627.331422328949 s, accu: 0.9705960750579834, loss_yt: 0.06330763548612595\n",
      "epocht 3, batch_num 6000, step 228031, time: 648.7160346508026 s, accu: 0.9705965518951416, loss_yt: 0.09705732017755508\n",
      "epocht 3, batch_num 6200, step 228231, time: 670.0222325325012 s, accu: 0.9705970883369446, loss_yt: 0.07451624423265457\n",
      "epocht 3, batch_num 6400, step 228431, time: 691.3255667686462 s, accu: 0.9705979824066162, loss_yt: 0.05612790957093239\n",
      "epocht 3, batch_num 6600, step 228631, time: 712.6008236408234 s, accu: 0.9705983996391296, loss_yt: 0.046850815415382385\n",
      "epocht 3, batch_num 6800, step 228831, time: 733.8541069030762 s, accu: 0.9705985188484192, loss_yt: 0.052155233919620514\n",
      "epocht 3, batch_num 7000, step 229031, time: 755.1185910701752 s, accu: 0.9705987572669983, loss_yt: 0.1377556473016739\n",
      "epocht 3, batch_num 7200, step 229231, time: 776.3956680297852 s, accu: 0.9705992341041565, loss_yt: 0.058140452951192856\n",
      "epocht 3, batch_num 7400, step 229431, time: 797.5626175403595 s, accu: 0.9706002473831177, loss_yt: 0.10229331254959106\n",
      "epocht 3, batch_num 7600, step 229631, time: 818.7025496959686 s, accu: 0.9706009030342102, loss_yt: 0.06791965663433075\n",
      "epocht 3, batch_num 7800, step 229831, time: 840.0788741111755 s, accu: 0.9706008434295654, loss_yt: 0.08568282425403595\n",
      "epocht 3, batch_num 8000, step 230031, time: 861.221572637558 s, accu: 0.9706008434295654, loss_yt: 0.08579757809638977\n",
      "epocht 3, batch_num 8200, step 230231, time: 882.481921672821 s, accu: 0.9706012010574341, loss_yt: 0.06922193616628647\n",
      "epocht 3, batch_num 8400, step 230431, time: 903.7895476818085 s, accu: 0.9706023931503296, loss_yt: 0.07598438858985901\n",
      "epocht 3, batch_num 8600, step 230631, time: 925.1222004890442 s, accu: 0.9706025123596191, loss_yt: 0.08596307784318924\n",
      "epocht 3, batch_num 8800, step 230831, time: 946.2859117984772 s, accu: 0.9706024527549744, loss_yt: 0.07088527083396912\n",
      "epocht 3, batch_num 9000, step 231031, time: 967.6511251926422 s, accu: 0.9706035256385803, loss_yt: 0.05015608295798302\n",
      "epocht 3, batch_num 9200, step 231231, time: 988.9591059684753 s, accu: 0.9706038236618042, loss_yt: 0.06758404523134232\n",
      "epocht 3, batch_num 9400, step 231431, time: 1010.2958307266235 s, accu: 0.9706043004989624, loss_yt: 0.02729547768831253\n",
      "epocht 3, batch_num 9600, step 231631, time: 1031.4286286830902 s, accu: 0.9706051349639893, loss_yt: 0.06364203989505768\n",
      "epocht 3, batch_num 9800, step 231831, time: 1052.5924756526947 s, accu: 0.9706054925918579, loss_yt: 0.07321517169475555\n",
      "epocht 3, batch_num 10000, step 232031, time: 1073.9716885089874 s, accu: 0.97060626745224, loss_yt: 0.07096554338932037\n",
      "epocht 3, batch_num 10200, step 232231, time: 1095.2426767349243 s, accu: 0.9706058502197266, loss_yt: 0.06839619576931\n",
      "epocht 3, batch_num 10400, step 232431, time: 1116.5480444431305 s, accu: 0.9706068634986877, loss_yt: 0.08336842060089111\n",
      "epocht 3, batch_num 10600, step 232631, time: 1137.7381174564362 s, accu: 0.9706073999404907, loss_yt: 0.07804522663354874\n",
      "epocht 3, batch_num 10800, step 232831, time: 1158.8829596042633 s, accu: 0.9706079959869385, loss_yt: 0.07254056632518768\n",
      "epocht 3, batch_num 11000, step 233031, time: 1180.1668837070465 s, accu: 0.9706076979637146, loss_yt: 0.04967908188700676\n",
      "epocht 3, batch_num 11200, step 233231, time: 1201.429038286209 s, accu: 0.9706085920333862, loss_yt: 0.09664461016654968\n",
      "epocht 3, batch_num 11400, step 233431, time: 1222.757437467575 s, accu: 0.9706094264984131, loss_yt: 0.07636260986328125\n",
      "epocht 3, batch_num 11600, step 233631, time: 1243.9767282009125 s, accu: 0.9706097841262817, loss_yt: 0.07531219720840454\n",
      "epocht 3, batch_num 11800, step 233831, time: 1265.3418319225311 s, accu: 0.970610499382019, loss_yt: 0.07607248425483704\n",
      "epocht 3, batch_num 12000, step 234031, time: 1286.698987007141 s, accu: 0.9706113338470459, loss_yt: 0.06753218173980713\n",
      "epocht 3, batch_num 12200, step 234231, time: 1307.9040579795837 s, accu: 0.9706112742424011, loss_yt: 0.0851973220705986\n",
      "epocht 3, batch_num 12400, step 234431, time: 1328.817681312561 s, accu: 0.9706116318702698, loss_yt: 0.09712149947881699\n",
      "epocht 3, batch_num 12600, step 234631, time: 1349.2733988761902 s, accu: 0.9706119298934937, loss_yt: 0.08344092965126038\n",
      "epocht 3, batch_num 12800, step 234831, time: 1369.8238468170166 s, accu: 0.9706129431724548, loss_yt: 0.0646887719631195\n",
      "epocht 3, batch_num 13000, step 235031, time: 1390.5348582267761 s, accu: 0.9706138372421265, loss_yt: 0.06705634295940399\n",
      "epocht 3, batch_num 13200, step 235231, time: 1411.2758536338806 s, accu: 0.9706149101257324, loss_yt: 0.07667291909456253\n",
      "epocht 3, batch_num 13400, step 235431, time: 1431.9500319957733 s, accu: 0.9706159234046936, loss_yt: 0.06277655065059662\n",
      "epocht 3, batch_num 13600, step 235631, time: 1452.6899843215942 s, accu: 0.9706165194511414, loss_yt: 0.09834197908639908\n",
      "epocht 3, batch_num 13800, step 235831, time: 1473.189948797226 s, accu: 0.9706166386604309, loss_yt: 0.08016033470630646\n",
      "epocht 3, batch_num 14000, step 236031, time: 1493.9181008338928 s, accu: 0.9706166982650757, loss_yt: 0.060363322496414185\n",
      "epocht 3, batch_num 14200, step 236231, time: 1515.1217629909515 s, accu: 0.9706161022186279, loss_yt: 0.09747298061847687\n",
      "epocht 3, batch_num 14400, step 236431, time: 1536.0554807186127 s, accu: 0.9706168174743652, loss_yt: 0.04200214147567749\n",
      "epocht 3, batch_num 14600, step 236631, time: 1556.7740695476532 s, accu: 0.9706172943115234, loss_yt: 0.0770285502076149\n",
      "epocht 3, batch_num 14800, step 236831, time: 1577.7135424613953 s, accu: 0.9706173539161682, loss_yt: 0.10229384899139404\n",
      "iter_validnum 3701\n",
      "epochv 3, step 236832, stop_n 0, time: 1618.7902936935425 s, accu_va: 0.9706232119688825, loss_yv: 0.07353522759147332\n",
      "iter_trainnum 14802\n",
      "epocht 3, batch_num 0, step 236833, time: 10.946061372756958 s, accu: 0.9706265926361084, loss_yt: 0.04366090148687363\n",
      "epocht 3, batch_num 200, step 237033, time: 32.233391761779785 s, accu: 0.9706271290779114, loss_yt: 0.08067724853754044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 3, batch_num 400, step 237233, time: 53.63523554801941 s, accu: 0.9706276655197144, loss_yt: 0.08807779848575592\n",
      "epocht 3, batch_num 600, step 237433, time: 74.88936495780945 s, accu: 0.9706286191940308, loss_yt: 0.05373561754822731\n",
      "epocht 3, batch_num 800, step 237633, time: 96.11623191833496 s, accu: 0.970629096031189, loss_yt: 0.08517591655254364\n",
      "epocht 3, batch_num 1000, step 237833, time: 117.44735670089722 s, accu: 0.9706288576126099, loss_yt: 0.07440147548913956\n",
      "epocht 3, batch_num 1200, step 238033, time: 138.48130917549133 s, accu: 0.9706291556358337, loss_yt: 0.06266561895608902\n",
      "epocht 3, batch_num 1400, step 238233, time: 159.90034866333008 s, accu: 0.9706293940544128, loss_yt: 0.05148157849907875\n",
      "epocht 3, batch_num 1600, step 238433, time: 181.15789985656738 s, accu: 0.9706297516822815, loss_yt: 0.04402722790837288\n",
      "epocht 3, batch_num 1800, step 238633, time: 202.24710702896118 s, accu: 0.9706307053565979, loss_yt: 0.08894020318984985\n",
      "epocht 3, batch_num 2000, step 238833, time: 223.40062046051025 s, accu: 0.9706310033798218, loss_yt: 0.06569132953882217\n",
      "epocht 3, batch_num 2200, step 239033, time: 244.51308941841125 s, accu: 0.9706315994262695, loss_yt: 0.09332072734832764\n",
      "epocht 3, batch_num 2400, step 239233, time: 265.913446187973 s, accu: 0.9706310033798218, loss_yt: 0.0637962743639946\n",
      "epocht 3, batch_num 2600, step 239433, time: 287.01116013526917 s, accu: 0.9706313610076904, loss_yt: 0.07860296219587326\n",
      "epocht 3, batch_num 2800, step 239633, time: 308.3685624599457 s, accu: 0.9706323742866516, loss_yt: 0.10731613636016846\n",
      "epocht 3, batch_num 3000, step 239833, time: 329.6345160007477 s, accu: 0.9706332683563232, loss_yt: 0.07435057312250137\n",
      "epocht 3, batch_num 3200, step 240033, time: 350.9537887573242 s, accu: 0.9706339240074158, loss_yt: 0.08474147319793701\n",
      "epocht 3, batch_num 3400, step 240233, time: 372.14649534225464 s, accu: 0.9706339836120605, loss_yt: 0.07255753129720688\n",
      "epocht 3, batch_num 3600, step 240433, time: 393.2941071987152 s, accu: 0.9706339836120605, loss_yt: 0.0912569910287857\n",
      "epocht 3, batch_num 3800, step 240633, time: 414.5941882133484 s, accu: 0.9706341624259949, loss_yt: 0.05856388807296753\n",
      "epocht 3, batch_num 4000, step 240833, time: 435.97303581237793 s, accu: 0.9706333875656128, loss_yt: 0.07249529659748077\n",
      "epocht 3, batch_num 4200, step 241033, time: 457.2780635356903 s, accu: 0.9706336259841919, loss_yt: 0.06829579919576645\n",
      "epocht 3, batch_num 4400, step 241233, time: 478.52522683143616 s, accu: 0.9706340432167053, loss_yt: 0.08976998925209045\n",
      "epocht 3, batch_num 4600, step 241433, time: 499.93028235435486 s, accu: 0.9706349968910217, loss_yt: 0.07739146053791046\n",
      "epocht 3, batch_num 4800, step 241633, time: 521.073415517807 s, accu: 0.9706356525421143, loss_yt: 0.09180882573127747\n",
      "epocht 3, batch_num 5000, step 241833, time: 542.4975733757019 s, accu: 0.9706363677978516, loss_yt: 0.09190353751182556\n",
      "epocht 3, batch_num 5200, step 242033, time: 563.817480802536 s, accu: 0.9706375002861023, loss_yt: 0.052804768085479736\n",
      "epocht 3, batch_num 5400, step 242233, time: 585.0518114566803 s, accu: 0.9706382751464844, loss_yt: 0.048207640647888184\n",
      "epocht 3, batch_num 5600, step 242433, time: 606.2375237941742 s, accu: 0.9706389307975769, loss_yt: 0.05051108077168465\n",
      "epocht 3, batch_num 5800, step 242633, time: 627.5353388786316 s, accu: 0.9706390500068665, loss_yt: 0.0789240300655365\n",
      "epocht 3, batch_num 6000, step 242833, time: 648.8532736301422 s, accu: 0.9706395864486694, loss_yt: 0.084706149995327\n",
      "epocht 3, batch_num 6200, step 243033, time: 669.9664671421051 s, accu: 0.9706401228904724, loss_yt: 0.08815564960241318\n",
      "epocht 3, batch_num 6400, step 243233, time: 691.1957306861877 s, accu: 0.9706406593322754, loss_yt: 0.07941132038831711\n",
      "epocht 3, batch_num 6600, step 243433, time: 712.4173846244812 s, accu: 0.9706408381462097, loss_yt: 0.10788014531135559\n",
      "epocht 3, batch_num 6800, step 243633, time: 733.662478685379 s, accu: 0.9706407785415649, loss_yt: 0.08827526867389679\n",
      "epocht 3, batch_num 7000, step 243833, time: 754.8663837909698 s, accu: 0.9706413149833679, loss_yt: 0.05492345988750458\n",
      "epocht 3, batch_num 7200, step 244033, time: 776.057724237442 s, accu: 0.9706417322158813, loss_yt: 0.09221932291984558\n",
      "epocht 3, batch_num 7400, step 244233, time: 797.2368719577789 s, accu: 0.9706418514251709, loss_yt: 0.0821763128042221\n",
      "epocht 3, batch_num 7600, step 244433, time: 818.50066614151 s, accu: 0.9706422090530396, loss_yt: 0.08643694967031479\n",
      "epocht 3, batch_num 7800, step 244633, time: 839.8180110454559 s, accu: 0.9706433415412903, loss_yt: 0.0876263827085495\n",
      "epocht 3, batch_num 8000, step 244833, time: 861.1413817405701 s, accu: 0.9706437587738037, loss_yt: 0.07262013107538223\n",
      "epocht 3, batch_num 8200, step 245033, time: 882.3660326004028 s, accu: 0.9706441164016724, loss_yt: 0.052428238093853\n",
      "epocht 3, batch_num 8400, step 245233, time: 903.6155564785004 s, accu: 0.9706441760063171, loss_yt: 0.08062338083982468\n",
      "epocht 3, batch_num 8600, step 245433, time: 924.6835842132568 s, accu: 0.9706446528434753, loss_yt: 0.08206383883953094\n",
      "epocht 3, batch_num 8800, step 245633, time: 945.882639169693 s, accu: 0.9706453680992126, loss_yt: 0.09006065875291824\n",
      "epocht 3, batch_num 9000, step 245833, time: 967.2267000675201 s, accu: 0.9706456065177917, loss_yt: 0.06826389580965042\n",
      "epocht 3, batch_num 9200, step 246033, time: 988.4860708713531 s, accu: 0.9706465601921082, loss_yt: 0.08045494556427002\n",
      "epocht 3, batch_num 9400, step 246233, time: 1009.7321264743805 s, accu: 0.9706470370292664, loss_yt: 0.05826164782047272\n",
      "epocht 3, batch_num 9600, step 246433, time: 1030.9531652927399 s, accu: 0.9706476926803589, loss_yt: 0.0701560229063034\n",
      "epocht 3, batch_num 9800, step 246633, time: 1052.1737577915192 s, accu: 0.9706481695175171, loss_yt: 0.0781753659248352\n",
      "epocht 3, batch_num 10000, step 246833, time: 1073.470048904419 s, accu: 0.970648467540741, loss_yt: 0.06320863962173462\n",
      "epocht 3, batch_num 10200, step 247033, time: 1094.7525165081024 s, accu: 0.9706488251686096, loss_yt: 0.06805199384689331\n",
      "epocht 3, batch_num 10400, step 247233, time: 1116.0499317646027 s, accu: 0.9706497192382812, loss_yt: 0.07383939623832703\n",
      "epocht 3, batch_num 10600, step 247433, time: 1137.1862397193909 s, accu: 0.9706499576568604, loss_yt: 0.0944153219461441\n",
      "epocht 3, batch_num 10800, step 247633, time: 1158.4318783283234 s, accu: 0.9706509709358215, loss_yt: 0.09028521180152893\n",
      "epocht 3, batch_num 11000, step 247833, time: 1179.735579252243 s, accu: 0.9706506729125977, loss_yt: 0.07438008487224579\n",
      "epocht 3, batch_num 11200, step 248033, time: 1201.0915098190308 s, accu: 0.9706513285636902, loss_yt: 0.10522843897342682\n",
      "epocht 3, batch_num 11400, step 248233, time: 1222.3811190128326 s, accu: 0.9706520438194275, loss_yt: 0.07724737375974655\n",
      "epocht 3, batch_num 11600, step 248433, time: 1243.609393119812 s, accu: 0.9706523418426514, loss_yt: 0.07814425230026245\n",
      "epocht 3, batch_num 11800, step 248633, time: 1264.8574364185333 s, accu: 0.9706524014472961, loss_yt: 0.05382702127099037\n",
      "epocht 3, batch_num 12000, step 248833, time: 1286.1184232234955 s, accu: 0.9706531167030334, loss_yt: 0.08048209547996521\n",
      "epocht 3, batch_num 12200, step 249033, time: 1307.2636365890503 s, accu: 0.970653235912323, loss_yt: 0.0806909129023552\n",
      "epocht 3, batch_num 12400, step 249233, time: 1328.4693582057953 s, accu: 0.9706543684005737, loss_yt: 0.05948120355606079\n",
      "epocht 3, batch_num 12600, step 249433, time: 1349.8264663219452 s, accu: 0.9706538915634155, loss_yt: 0.06446860730648041\n",
      "epocht 3, batch_num 12800, step 249633, time: 1371.1505992412567 s, accu: 0.9706546068191528, loss_yt: 0.05352512374520302\n",
      "epocht 3, batch_num 13000, step 249833, time: 1392.2616245746613 s, accu: 0.9706546664237976, loss_yt: 0.08486096560955048\n",
      "epocht 3, batch_num 13200, step 250033, time: 1413.342572927475 s, accu: 0.9706555008888245, loss_yt: 0.054952770471572876\n",
      "epocht 3, batch_num 13400, step 250233, time: 1434.5486812591553 s, accu: 0.9706557393074036, loss_yt: 0.05745575577020645\n",
      "epocht 3, batch_num 13600, step 250433, time: 1455.7381405830383 s, accu: 0.9706562757492065, loss_yt: 0.07063135504722595\n",
      "epocht 3, batch_num 13800, step 250633, time: 1477.0498485565186 s, accu: 0.97065669298172, loss_yt: 0.0956525057554245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 3, batch_num 14000, step 250833, time: 1498.2168254852295 s, accu: 0.9706575274467468, loss_yt: 0.08927468955516815\n",
      "epocht 3, batch_num 14200, step 251033, time: 1519.4311270713806 s, accu: 0.970658540725708, loss_yt: 0.06364771723747253\n",
      "epocht 3, batch_num 14400, step 251233, time: 1540.7958235740662 s, accu: 0.9706593155860901, loss_yt: 0.07846654206514359\n",
      "epocht 3, batch_num 14600, step 251433, time: 1561.9832270145416 s, accu: 0.9706593155860901, loss_yt: 0.04813617840409279\n",
      "epocht 3, batch_num 14800, step 251633, time: 1583.270447731018 s, accu: 0.9706598520278931, loss_yt: 0.038823358714580536\n",
      "iter_validnum 3701\n",
      "epochv 3, step 251634, stop_n 0, time: 1624.1005392074585 s, accu_va: 0.9706646827258667, loss_yv: 0.07361778085238545\n",
      "iter_trainnum 14802\n",
      "epocht 3, batch_num 0, step 251635, time: 11.499691247940063 s, accu: 0.9706684350967407, loss_yt: 0.0537378266453743\n",
      "epocht 3, batch_num 200, step 251835, time: 32.69783091545105 s, accu: 0.9706693887710571, loss_yt: 0.1010352298617363\n",
      "epocht 3, batch_num 400, step 252035, time: 53.970442056655884 s, accu: 0.9706701040267944, loss_yt: 0.054146185517311096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a56a3f39c540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# batch_size, num_epochs = 512, 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mglobalstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelcrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainnormalpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e72f925b6ab7>\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(self, trainpd, labels, batch_size, num_epochs, retrain)\u001b[0m\n\u001b[1;32m    265\u001b[0m                             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                             \u001b[0maccu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m                             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# batch_size, num_epochs = 4096, 1000\n",
    "# batch_size, num_epochs = 512, 1000\n",
    "batch_size, num_epochs = 512, 1000\n",
    "globalstep = modelcrnn.batch_train(trainnormalpd, labels, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "INFO:tensorflow:Restoring parameters from ../data/particles/model/modelevery_mul4_modeltailv2/v2-252035\n"
     ]
    }
   ],
   "source": [
    "y_pred = modelcrnn.predict(testpd[feature])\n",
    "fy_submission = np.squeeze(y_pred)\n",
    "fy_submission = (fy_submission - fy_submission.min()) / (fy_submission.max() - fy_submission.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#阈值大概在0.2-0.4之间 本题对召回率较敏感，可适当降低一下阈值\n",
    "thre = 0.5\n",
    "#生成提交文件\n",
    "sub = pd.DataFrame()\n",
    "sub['hit_id'] = testpd['hit_id']\n",
    "sub['flag_pred'] = fy_submission\n",
    "sub['event_id'] = testpd['event_id']\n",
    "sub['flag_pred'] = sub['flag_pred'].apply(lambda x: 1 if x >= thre else 0)\n",
    "sub.to_csv(os.path.join(pathf, \"subsample.csv\").format(sub['flag_pred'].mean()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}