{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from scipy.stats import entropy, kurtosis\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from IPython.display import display\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('max_columns', None)\n",
    "# pd.set_option('max_rows', None)\n",
    "pd.set_option('float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x           y  z          t   terror        q  flag  event_id  \\\n",
      "0 -142.500000 -147.500000  0 767.879000 2.029660 1.050520     0         7   \n",
      "\n",
      "   hit_id  \n",
      "0       1  \n",
      "(9473201, 9)\n",
      "   event_id  nhit  nhitreal     energymc   thetamc     phimc       xcmc  \\\n",
      "0         7   426        70 48348.900000 63.168600 11.098200 -40.830000   \n",
      "\n",
      "        ycmc  \n",
      "0 114.030000  \n",
      "(13315, 8)\n",
      "            x           y  z          t   terror        q  event_id  hit_id\n",
      "0 -142.500000 -127.500000  0 848.061000 1.998400 1.150670         9       1\n",
      "(4086511, 8)\n"
     ]
    }
   ],
   "source": [
    "pathf = os.path.join(\"..\", \"data\", \"particles\")\n",
    "\n",
    "trainpd = pd.read_csv(os.path.join(pathf, \"train.csv\"))\n",
    "print(trainpd.head(1))\n",
    "trainshape = trainpd.shape\n",
    "print(trainshape)\n",
    "eventpd = pd.read_csv(os.path.join(pathf, \"event.csv\"))\n",
    "print(eventpd.head(1))\n",
    "print(eventpd.shape)\n",
    "testpd = pd.read_csv(os.path.join(pathf, \"test.csv\"))\n",
    "testshape = testpd.shape\n",
    "print(testpd.head(1))\n",
    "print(testpd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (k(q,mc)*(t0+L))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+L) = (t+L)^2\n",
    "# t0 方程 \n",
    "# a = k(q,mc)^2\n",
    "# b = 2*L*k(q,mc)^2 -dis*cos(phi)*sin(thmc)\n",
    "# c = L^2 * k(q,mc)^2 + dis^2 - dis*cos(phi)*sin(thmc)*L - (t+L)^2 \n",
    "# t0 = (-b +- (b^2 - 4*a*c)^(1/2))/2*a\n",
    "data = pd.concat([trainpd, testpd], ignore_index=True)\n",
    "data = pd.merge(data, eventpd, on='event_id', how='left')\n",
    "\n",
    "data['fx'] = data['x'] - data['xcmc']\n",
    "data['fy'] = data['y'] - data['ycmc']\n",
    "data['phimc'] = data['phimc'] * np.pi / 180.\n",
    "data['fphi'] = np.arctan2(data['fy'], data['fx']) - data['phimc']\n",
    "data['fdis'] = np.sqrt(data['fx'] ** 2 + data['fy'] ** 2)\n",
    "data['thetamc'] = data['thetamc'] * np.pi / 180.\n",
    "\n",
    "data['fsinthmc'] = np.sin(data['thetamc'])\n",
    "data['fsinthmc_v'] = 1.0/data['fsinthmc']\n",
    "data['fcosphi'] = np.cos(data['fphi'])\n",
    "data['fcosphi_v'] = 1.0/data['fcosphi']\n",
    "\n",
    "data['fcosthmc'] = np.cos(data['thetamc'])\n",
    "data['fcosthmc_v'] = 1.0/data['fcosthmc']\n",
    "data['fsinphi'] = np.sin(data['fphi'])\n",
    "data['fsinphi_v'] = 1.0/data['fsinphi']\n",
    "\n",
    "data['ftanphi'] = np.tan(data['fphi'])\n",
    "data['ftanphi_v'] = 1.0/data['ftanphi']\n",
    "data['ftanthmc'] = np.tan(data['thetamc'])\n",
    "data['ftanthmc_v'] = 1.0/data['ftanthmc']\n",
    "\n",
    "\n",
    "# data['ft2'] = data['t'] ** 2\n",
    "# data['fdis2'] = data['fdis'] ** 2\n",
    "\n",
    "data['fttrue'] = data['t'] / data['terror']\n",
    "data['terror_v'] = 1.0 / data['terror']\n",
    "data['terror_v2'] =data['terror_v'] ** 2 \n",
    "data['fttrue_v'] = 1.0 / data['fttrue']\n",
    "data['fttrue2'] = data['fttrue'] ** 2\n",
    "data['fttrue2_v'] = 1.0 / data['fttrue2'] \n",
    "data['nhitratio'] = data['nhit'] / data['nhitreal']\n",
    "data['nhitratio_v'] = data['nhitratio']\n",
    "data['energymc_v'] = 1.0 / data['energymc']\n",
    "data['fenergymc2'] = data['energymc'] ** 2\n",
    "data['fenergymc2_v'] = 1.0 / data['fenergymc2'] \n",
    "# data['q_v'] = 1.0 / data['q']\n",
    "data['q2'] = data['q']\n",
    "# data['q2_v'] = 1.0 / data['q2']\n",
    "\n",
    "del data['fx']\n",
    "del data['fy']\n",
    "del data['x']\n",
    "del data['y']\n",
    "del data['z']\n",
    "del data['xcmc']\n",
    "del data['ycmc']\n",
    "del data['fphi']\n",
    "del data['phimc']\n",
    "del data['nhitreal']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_new = pd.DataFrame()\n",
    "info_new[\"event_id\"] = data.groupby([\"event_id\"])[\"event_id\"].mean()\n",
    "info_new[\"fdis_mean\"] = data.groupby([\"event_id\"])[\"fdis\"].mean()\n",
    "info_new[\"fdis_std\"] = data.groupby([\"event_id\"])[\"fdis\"].std()\n",
    "info_new[\"fdis_stdmean\"] = info_new[\"fdis_std\"] / info_new[\"fdis_mean\"]\n",
    "info_new[\"ft_min\"] = data.groupby([\"event_id\"])[\"t\"].min()\n",
    "info_new[\"ft_max\"] = data.groupby([\"event_id\"])[\"t\"].max()\n",
    "info_new[\"t_mean\"] = data.groupby([\"event_id\"])[\"t\"].mean()\n",
    "info_new[\"ft_std\"] = data.groupby([\"event_id\"])[\"t\"].std()\n",
    "info_new[\"ft_stdmean\"] = info_new[\"ft_std\"] / info_new[\"t_mean\"]\n",
    "info_new[\"ft_mean\"] = (info_new['t_mean']-info_new['ft_min']) / (info_new['ft_max']-info_new['ft_min'])\n",
    "info_new.reset_index(drop=True, inplace=True)\n",
    "data = pd.merge(data, info_new, on='event_id', how='left')\n",
    "\n",
    "# data['ft_rel'] = (data['t']-data['ft_min']) / (data['ft_max']-data['ft_min'])\n",
    "data['ft_rel'] = data['t'] / data['ft_std']\n",
    "# data['ft_rel_std'] = data['ft_rel'] / data['ft_std']\n",
    "data['ft2_rel'] = data['ft_rel'] ** 2\n",
    "# data['ft2_rel_std'] = data['ft_rel_std'] ** 2\n",
    "data['ft_rel_v'] = 1.0 / data['ft_rel']\n",
    "data['ft2_rel_v'] = 1.0 / data['ft2_rel'] \n",
    "# (k(q,mc)*(t0+L))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+L) = (t+L)^2\n",
    "data = data.sort_values(by=['event_id', 'ft_rel']).reset_index(drop=True)\n",
    "for i in [4, 6, 8, 10, 12]:\n",
    "# for i in [7, 8, 11,17, 47]:\n",
    "    data[f'ft_{i}diff'] = data.groupby('event_id')['ft_rel'].diff(periods=i).fillna(0)\n",
    "    \n",
    "data['fdis_rel'] = data['fdis'] / data['fdis_mean']\n",
    "data['fdis_rel_std'] = data['fdis_rel'] / data['fdis_std']\n",
    "data['fdis2_rel'] = data['fdis_rel'] ** 2\n",
    "data['fdis2_rel_std'] = data['fdis_rel_std'] ** 2\n",
    "\n",
    "data['fdis_t_rel'] = data['fdis_rel'] / data['ft_rel']\n",
    "data['fdis_tmean_rel'] = data['fdis_rel'] / data['ft_mean']\n",
    "data['fdis_t_rel'].fillna(0, inplace=True)\n",
    "# data['fdis-t_rel'] = data['fdis_rel'] * data['ft_rel']\n",
    "# data['fdis-tmean_rel'] = data['fdis_rel'] * data['ft_mean']\n",
    "# data['fdis_t_normal'] = data['fdis_rel'] / data['ft_normal']\n",
    "# data['fdis_t_normal'].fillna(0, inplace=True)\n",
    "# data['fdis-t_normal'] = data['fdis_rel'] * data['ft_normal']\n",
    "\n",
    "# data = data.sort_values(by=['event_id', 'fdis_rel']).reset_index(drop=True)\n",
    "# for i in [5, 6, 7, 8, 10, 11,12]:\n",
    "#     data[f'fdis_rel_{i}diff'] = data.groupby('event_id')['fdis_rel'].diff(periods=i).fillna(0)\n",
    "data['fx_normal'] = data['fcosphi'] * data['fdis_rel']\n",
    "# data['fy_normal'] = data['fsinphi'] * data['fdis_rel']\n",
    "\n",
    "data['fcossin'] = data['fcosphi'] * data['fsinthmc']\n",
    "data['fdis_relcossin'] =  data['fdis_rel'] * data['fcossin']\n",
    "data['fdis2_relcossin'] = data['fdis2_rel'] * data['fcossin']\n",
    "# data['fdis2_rel_stdcossin'] = data['fdis2_rel_std'] * data['fcossin']\n",
    "data['ft2_relcossin'] = data['ft2_rel'] * data['fcossin']\n",
    "\n",
    "data['fsincos'] = data['fsinphi'] * data['fcosthmc']\n",
    "# data['fdis_relsincos'] = data['fdis_rel'] * data['fsincos']\n",
    "# data['fdis2_relsincos'] = data['fdis2_rel'] * data['fsincos']\n",
    "# data['fdis2_rel_stdsincos'] = data['fdis2_rel_std'] * data['fsincos']\n",
    "# data['ft_relsincos'] =  data['ft_rel'] * data['fsincos']\n",
    "# data['ft2_relsincos'] = data['ft2_rel'] * data['fsincos']\n",
    "\n",
    "data['fsinsin'] = data['fsinphi'] * data['fsinthmc']\n",
    "# data['fdis_relsinsin'] =  data['fdis_rel'] * data['fsinsin']\n",
    "# data['fdis2_relsinsin'] = data['fdis2_rel'] * data['fsinsin']\n",
    "# data['fdis2_rel_stdsinsin'] = data['fdis2_rel_std'] * data['fsinsin']\n",
    "# data['ft_relsinsin'] =  data['ft_rel'] * data['fsinsin']\n",
    "# data['ft2_relsinsin'] = data['ft2_rel'] * data['fsinsin']\n",
    "\n",
    "data['fcoscos'] = data['fcosphi'] * data['fcosthmc']\n",
    "\n",
    "data['ft_relsinsin'] = data['ft_rel'] * data['fsinsin']\n",
    "data['ft_relcoscos'] = data['ft_rel'] * data['fcoscos']\n",
    "data['ft_relsincos'] = data['ft_rel'] * data['fsincos']\n",
    "data['ft_relcossin'] = data['ft_rel'] * data['fcossin']\n",
    "\n",
    "data['ft_rel_errcoscos'] = data['ft_relcoscos'] * data['terror']\n",
    "data['fdis2_rel_stdcoscos'] = data['fdis2_rel_std'] * data['fcoscos']\n",
    "data['ft2_relcoscos'] = data['ft2_rel'] * data['fcoscos']\n",
    "\n",
    "data['fdis_t_relcossin'] = data['fdis_relcossin'] / data['ft_rel']\n",
    "data['fdis_t_relcossin'].fillna(0, inplace=True)\n",
    "data['fdis-t_relcossin'] = data['fdis_relcossin'] * data['ft_rel']\n",
    "\n",
    "# # 要保留\n",
    "# del data['ft_7diff']\n",
    "# del data['ft_8diff']\n",
    "# del data['ft_11diff']\n",
    "# del data['ft_17diff']\n",
    "# del data['fdis_t_rel']\n",
    "# del data['fttrue2']\n",
    "# del data['ft_rel_errcoscos']\n",
    "# del data['fdis_t_relcossin']\n",
    "# del data['fdis-t_relcossin']\n",
    "# del data['q2']\n",
    "# del data['q']\n",
    "# del data['ft_47diff']\n",
    "# del data['ft2_rel_v']\n",
    "# del data['fttrue2_v']\n",
    "# del data['fttrue_v']\n",
    "# del data['fsinthmc_v']\n",
    "# del data['ft_rel']\n",
    "\n",
    "del data['t']\n",
    "del data['ft_relcossin']\n",
    "del data['ft2_rel']\n",
    "del data['ft_relcoscos']\n",
    "del data['ft_rel_v']\n",
    "del data['fcosphi_v']\n",
    "del data['ft2_relcossin']\n",
    "del data['ft2_relcoscos']\n",
    "del data['ftanphi']\n",
    "del data['ft_relsincos']\n",
    "del data['ft_relsinsin']\n",
    "del data['fttrue']\n",
    "del data['ftanphi_v']\n",
    "del data['fsinphi_v']\n",
    "\n",
    "# 要保留但不明显\n",
    "del data['fdis']\n",
    "del data['fdis2_rel']\n",
    "del data['fdis_tmean_rel']\n",
    "del data['terror_v2']\n",
    "del data['nhitratio']\n",
    "del data['fdis2_relcossin']\n",
    "del data['fdis2_rel_stdcoscos']\n",
    "del data['ftanthmc']\n",
    "del data['energymc_v']\n",
    "del data['nhitratio_v']\n",
    "del data['fdis_rel_std']\n",
    "del data['fenergymc2']\n",
    "del data['fdis2_rel_std']\n",
    "del data['fdis_stdmean']\n",
    "\n",
    "# 真删除\n",
    "del data['fx_normal']\n",
    "del data['fcossin']\n",
    "del data['fdis_relcossin']\n",
    "del data['terror_v']\n",
    "del data['ft_std']\n",
    "del data['fdis_rel']\n",
    "del data['ftanthmc_v']\n",
    "del data['fenergymc2_v']\n",
    "del data['fcoscos']\n",
    "del data['fcosthmc_v']\n",
    "del data['fdis_mean']\n",
    "del data['ft_mean']\n",
    "del data['nhit']\n",
    "del data['fdis_std']\n",
    "del data['t_mean']\n",
    "del data['energymc']\n",
    "del data['terror']\n",
    "del data['ft_stdmean']\n",
    "del data['ft_min']\n",
    "del data['ft_max']\n",
    "del data['fcosphi']\n",
    "del data['thetamc']\n",
    "del data['fsinthmc']\n",
    "del data['fcosthmc']\n",
    "del data['fsincos']\n",
    "del data['fsinsin']\n",
    "del data['fsinphi']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9473201\n",
      "(13559712, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainshape[0])\n",
    "print(data.shape)\n",
    "testpd = data[data.flag.isna()].reset_index()\n",
    "# data.loc[data.flag.isna() & (data.t < -900), 'flag'] = 0\n",
    "# data.loc[data.flag.isna() & ((data.t > 1850) | (data.q < 0)), 'flag'] = 1\n",
    "trainpd = data[data.flag.notna()].reset_index()\n",
    "trainpd['flag'] = trainpd['flag'].astype('int')\n",
    "# trainpd = data[:trainshape[0]].reset_index()\n",
    "# testpd = data[trainshape[0]:].reset_index()\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'q', 'flag', 'event_id', 'hit_id', 'fsinthmc_v', 'fttrue_v',\n",
      "       'fttrue2', 'fttrue2_v', 'q2', 'ft_rel', 'ft2_rel_v', 'ft_4diff',\n",
      "       'ft_6diff', 'ft_8diff', 'ft_10diff', 'ft_12diff', 'fdis_t_rel',\n",
      "       'ft_rel_errcoscos', 'fdis_t_relcossin', 'fdis-t_relcossin'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(trainpd.columns)\n",
    "feature = [x for x in trainpd.columns if x not in ['flag', 'index', 'hit_id', 'event_id']]\n",
    "# feature = [x for x in trainpd.columns if x not in ['flag', 'index', 'event_id']]\n",
    "labels = trainpd['flag']\n",
    "\n",
    "del trainpd['flag']\n",
    "del testpd['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [      3       4       5 ... 9473197 9473198 9473199]\n",
      "[09:18:59] DEBUG: /workspace/src/tree/updater_gpu_hist.cu:1167: [GPU Hist]: Configure\n",
      "[09:19:00] DEBUG: /workspace/src/common/device_helpers.cu:38: Running nccl init on: 9.0\n",
      "[09:19:00] ======== Monitor: DenseCuts ========\n",
      "[09:19:00] ======== Monitor: HistogramCuts ========\n",
      "[09:19:00] ======== Monitor: ellpack_page ========\n",
      "[09:19:00] WARNING: /workspace/src/common/timer.cc:88: Timer for Quantiles did not get stopped properly.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[09:19:00] /workspace/src/tree/updater_gpu_hist.cu:1058: Exception in gpu_hist: parallel_for failed: out of memory\n\nStack trace:\n  [bt] (0) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x54) [0x7f877eb40614]\n  [bt] (1) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(xgboost::tree::GPUHistMakerSpecialised<xgboost::detail::GradientPairInternal<double> >::Update(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, std::vector<xgboost::RegTree*, std::allocator<xgboost::RegTree*> > const&)+0x47f) [0x7f877ee23dcf]\n  [bt] (2) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(xgboost::gbm::GBTree::BoostNewTrees(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, int, std::vector<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> >, std::allocator<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> > > >*)+0xed0) [0x7f877ebef980]\n  [bt] (3) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(xgboost::gbm::GBTree::DoBoost(xgboost::DMatrix*, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::ObjFunction*)+0x121) [0x7f877ebf1b91]\n  [bt] (4) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x3e1) [0x7f877ec2ca11]\n  [bt] (5) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(XGBoosterUpdateOneIter+0x29) [0x7f877eb2f639]\n  [bt] (6) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f881f3a8ec0]\n  [bt] (7) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7f881f3a887d]\n  [bt] (8) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f881f5beede]\n\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-26fb3a54ac56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m                       (trainpd[feature].iloc[test_index], labels[test_index])],\n\u001b[1;32m     73\u001b[0m             \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    821\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    207\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1248\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [09:19:00] /workspace/src/tree/updater_gpu_hist.cu:1058: Exception in gpu_hist: parallel_for failed: out of memory\n\nStack trace:\n  [bt] (0) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x54) [0x7f877eb40614]\n  [bt] (1) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(xgboost::tree::GPUHistMakerSpecialised<xgboost::detail::GradientPairInternal<double> >::Update(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, std::vector<xgboost::RegTree*, std::allocator<xgboost::RegTree*> > const&)+0x47f) [0x7f877ee23dcf]\n  [bt] (2) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(xgboost::gbm::GBTree::BoostNewTrees(xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::DMatrix*, int, std::vector<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> >, std::allocator<std::unique_ptr<xgboost::RegTree, std::default_delete<xgboost::RegTree> > > >*)+0xed0) [0x7f877ebef980]\n  [bt] (3) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(xgboost::gbm::GBTree::DoBoost(xgboost::DMatrix*, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*, xgboost::ObjFunction*)+0x121) [0x7f877ebf1b91]\n  [bt] (4) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x3e1) [0x7f877ec2ca11]\n  [bt] (5) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/site-packages/xgboost/./lib/libxgboost.so(XGBoosterUpdateOneIter+0x29) [0x7f877eb2f639]\n  [bt] (6) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f881f3a8ec0]\n  [bt] (7) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7f881f3a887d]\n  [bt] (8) /home/aa/anaconda2/envs/pygpu36/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f881f5beede]\n\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "if \"xgb\"==\"xgb\":\n",
    "    n_splits = 2\n",
    "#     X, val_X, y, val_y = train_test_split(\n",
    "#         train_x,\n",
    "#         train_y,\n",
    "#         test_size=0.01,\n",
    "#         random_state=1,\n",
    "#         stratify=train_y\n",
    "#     )\n",
    "\n",
    "#     # xgb矩阵赋值\n",
    "#     xgb_val = xgb.DMatrix(val_X, label=val_y)\n",
    "#     xgb_train = xgb.DMatrix(X, label=y)\n",
    "#     xgb_test = xgb.DMatrix(test_x)\n",
    "\n",
    "#     # xgboost模型 #####################\n",
    "\n",
    "#     params = {\n",
    "#         'booster': 'gbtree',\n",
    "#         # 'objective': 'multi:softmax',  # 多分类的问题、\n",
    "#         # 'objective': 'multi:softprob',   # 多分类概率\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'eval_metric': 'logloss',\n",
    "#         # 'num_class': 9,  # 类别数，与 multisoftmax 并用\n",
    "#         'gamma': 0.1,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "#         'max_depth': 8,  # 构建树的深度，越大越容易过拟合\n",
    "#         'alpha': 0,   # L1正则化系数\n",
    "#         'lambda': 10,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "#         'subsample': 0.7,  # 随机采样训练样本\n",
    "#         'colsample_bytree': 0.5,  # 生成树时进行的列采样\n",
    "#         'min_child_weight': 3,\n",
    "#         # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "#         # ，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "#         # 这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "#         'silent': 0,  # 设置成1则没有运行信息输出，最好是设置为0.\n",
    "#         'eta': 0.03,  # 如同学习率\n",
    "#         'seed': 1000,\n",
    "#         'nthread': -1,  # cpu 线程数\n",
    "#         'missing': 1,\n",
    "#         'scale_pos_weight': (np.sum(y==0)/np.sum(y==1))  # 用来处理正负样本不均衡的问题,通常取：sum(negative cases) / sum(positive cases)\n",
    "#         # 'eval_metric': 'auc'\n",
    "#     }\n",
    "#     plst = list(params.items())\n",
    "#     num_rounds = 2000  # 迭代次数\n",
    "#     watchlist = [(xgb_train, 'train'), (xgb_val, 'val')]\n",
    "#     # 交叉验证\n",
    "#     result = xgb.cv(plst, xgb_train, num_boost_round=200, nfold=4, early_stopping_rounds=200, verbose_eval=True, folds=StratifiedKFold(n_splits=4).split(X, y))\n",
    "#     # 训练模型并保存\n",
    "#     # early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练\n",
    "#     model = xgb.train(plst, xgb_train, num_rounds, watchlist, early_stopping_rounds=200)\n",
    "#     model.save_model('../data/model/xgb.model')  # 用于存储训练出的模型\n",
    "\n",
    "#     preds = model.predict(xgb_test)\n",
    "\n",
    "#     # 导出结果\n",
    "#     threshold = 0.5\n",
    "#     for pred in preds:\n",
    "#         result = 1 if pred > threshold else 0\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=4399)\n",
    "    fy_submission = np.zeros(testshape[0])\n",
    "#     y_pp_xgb_stacking = np.zeros(len(labels))\n",
    "    for train_index, test_index in kf.split(trainpd):\n",
    "        print(\">>>\", train_index)\n",
    "        start_time = time.time()\n",
    "        clf = xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.05, verbosity=3,\n",
    "                                min_child_weight=1,colsample_bytree=0.7,subsample=1,\n",
    "                                eval_metric='auc', n_estimators=5000)\n",
    "#                                 eval_metric='auc', n_estimators=2000, predictor='cpu_predictor')\n",
    "        clf.fit(\n",
    "            trainpd[feature].iloc[train_index], labels[train_index],\n",
    "            eval_set=[(trainpd[feature].iloc[train_index], labels[train_index]),\n",
    "                      (trainpd[feature].iloc[test_index], labels[test_index])],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=True,\n",
    "        )\n",
    "        fig,ax = plt.subplots(figsize=(15,15))\n",
    "        for i1 in sorted({icon[0]:icon[1]for icon in zip(feature, clf.feature_importances_)}.items(), key=lambda x: -x[1]):\n",
    "            print(i1)\n",
    "        xgb.plot_importance(clf, height=0.5,max_num_features=None,ax=ax,importance_type='gain')\n",
    "        plt.show()\n",
    "        used_time = (time.time() - start_time) / 3600\n",
    "        print(f'used_time: {used_time:.2f} hours')\n",
    "        y_pred = clf.predict(trainpd[feature].iloc[test_index])\n",
    "        y_predprob = clf.predict_proba(trainpd[feature].iloc[test_index])[:, 1]\n",
    "#         y_pp_xgb_stacking[test_index] = y_predprob\n",
    "\n",
    "        auc = metrics.roc_auc_score(labels[test_index], y_predprob)\n",
    "        print(\"AUC Score (Train): %f\" % auc)\n",
    "\n",
    "        fy_submission += clf.predict_proba(testpd[feature])[:, 1] / n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"lgbt\"!=\"lgbt\":\n",
    "    def run_lgb(df_train, labels, df_test, use_features):\n",
    "        target = 'flag'\n",
    "        oof_pred = np.zeros((len(df_train),))\n",
    "        y_pred = np.zeros((len(df_test),))\n",
    "        folds = GroupKFold(n_splits=2)  # 6 折比 5 折好一点, 当然有时间有机器可以试下更多的 folds\n",
    "        for fold, (tr_ind, val_ind) in enumerate(folds.split(df_train, labels, df_train['event_id'])):\n",
    "            start_time = time.time()\n",
    "            print(f'Fold {fold + 1}')\n",
    "            x_train, x_val = df_train[use_features].iloc[tr_ind], df_train[use_features].iloc[val_ind]\n",
    "            y_train, y_val = labels.iloc[tr_ind], labels.iloc[val_ind]\n",
    "            train_set = lgb.Dataset(x_train, y_train)\n",
    "            val_set = lgb.Dataset(x_val, y_val)\n",
    "#             https://www.cnblogs.com/infaraway/p/7890558.html\n",
    "            params = {\n",
    "                'learning_rate': 0.1,\n",
    "                'metric': 'auc',\n",
    "                'objective': 'binary',\n",
    "                'n_estimators': 5000,\n",
    "                'n_jobs': -1,\n",
    "                'seed': 1029,\n",
    "                'device':'gpu',\n",
    "#                 'is_unbalance': True,\n",
    "#                 过拟合\n",
    "                'max_depth': 8,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.75,\n",
    "                'bagging_freq': 2,\n",
    "#                 'min_data_in_leaf':1,\n",
    "#                 'min_sum_hessian_in_leaf':0.001,\n",
    "#                 'min_gain_to_split': 0.02,\n",
    "#                 'min_gain_to_split': 0.004,\n",
    "                'num_leaves': 64,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5, # 越小l2正则程度越高\n",
    "            }\n",
    "            model = lgb.train(params,\n",
    "                              train_set,\n",
    "                              num_boost_round=5000,\n",
    "                              early_stopping_rounds=200,\n",
    "                              valid_sets=[train_set, val_set],\n",
    "                              verbose_eval=50)\n",
    "            oof_pred[val_ind] = model.predict(x_val)\n",
    "            y_pred += model.predict(df_test[use_features]) / folds.n_splits\n",
    "\n",
    "            print(\"Features importance...\")\n",
    "            gain = model.feature_importance('gain')\n",
    "            print(gain)\n",
    "            feat_imp = pd.DataFrame({'feature': model.feature_name(),\n",
    "                                     'split': model.feature_importance('split'),\n",
    "                                     'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "            display(feat_imp)\n",
    "            used_time = (time.time() - start_time) / 3600\n",
    "            print(f'used_time: {used_time:.2f} hours')\n",
    "            del x_train, x_val, y_train, y_val, train_set, val_set\n",
    "            gc.collect()\n",
    "        return y_pred, oof_pred\n",
    "#     lentht = trainpd.shape[0]\n",
    "#     print(trainpd.shape)\n",
    "#     fy_submission, oof_pred = run_lgb(trainpd.iloc[0:lentht//100], labels.iloc[0:lentht//100], testpd, feature)\n",
    "    fy_submission, oof_pred = run_lgb(trainpd, labels, testpd, feature)\n",
    "    score = roc_auc_score(labels, oof_pred)\n",
    "    print('auc: ', score)\n",
    "    print(fy_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"stack\" != \"stack\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr'),\n",
    "        xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.1, verbosity=3,\n",
    "                                eval_metric='auc', n_estimators=1000),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=8, n_estimators=1000)\n",
    "    ]\n",
    "    '''切分一部分数据作为测试集'''\n",
    "    train1pd, train2pd ,label1s, label2s = train_test_split(trainpd[feature], labels, test_size=0.33, random_state=2017)\n",
    "    dataset_blend_1train = np.zeros((train1pd.shape[0], len(clfs)))\n",
    "    dataset_blend_2train = np.zeros((train2pd.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((testpd.shape[0], len(clfs)))\n",
    "    '''5折stacking'''\n",
    "    n_folds = 2\n",
    "    skf = list(StratifiedKFold(label1s, n_folds))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        '''依次训练各个单模型'''\n",
    "        print(j, clf)\n",
    "        start_time = time.time() \n",
    "        dataset_blend_2train_j = np.zeros((train2pd.shape[0], len(skf)))\n",
    "        dataset_blend_test_j = np.zeros((testpd.shape[0], len(skf)))\n",
    "        for i, (train_index, test_index) in enumerate(skf):\n",
    "            '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "            # print(\"Fold\", i)\n",
    "            X_d1, y_d1, X_d2, y_d2 = train1pd[train_index], label1s[train_index], train2pd[test_index], label1s[test_index]\n",
    "            clf.fit(X_d1, y_d1)\n",
    "            dataset_blend_1train[test_index, j] = clf.predict_proba(X_d2)[:, 1]\n",
    "            dataset_blend_2train_j[:, i] = clf.predict_proba(train2pd)[:, 1]\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "        '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "        dataset_blend_2train[:, j] = dataset_blend_2train_j.mean(1)\n",
    "        dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "        used_time = (time.time() - start_time) / 3600\n",
    "        print(f'used_time: {used_time:.2f} hours')\n",
    "        print(\"val auc Score: %f\" % roc_auc_score(label2s, dataset_blend_2train[:, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"stack\" != \"stack\":\n",
    "# clf = LogisticRegression()\n",
    "    clf2 = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "    clf2.fit(dataset_blend_1train, label1s)\n",
    "    y_submission = clf2.predict_proba(dataset_blend_2train)[:, 1]\n",
    "    fy_submission = clf2.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "    print(\"Linear stretch of predictions to [0,1]\")\n",
    "    y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "    print(\"blend result\")\n",
    "    print(\"val auc Score: %f\" % (roc_auc_score(y_predict, y_submission)))\n",
    "    fy_submission = (fy_submission - fy_submission.min()) / (fy_submission.max() - fy_submission.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"blend\" != \"blend\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators=5000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=5000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=5000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "#         svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr'),\n",
    "        xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.1, verbosity=3,\n",
    "                                eval_metric='auc', n_estimators=1000),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=8, n_estimators=1000)\n",
    "    ]\n",
    "    '''切分训练数据集为d1,d2两部分'''\n",
    "    X_d1, X_d2, y_d1, y_d2 = train_test_split(trainpd[feature], labels, test_size=0.33, random_state=2017)\n",
    "#     dataset_d1 = np.zeros((X_d1.shape[0], len(clfs)))\n",
    "    dataset_d2 = np.zeros((X_d2.shape[0], len(clfs)))\n",
    "    dataset_test = np.zeros((testpd.shape[0], len(clfs)))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        '''依次训练各个单模型'''\n",
    "        print(j, clf)\n",
    "        start_time = time.time()\n",
    "        clf.fit(X_d1, y_d1)\n",
    "        '''使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。'''\n",
    "        dataset_d2[:, j] = clf.predict_proba(X_d2)[:, 1]\n",
    "        '''对于测试集，直接用这k个模型的预测值作为新的特征。'''\n",
    "        dataset_test[:, j] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "#         dataset_test[:, j] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "        used_time = (time.time() - start_time) / 3600\n",
    "        print(f'used_time: {used_time:.2f} hours')\n",
    "        print(\"val auc Score: %f\" % roc_auc_score(y_d2, dataset_d2[:, j]))\n",
    "        used_time = (time.time() - start_time) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"blend\" != \"blend\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    '''融合使用的模型'''\n",
    "    # clf = LogisticRegression()\n",
    "    clf2 = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "    clf2.fit(dataset_d2, y_d2)\n",
    "#     y_submission = clf2.predict_proba(dataset_d2)[:, 1]\n",
    "    fy_submission = clf2.predict_proba(dataset_test)[:, 1]\n",
    "\n",
    "#     print(\"Linear stretch of predictions to [0,1]\")\n",
    "#     y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "#     print(\"blend result\")\n",
    "#     print(\"val auc Score: %f\" % (roc_auc_score(y_d2, y_submission)))\n",
    "    fy_submission = (fy_submission - fy_submission.min()) / (fy_submission.max() - fy_submission.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#阈值大概在0.2-0.4之间 本题对召回率较敏感，可适当降低一下阈值\n",
    "thre = 0.25\n",
    "\n",
    "#生成提交文件\n",
    "sub = pd.DataFrame()\n",
    "sub['hit_id'] = testpd['hit_id']\n",
    "sub['flag_pred'] = fy_submission\n",
    "sub['event_id'] = testpd['event_id']\n",
    "sub['flag_pred'] = sub['flag_pred'].apply(lambda x: 1 if x >= thre else 0)\n",
    "sub.to_csv(os.path.join(pathf, \"subsample.csv\").format(sub['flag_pred'].mean()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}