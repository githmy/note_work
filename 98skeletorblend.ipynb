{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from scipy.stats import entropy, kurtosis\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from IPython.display import display\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('float_format', lambda x: '%.6f' % x)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathf = os.path.join(\"..\", \"data\", \"particles\")\n",
    "\n",
    "trainpd = pd.read_csv(os.path.join(pathf, \"train.csv\"))\n",
    "print(trainpd.head(1))\n",
    "trainshape = trainpd.shape\n",
    "print(trainshape)\n",
    "eventpd = pd.read_csv(os.path.join(pathf, \"event.csv\"))\n",
    "print(eventpd.head(1))\n",
    "print(eventpd.shape)\n",
    "testpd = pd.read_csv(os.path.join(pathf, \"test.csv\"))\n",
    "testshape = testpd.shape\n",
    "print(testpd.head(1))\n",
    "print(testpd.shape)\n",
    "\n",
    "data = pd.concat([trainpd, testpd], ignore_index=True)\n",
    "data = pd.merge(data, eventpd, on='event_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (k(q,mc)*(t0+l))^2 + dis^2 -dis*cos(phi)*sin(thmc)*(t0+l) = (t+l)^2\n",
    "data['fx'] = data['x'] - data['xcmc']\n",
    "data['fy'] = data['y'] - data['ycmc']\n",
    "data['phimc'] = data['phimc'] * np.pi / 180.\n",
    "data['fphi'] = np.arctan2(data['fy'], data['fx']) - data['phimc']\n",
    "data['fdis'] = np.sqrt(data['fx'] ** 2 + data['fy'] ** 2)\n",
    "data['thetamc'] = data['thetamc'] * np.pi / 180.\n",
    "data['fsinthmc'] = np.sin(data['thetamc'])\n",
    "data['fcosphi'] = np.cos(data['fphi'])\n",
    "\n",
    "data['ft2'] = data['t'] ** 2\n",
    "data['fdis2'] = data['fdis'] ** 2\n",
    "data['fsencond'] = data['fdis'] * data['fcosphi'] * data['fsinthmc']\n",
    "\n",
    "data['fttrue'] = data['t'] / data['terror']\n",
    "data['nhitratio'] = data['nhit'] / data['nhitreal']\n",
    "\n",
    "data['fenergymc2'] = data['energymc'] ** 2\n",
    "\n",
    "del data['fx']\n",
    "del data['fy']\n",
    "del data['x']\n",
    "del data['y']\n",
    "del data['z']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_new = pd.DataFrame()\n",
    "info_new[\"event_id\"] = data.groupby([\"event_id\"])[\"event_id\"].mean()\n",
    "info_new[\"fdis_mean\"] = data.groupby([\"event_id\"])[\"fdis\"].mean()\n",
    "info_new[\"fdis_std\"] = data.groupby([\"event_id\"])[\"fdis\"].std()\n",
    "info_new[\"fdis_stdmean\"] = info_new[\"fdis_std\"] / info_new[\"fdis_mean\"]\n",
    "info_new[\"ft_mean\"] = data.groupby([\"event_id\"])[\"t\"].mean()\n",
    "info_new[\"ft_std\"] = data.groupby([\"event_id\"])[\"t\"].std()\n",
    "info_new[\"ft_stdmean\"] = info_new[\"ft_std\"] / info_new[\"ft_mean\"]\n",
    "info_new[\"ft_mean2\"] = info_new[\"ft_mean\"] ** 2\n",
    "info_new.reset_index(drop=True, inplace=True)\n",
    "data = pd.merge(data, info_new, on='event_id', how='left')\n",
    "\n",
    "data['fdis_rel'] = data['fdis'] / data['fdis_mean']\n",
    "data['fdis_rel_std'] = data['fdis_rel'] / data['fdis_std']\n",
    "data['ft_rel'] = data['t'] / data['ft_mean']\n",
    "data['ft_rel_std'] = data['ft_rel'] / data['ft_std']\n",
    "\n",
    "data['fdis2_rel'] = data['fdis_rel'] ** 2\n",
    "data['fdis2_rel_std'] = data['fdis_rel_std'] ** 2\n",
    "data['ft2_rel'] = data['ft_rel'] ** 2\n",
    "data['ft2_rel_std'] = data['ft_rel_std'] ** 2\n",
    "\n",
    "data['fsencond_rel'] = data['fdis2_rel'] * data['fcosphi'] * data['fsinthmc']\n",
    "data['fsencond_rel_std'] = data['fdis2_rel_std'] * data['fcosphi'] * data['fsinthmc']\n",
    "\n",
    "data['fsencond_ful'] = data['fsencond'] * data['ft_mean']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainshape[0])\n",
    "print(data.shape)\n",
    "\n",
    "trainpd = data[:trainshape[0]].reset_index()\n",
    "testpd = data[trainshape[0]:].reset_index()\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainpd.columns)\n",
    "feature = [x for x in trainpd.columns if x not in ['flag', 'index', 'hit_id', 'event_id']]\n",
    "labels = trainpd['flag']\n",
    "del trainpd['flag']\n",
    "del testpd['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"xgb\"!=\"xgb\":\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=4399)\n",
    "    y_pp_xgb = np.zeros(testshape[0])\n",
    "#     y_pp_xgb_stacking = np.zeros(len(labels))\n",
    "    for train_index, test_index in kf.split(trainpd):\n",
    "        print(\">>>\", train_index)\n",
    "        clf = xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.1, verbosity=3,\n",
    "                                eval_metric='auc', n_estimators=2000)\n",
    "#                                 eval_metric='auc', n_estimators=2000, predictor='cpu_predictor')\n",
    "        clf.fit(\n",
    "            trainpd[feature].iloc[train_index], labels[train_index],\n",
    "            eval_set=[(trainpd[feature].iloc[train_index], labels[train_index]),\n",
    "                      (trainpd[feature].iloc[test_index], labels[test_index])],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        y_pred = clf.predict(trainpd[feature].iloc[test_index])\n",
    "        y_predprob = clf.predict_proba(trainpd[feature].iloc[test_index])[:, 1]\n",
    "#         y_pp_xgb_stacking[test_index] = y_predprob\n",
    "\n",
    "        auc = metrics.roc_auc_score(labels[test_index], y_predprob)\n",
    "        print(\"AUC Score (Train): %f\" % auc)\n",
    "\n",
    "        y_pp_xgb += clf.predict_proba(testpd[feature])[:, 1] / n_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"lgbt\"==\"lgbt\":\n",
    "    def run_lgb(df_train, df_test, use_features):\n",
    "        target = 'flag'\n",
    "        oof_pred = np.zeros((len(df_train),))\n",
    "        y_pred = np.zeros((len(df_test),))\n",
    "        folds = GroupKFold(n_splits=2)  # 6 折比 5 折好一点, 当然有时间有机器可以试下更多的 folds\n",
    "        for fold, (tr_ind, val_ind) in enumerate(folds.split(train, train[target], train['event_id'])):\n",
    "            start_time = time.time()\n",
    "            print(f'Fold {fold + 1}')\n",
    "            x_train, x_val = df_train[use_features].iloc[tr_ind], df_train[use_features].iloc[val_ind]\n",
    "            y_train, y_val = df_train[target].iloc[tr_ind], df_train[target].iloc[val_ind]\n",
    "            train_set = lgb.Dataset(x_train, y_train)\n",
    "            val_set = lgb.Dataset(x_val, y_val)\n",
    "            params = {\n",
    "                'learning_rate': 0.2,\n",
    "                'metric': 'auc',\n",
    "                'objective': 'binary',\n",
    "                'feature_fraction': 0.80,\n",
    "                'bagging_fraction': 0.75,\n",
    "                'bagging_freq': 2,\n",
    "                'n_jobs': -1,\n",
    "                'seed': 1029,\n",
    "                'max_depth': 8,\n",
    "                'num_leaves': 64,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5\n",
    "            }\n",
    "            model = lgb.train(params,\n",
    "                              train_set,\n",
    "                              num_boost_round=5000,\n",
    "                              early_stopping_rounds=100,\n",
    "                              valid_sets=[train_set, val_set],\n",
    "                              verbose_eval=100)\n",
    "            oof_pred[val_ind] = model.predict(x_val)\n",
    "            y_pred += model.predict(df_test[use_features]) / folds.n_splits\n",
    "\n",
    "            print(\"Features importance...\")\n",
    "            gain = model.feature_importance('gain')\n",
    "            feat_imp = pd.DataFrame({'feature': model.feature_name(),\n",
    "                                     'split': model.feature_importance('split'),\n",
    "                                     'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "            display(feat_imp)\n",
    "            used_time = (time.time() - start_time) / 3600\n",
    "            print(f'used_time: {used_time:.2f} hours')\n",
    "            del x_train, x_val, y_train, y_val, train_set, val_set\n",
    "            gc.collect()\n",
    "        return y_pred, oof_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, oof_pred = run_lgb(train, test, use_features)\n",
    "score = roc_auc_score(train['flag'], oof_pred)\n",
    "print('auc: ', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"stack\" != \"stack\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr'),\n",
    "        xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.1, verbosity=3,\n",
    "                                eval_metric='auc', n_estimators=1000),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=8, n_estimators=1000)\n",
    "    ]\n",
    "    '''切分一部分数据作为测试集'''\n",
    "    train1pd, train2pd ,label1s, label2s = train_test_split(trainpd[feature], labels, test_size=0.33, random_state=2017)\n",
    "    dataset_blend_1train = np.zeros((train1pd.shape[0], len(clfs)))\n",
    "    dataset_blend_2train = np.zeros((train2pd.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((testpd.shape[0], len(clfs)))\n",
    "    '''5折stacking'''\n",
    "    n_folds = 2\n",
    "    skf = list(StratifiedKFold(label1s, n_folds))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        '''依次训练各个单模型'''\n",
    "        # print(j, clf)\n",
    "        dataset_blend_2train_j = np.zeros((train2pd.shape[0], len(skf)))\n",
    "        dataset_blend_test_j = np.zeros((testpd.shape[0], len(skf)))\n",
    "        for i, (train_index, test_index) in enumerate(skf):\n",
    "            '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "            # print(\"Fold\", i)\n",
    "            X_d1, y_d1, X_d2, y_d2 = train1pd[train_index], label1s[train_index], train2pd[test_index], label1s[test_index]\n",
    "            clf.fit(X_d1, y_d1)\n",
    "            dataset_blend_1train[test_index, j] = clf.predict_proba(X_d2)[:, 1]\n",
    "            dataset_blend_2train_j[:, i] = clf.predict_proba(train2pd)[:, 1]\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "        '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "        dataset_blend_2train[:, j] = dataset_blend_2train_j.mean(1)\n",
    "        dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "        print(\"val auc Score: %f\" % roc_auc_score(label2s, dataset_blend_2train[:, j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"stack\" != \"stack\":\n",
    "# clf = LogisticRegression()\n",
    "    clf2 = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "    clf2.fit(dataset_blend_1train, label1s)\n",
    "    y_submission = clf2.predict_proba(dataset_blend_2train)[:, 1]\n",
    "    fy_submission = clf2.predict_proba(dataset_blend_test)[:, 1]\n",
    "\n",
    "    print(\"Linear stretch of predictions to [0,1]\")\n",
    "    y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "    print(\"blend result\")\n",
    "    print(\"val auc Score: %f\" % (roc_auc_score(y_predict, y_submission)))\n",
    "    fy_submission = (fy_submission - fy_submission.min()) / (fy_submission.max() - fy_submission.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"blend\" != \"blend\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=1000, max_depth=8, n_jobs=-1, criterion='entropy'),\n",
    "        svm.SVC(C=0.1, kernel='linear', decision_function_shape='ovr'),\n",
    "        xgb.XGBClassifier(tree_method='gpu_hist', max_depth=8, learning_rate=0.1, verbosity=3,\n",
    "                                eval_metric='auc', n_estimators=1000),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=8, n_estimators=1000)\n",
    "    ]\n",
    "    '''切分训练数据集为d1,d2两部分'''\n",
    "    X_d1, X_d2, y_d1, y_d2 = train_test_split(trainpd[feature], labels, test_size=0.33, random_state=2017)\n",
    "    dataset_d1 = np.zeros((train1pd.shape[0], len(clfs)))\n",
    "    dataset_d2 = np.zeros((train2pd.shape[0], len(clfs)))\n",
    "#     dataset_test = np.zeros((testpd.shape[0], len(clfs)))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        '''依次训练各个单模型'''\n",
    "        print(j, clf)\n",
    "        clf.fit(X_d1, y_d1)\n",
    "        '''使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。'''\n",
    "        dataset_d1[:, j] = clf.predict_proba(X_d2)[:, 1]\n",
    "        '''对于测试集，直接用这k个模型的预测值作为新的特征。'''\n",
    "        dataset_d2[:, j] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "#         dataset_test[:, j] = clf.predict_proba(testpd[feature])[:, 1]\n",
    "        print(\"val auc Score: %f\" % roc_auc_score(y_d2, dataset_d1[:, j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"blend\" != \"blend\":\n",
    "    '''模型融合中使用到的各个单模型'''\n",
    "    '''融合使用的模型'''\n",
    "    # clf = LogisticRegression()\n",
    "    clf2 = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "    clf2.fit(dataset_d1, y_d2)\n",
    "#     y_submission = clf2.predict_proba(dataset_d2)[:, 1]\n",
    "    fy_submission = clf2.predict_proba(dataset_d2)[:, 1]\n",
    "\n",
    "#     print(\"Linear stretch of predictions to [0,1]\")\n",
    "#     y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "#     print(\"blend result\")\n",
    "#     print(\"val auc Score: %f\" % (roc_auc_score(y_d2, y_submission)))\n",
    "    fy_submission = (fy_submission - fy_submission.min()) / (fy_submission.max() - fy_submission.min())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-572fd32a2eee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#生成提交文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hit_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestpd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hit_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flag_pred'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pp_xgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#阈值大概在0.2-0.4之间 本题对召回率较敏感，可适当降低一下阈值\n",
    "thre = 0.25\n",
    "\n",
    "#生成提交文件\n",
    "sub = pd.DataFrame()\n",
    "sub['hit_id'] = testpd['hit_id']\n",
    "sub['flag_pred'] = fy_submission\n",
    "sub['event_id'] = testpd['event_id']\n",
    "sub['flag_pred'] = sub['flag_pred'].apply(lambda x: 1 if x >= thre else 0)\n",
    "sub.to_csv(os.path.join(pathf, \"subsample.csv\").format(sub['flag_pred'].mean()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
