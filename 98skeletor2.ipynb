{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from scipy.stats import entropy, kurtosis\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from xgboost import plot_importance\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import *\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x      y  z        t   terror        q  flag  event_id  hit_id\n",
      "0 -142.5 -147.5  0  767.879  2.02966  1.05052     0         7       1\n",
      "(9473201, 9)\n",
      "   event_id  nhit  nhitreal  energymc  thetamc    phimc   xcmc    ycmc\n",
      "0         7   426        70   48348.9  63.1686  11.0982 -40.83  114.03\n",
      "(13315, 8)\n",
      "       x      y  z        t  terror        q  event_id  hit_id\n",
      "0 -142.5 -127.5  0  848.061  1.9984  1.15067         9       1\n",
      "(4086511, 8)\n"
     ]
    }
   ],
   "source": [
    "pathf = os.path.join(\"..\", \"data\", \"particles\")\n",
    "model_path  = os.path.join(pathf, \"model\")\n",
    "log_path  = os.path.join(pathf, \"model\")\n",
    "trainpd = pd.read_csv(os.path.join(pathf, \"train.csv\"))\n",
    "print(trainpd.head(1))\n",
    "trainshape = trainpd.shape\n",
    "print(trainshape)\n",
    "eventpd = pd.read_csv(os.path.join(pathf, \"event.csv\"))\n",
    "print(eventpd.head(1))\n",
    "print(eventpd.shape)\n",
    "testpd = pd.read_csv(os.path.join(pathf, \"test.csv\"))\n",
    "testshape = testpd.shape\n",
    "print(testpd.head(1))\n",
    "print(testpd.shape)\n",
    "\n",
    "data = pd.concat([trainpd, testpd], ignore_index=True)\n",
    "data = pd.merge(data, eventpd, on='event_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#组合特征 \n",
    "#train表的特征与event表的特征交互\n",
    "data['fx'] = data['x']-data['xcmc']\n",
    "data['fy'] = data['y']-data['ycmc']\n",
    "data['fdis'] = np.sqrt(data['fx']**2+data['fy']**2)\n",
    "data['fscala'] = np.sin(data['thetamc']) * data['t']\n",
    "data['fphi'] = np.arctan2(data['fy'], data['fx']) * 180\n",
    "data['fttrue'] = data['t']/data['terror']\n",
    "data['nhitratio'] = data['nhit']/data['nhitreal']\n",
    "\n",
    "del data['fx']\n",
    "del data['fy']\n",
    "del data['x']\n",
    "del data['y']\n",
    "del data['z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_new = pd.DataFrame()\n",
    "info_new[\"event_id\"] = data.groupby([\"event_id\"])[\"event_id\"].mean()\n",
    "info_new[\"fdis_stdmean\"] = data.groupby([\"event_id\"])[\"fdis\"].std()/data.groupby([\"event_id\"])[\"fdis\"].mean()\n",
    "info_new.reset_index(drop=True, inplace=True)\n",
    "data = pd.merge(data, info_new, on='event_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9473201\n",
      "<class 'int'>\n",
      "(13559712, 19)\n"
     ]
    }
   ],
   "source": [
    "trainpd = data[:trainshape[0]].reset_index()\n",
    "testpd = data[trainshape[0]:].reset_index()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'event_id', 'flag', 'hit_id', 'q', 't', 'terror', 'nhit',\n",
      "       'nhitreal', 'energymc', 'thetamc', 'phimc', 'xcmc', 'ycmc', 'fdis',\n",
      "       'fscala', 'fphi', 'fttrue', 'nhitratio', 'fdis_stdmean'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(trainpd.columns)\n",
    "feature= [x for x in trainpd.columns if x not in ['flag','index','hit_id','event_id']]\n",
    "labels = trainpd['flag']\n",
    "del trainpd['flag']\n",
    "del testpd['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow 考虑单event聚类\n",
    "def batch_iter_list(data_list, batch_size, num_epochs, shuffle=True):\n",
    "    data_size = len(data_list[0])\n",
    "    num_batches_per_epoch = data_size // batch_size  # 每个epoch中包含的batch数量\n",
    "    for epoch in range(num_epochs):\n",
    "        # 每个epoch是否进行shuflle\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data_list = [data[shuffle_indices] for data in data_list]\n",
    "        else:\n",
    "            shuffled_data_list = data_list\n",
    "\n",
    "        for batch_num in range(num_batches_per_epoch + 1):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield [shuffled_data[start_index:end_index] for shuffled_data in shuffled_data_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractModeltensor(object):\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config\n",
    "\n",
    "    # You need to override this method.\n",
    "    def buildModel(self):\n",
    "        raise NotImplementedError(\"You need to implement your own model.\")\n",
    "\n",
    "class NeurousNet(AbstractModeltensor):\n",
    "    def __init__(self, xlenth, config=None):\n",
    "        super(NeurousNet, self).__init__(config)\n",
    "        self.graph = tf.Graph()  # 为每个类(实例)单独创建一个graph\n",
    "        self.modeldic = {\n",
    "            \"cnn_dense_less\": self._cnn_dense_less_model,\n",
    "            \"nomul_model\": self._nomul_model,\n",
    "        }\n",
    "        self.ydim = 1\n",
    "        self.keep_prob_ph = config[\"dropout\"]\n",
    "        self.input_dim = xlenth\n",
    "        self.out_dim = 1\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('Inputs'):\n",
    "                self.input_p = tf.placeholder(tf.float32, [None, self.input_dim])\n",
    "                self.learn_rate_p = tf.placeholder(dtype=tf.float32, shape=[], name=\"lr\")\n",
    "                self.lr_decay = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "            with tf.name_scope('Outputs'):\n",
    "                self.target_y = tf.placeholder(dtype=tf.float32, shape=[None, self.out_dim])\n",
    "\n",
    "    def buildModel(self):\n",
    "        tf.reset_default_graph()\n",
    "        with self.graph.as_default():\n",
    "            # 不同选择加载\n",
    "            self.modeldic[self.config[\"modelname\"]]()\n",
    "            # 打印打包\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            # 损失目标\n",
    "            tvars = tf.trainable_variables()  # 返回需要训练的variable\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.train_list, tvars), 2)\n",
    "            grads_and_vars = tuple(zip(grads, tvars))\n",
    "            self.train_op = tf.train.AdamOptimizer(self.learn_rate_p).apply_gradients(grads_and_vars)\n",
    "#             self.train_op = []\n",
    "#             for i2 in self.train_list:\n",
    "#                 self.train_op.append(tf.train.AdamOptimizer(self.learn_rate_p).minimize(i2))\n",
    "            # 同一保存加载\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            # [print(n.name) for n in tf.get_default_graph().as_graph_def().node]\n",
    "            # return self.saver\n",
    "\n",
    "    def _cnn_dense_less_model(self):\n",
    "        with self.graph.as_default():\n",
    "            # 部分1，预测值\n",
    "            dense1 = tf.layers.dense(inputs=self.input_p, units=self.input_dim, activation=tf.nn.softmax, name=\"layer_dense1\")\n",
    "            tf.summary.histogram('dense1', dense1)  # 记录标量的变化\n",
    "            mult_layer1 = tf.nn.softmax(dense1*self.input_p, name='mult_layer1')\n",
    "            mult_layer2 = tf.nn.softmax(mult_layer1*self.input_p, name='mult_layer2')\n",
    "            concat1 = tf.concat([self.input_p, dense1,mult_layer1,mult_layer2], 1, name='concat1')\n",
    "            tf.summary.histogram('concat1', concat1)  # 记录标量的变化\n",
    "            denseo1 = tf.nn.dropout(concat1, keep_prob=self.keep_prob_ph)\n",
    "            denseo2 = tf.layers.dense(inputs=denseo1, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense2\")\n",
    "            denseo3 = tf.layers.dense(inputs=denseo2, units=self.input_dim//4, activation=tf.nn.elu, name=\"layer_dense3\")\n",
    "            y_res_t = tf.layers.dense(inputs=denseo3, units=self.out_dim, activation=None)\n",
    "            y_res_v = tf.nn.sigmoid(y_res_t, name=\"y_res_v\")\n",
    "            tf.summary.histogram('y_res_v', y_res_v)  # 记录标量的变化\n",
    "            # 损失返回值\n",
    "            y_los = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_res_t, labels=self.target_y, name=\"y_los\")\n",
    "            y_loss_t = tf.reduce_mean(y_los, name=\"y_loss_t\")\n",
    "            y_loss_v = tf.add(y_loss_t,0, name=\"y_loss_v\")\n",
    "            \n",
    "            one = tf.ones_like(y_res_t)\n",
    "            zero = tf.zeros_like(y_res_t)\n",
    "            label_bool = tf.where(y_res_t < 0.5, x=zero, y=one)\n",
    "            self.auc_value, self.auc_op = tf.metrics.auc(self.target_y, label_bool, num_thresholds=4000)\n",
    "            # 猜错的获取 实际盈利值的负数\n",
    "            self.train_list = [y_loss_t]\n",
    "            self.valid_list = [y_loss_v]\n",
    "            self.pred_list = [y_res_v]\n",
    "            # 打印信息\n",
    "            tf.summary.scalar('y_loss_t', y_loss_t)  # 记录标量的变化\n",
    "            tf.summary.scalar('y_loss_v', y_loss_v)  # 记录标量的变化\n",
    "            tf.summary.histogram('mult_layer1', mult_layer1)  # 记录标量的变化\n",
    "            tf.summary.histogram('mult_layer2', mult_layer2)  # 记录标量的变化\n",
    "            \n",
    "            tf.summary.scalar('lr', self.learn_rate_p)  # 记录标量的变化\n",
    "            return None\n",
    "\n",
    "    def _nomul_model(self):\n",
    "        with self.graph.as_default():\n",
    "            # 部分1，预测值\n",
    "            dense1 = tf.layers.dense(inputs=self.input_p, units=self.input_dim, activation=tf.nn.softmax, name=\"layer_dense1\")\n",
    "            tf.summary.histogram('dense1', dense1)  # 记录标量的变化\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense2\")\n",
    "            dense3 = tf.layers.dense(inputs=dense2, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense3\")\n",
    "            dense4 = tf.layers.dense(inputs=dense3, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense4\")\n",
    "            dense5 = tf.layers.dense(inputs=dense4, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense5\")\n",
    "            dense6 = tf.layers.dense(inputs=dense5, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense6\")\n",
    "            dense7 = tf.layers.dense(inputs=dense6, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense7\")\n",
    "            dense8 = tf.layers.dense(inputs=dense7, units=self.input_dim, activation=tf.nn.elu, name=\"layer_dense8\")\n",
    "            concat1 = tf.concat([self.input_p, dense1,dense2,dense3,dense4,dense5,dense6,dense7,dense8], 1, name='concat1')\n",
    "            tf.summary.histogram('concat1', concat1)  # 记录标量的变化\n",
    "            denseo1 = tf.nn.dropout(concat1, keep_prob=self.keep_prob_ph)\n",
    "            denseo2 = tf.layers.dense(inputs=denseo1, units=self.input_dim*4, activation=tf.nn.elu, name=\"layer_denseo2\")\n",
    "            denseo3 = tf.layers.dense(inputs=denseo2, units=self.input_dim*4, activation=tf.nn.elu, name=\"layer_denseo3\")\n",
    "            denseo4 = tf.layers.dense(inputs=denseo3, units=self.input_dim//4, activation=tf.nn.elu, name=\"layer_denseo4\")\n",
    "            y_res_t = tf.layers.dense(inputs=denseo4, units=self.out_dim, activation=None)\n",
    "            y_res_v = tf.nn.sigmoid(y_res_t, name=\"y_res_v\")\n",
    "            tf.summary.histogram('y_res_v', y_res_v)  # 记录标量的变化\n",
    "            # 损失返回值\n",
    "            y_los = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_res_t, labels=self.target_y, name=\"y_los\")\n",
    "            y_loss_t = tf.reduce_mean(y_los, name=\"y_loss_t\")\n",
    "            y_loss_v = tf.add(y_loss_t,0, name=\"y_loss_v\")\n",
    "            \n",
    "            one = tf.ones_like(y_res_t)\n",
    "            zero = tf.zeros_like(y_res_t)\n",
    "            label_bool = tf.where(y_res_t < 0.5, x=zero, y=one)\n",
    "            self.auc_value, self.auc_op = tf.metrics.auc(self.target_y, label_bool, num_thresholds=4000)\n",
    "            # 猜错的获取 实际盈利值的负数\n",
    "            self.train_list = [y_loss_t]\n",
    "            self.valid_list = [y_loss_v]\n",
    "            self.pred_list = [y_res_v]\n",
    "            # 打印信息\n",
    "            tf.summary.scalar('y_loss_t', y_loss_t)  # 记录标量的变化\n",
    "            tf.summary.scalar('y_loss_v', y_loss_v)  # 记录标量的变化\n",
    "            \n",
    "            tf.summary.scalar('lr', self.learn_rate_p)  # 记录标量的变化\n",
    "            return None\n",
    "\n",
    "    def batch_train(self, trainpd, labels, batch_size=8, num_epochs=1, retrain=True):\n",
    "        # 设置\n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        with sess.as_default():\n",
    "            with self.graph.as_default():\n",
    "                if self.config[\"retrain\"] == 1:\n",
    "                    model_dir = os.path.join(model_path, \"modelevery_%s\" % self.config[\"tailname\"])\n",
    "                    latest_ckpt = tf.train.latest_checkpoint(model_dir)\n",
    "                    if os.path.isfile(\"{}.index\".format(latest_ckpt)):\n",
    "                        self.saver.restore(sess, latest_ckpt)\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        print(\"retraining {}\".format(latest_ckpt))\n",
    "                    else:\n",
    "                        sess.run(tf.global_variables_initializer())\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        print(\"no old model, training new----\")\n",
    "                writer = tf.summary.FileWriter(os.path.join(log_path, \"logsevery_%s\" % self.config[\"tailname\"]),\n",
    "                                               sess.graph)\n",
    "                global_n = 0\n",
    "                stop_n = 0\n",
    "                startt = time.time()\n",
    "                pre_t_base_loss = pre_t_much_loss = pre_v_much_loss = pre_v_base_loss = 100000\n",
    "                \n",
    "                n_splits = 5\n",
    "                kf = KFold(n_splits=n_splits, shuffle=True, random_state=4389)\n",
    "                for epoch in range(num_epochs):\n",
    "                    trainevenidlist = list(set(trainpd['event_id']))\n",
    "                    for train_index, valid_index in kf.split(trainevenidlist):\n",
    "                        starte = time.time()\n",
    "                        print(\"iter_trainnum\", len(train_index))\n",
    "                        np.random.shuffle(train_index)\n",
    "                        np.random.shuffle(valid_index)\n",
    "                        for batch_num, eventindex in enumerate(train_index):\n",
    "                            # 获取数据\n",
    "                            thisindex = trainpd[trainpd['event_id'] == trainevenidlist[eventindex]].index\n",
    "                            r_inputs_t = np.array(trainpd.iloc[thisindex][feature])\n",
    "                            r_output_t = np.expand_dims(np.array(labels[thisindex]),-1)\n",
    "                            feed_dict_t = {\n",
    "                                self.input_p: r_inputs_t,\n",
    "                                self.target_y: r_output_t,\n",
    "                                self.learn_rate_p: self.config[\"learn_rate\"],\n",
    "                                self.lr_decay: 1,\n",
    "                            }\n",
    "                            # 更新学习率\n",
    "                            sess.run(self.train_op, feed_dict_t)\n",
    "                            global_n += 1\n",
    "                            losslist_t = sess.run(self.train_list, feed_dict_t)\n",
    "                            sess.run(self.auc_op, feed_dict=feed_dict_t)\n",
    "                            accu = sess.run(self.auc_value)\n",
    "                            result = sess.run(self.merged, feed_dict_t)\n",
    "                            if batch_num % 200 == 0:\n",
    "                                writer.add_summary(result, global_n)\n",
    "                                self.saver.save(sess, os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                                   self.config[\"modelfile\"]), global_step=global_n)\n",
    "                                print(\"epocht {}, batch_num {}, step {}, time: {} s, accu: {}, loss_yt: {}\".format(\n",
    "                                        epoch, batch_num, global_n, time.time() - starte, accu, *losslist_t))\n",
    "                        # valid part\n",
    "                        print(\"iter_validnum\", len(valid_index))\n",
    "                        losslist_va = 0\n",
    "                        accu_va = 0\n",
    "                        for batch_num, eventindex in enumerate(valid_index):\n",
    "                            # 获取数据\n",
    "                            thisindex = trainpd[trainpd['event_id'] == trainevenidlist[eventindex]].index\n",
    "                            r_inputs_v = np.array(trainpd.iloc[thisindex][feature])\n",
    "                            r_output_v = np.expand_dims(np.array(labels[thisindex]),-1)\n",
    "                            feed_dict_v = {\n",
    "                                self.input_p: r_inputs_v,\n",
    "                                self.target_y: r_output_v,\n",
    "                                self.learn_rate_p: self.config[\"learn_rate\"],\n",
    "                                self.lr_decay: 1,\n",
    "                            }\n",
    "                            losslist_v = sess.run(self.valid_list, feed_dict_v)\n",
    "                            sess.run(self.auc_op, feed_dict=feed_dict_v)\n",
    "                            accu = sess.run(self.auc_value)\n",
    "                            losslist_va += losslist_v[0]\n",
    "                            accu_va += accu\n",
    "                        losslist_va /= len(valid_index)\n",
    "                        accu_va /= len(valid_index)\n",
    "                        result = sess.run(self.merged, feed_dict_v)\n",
    "                        writer.add_summary(result, global_n)\n",
    "                        if losslist_t[0] < pre_t_base_loss and losslist_va < pre_v_base_loss:\n",
    "                            stop_n += 1\n",
    "                            if stop_n > self.config[\"early_stop\"]:\n",
    "                                break\n",
    "                            else:\n",
    "                                self.saver.save(sess, os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                            self.config[\"modelfile\"]), global_step=global_n)\n",
    "                        else:\n",
    "                            stop_n = 0\n",
    "                            self.saver.save(sess, os.path.join(model_path, 'modelevery_%s' % self.config[\"tailname\"],\n",
    "                                                            self.config[\"modelfile\"]), global_step=global_n)\n",
    "                        print(\"epochv {}, step {}, stop_n {}, time: {} s, accu_va: {}, loss_yv: {}\".format(\n",
    "                            epoch, global_n, stop_n, time.time() - starte,accu_va, losslist_va))\n",
    "                        pre_t_base_loss = losslist_t[0]\n",
    "                        pre_v_base_loss = losslist_va\n",
    "                writer.close()\n",
    "                print(\"total time: %s s\" % (time.time() - startt))\n",
    "        # 结束\n",
    "        print(\"train finished!\")\n",
    "        return None\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        model_dir = os.path.join(model_path, \"modelevery_%s\" % self.config[\"tailname\"])\n",
    "        print(\"loading model...\")\n",
    "        latest_ckpt = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        with sess.as_default():\n",
    "            with self.graph.as_default():\n",
    "                if os.path.isfile(\"{}.index\".format(latest_ckpt)):\n",
    "                    self.saver.restore(sess, latest_ckpt)\n",
    "                else:\n",
    "                    raise Exception(\"没有找到模型:{}\".format(latest_ckpt))\n",
    "                nplist = []\n",
    "                oneiter = 2000\n",
    "                redi = inputs.shape[0]%oneiter\n",
    "                lenth = inputs.shape[0]//oneiter\n",
    "                if 0!=redi:\n",
    "                    lenth += 1\n",
    "                counter = 0\n",
    "                for num in range(lenth):\n",
    "                    # 获取数据\n",
    "                    startindex = num*oneiter\n",
    "                    if num==lenth-1 and redi!=0:\n",
    "                        endindex = num*oneiter+redi\n",
    "                    else:\n",
    "                        endindex = (num+1)*oneiter\n",
    "                    tmppd = inputs.iloc[startindex:endindex][feature]\n",
    "                    r_inputs_v = np.array(tmppd)\n",
    "                    feed_dict = {\n",
    "                        self.input_p: r_inputs_v,\n",
    "                    }\n",
    "                    teslis= sess.run(self.pred_list, feed_dict)\n",
    "                    nplist.append(teslis)\n",
    "                feed_dict = {\n",
    "                    self.input_p: inputs,\n",
    "                }\n",
    "                teslist = np.concatenate(nplist,axis=1)\n",
    "                return teslist\n",
    "\n",
    "trainconfig ={\n",
    "    \"dropout\":0.5,\n",
    "    \"early_stop\":100,\n",
    "    \"tailname\":\"nomul_model\",\n",
    "    \"modelname\":\"nomul_model\",\n",
    "#     \"tailname\":\"test1\",\n",
    "#     \"modelname\":\"cnn_dense_less\",\n",
    "    \"modelfile\":\"v1\",\n",
    "    \"learn_rate\":0.00001,\n",
    "    \"retrain\":1\n",
    "             }\n",
    "modelcrnn = NeurousNet(len(feature), config=trainconfig)\n",
    "modelcrnn.buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  event_id  hit_id          q         t   terror  nhit  nhitreal  \\\n",
      "0      0         7       1   1.050520  767.8790  2.02966   426        70   \n",
      "1      1         7       2   0.999853  -70.5552  2.02966   426        70   \n",
      "2      2         7       3   2.052540 -837.8410  1.85146   426        70   \n",
      "3      3         7       4  19.513100 -973.1950  1.39994   426        70   \n",
      "4      4         7       5   0.800334 -159.1400  2.02966   426        70   \n",
      "\n",
      "   energymc  thetamc    phimc   xcmc    ycmc        fdis      fscala  \\\n",
      "0   48348.9  63.1686  11.0982 -40.83  114.03  280.597095  253.721415   \n",
      "1   48348.9  63.1686  11.0982 -40.83  114.03  283.519540  -23.312742   \n",
      "2   48348.9  63.1686  11.0982 -40.83  114.03  264.805834 -276.838153   \n",
      "3   48348.9  63.1686  11.0982 -40.83  114.03  252.869393 -321.561617   \n",
      "4   48348.9  63.1686  11.0982 -40.83  114.03  250.900837  -52.582798   \n",
      "\n",
      "         fphi      fttrue  nhitratio  fdis_stdmean  \n",
      "0 -349.482181  378.328883   6.085714      0.542283  \n",
      "1 -345.372956  -34.762078   6.085714      0.542283  \n",
      "2 -350.008831 -452.529895   6.085714      0.542283  \n",
      "3 -357.222182 -695.169079   6.085714      0.542283  \n",
      "4 -353.937634  -78.407221   6.085714      0.542283  \n",
      "INFO:tensorflow:Restoring parameters from ..\\data\\particles\\model\\modelevery_nomul_model\\v1-1\n",
      "retraining ..\\data\\particles\\model\\modelevery_nomul_model\\v1-1\n",
      "iter_trainnum 7440\n",
      "epocht 0, batch_num 0, step 1, time: 0.9444751739501953 s, accu: 0.8951493501663208, loss_yt: 0.2560234069824219\n",
      "epocht 0, batch_num 200, step 201, time: 10.47698426246643 s, accu: 0.9004042148590088, loss_yt: 0.10310710966587067\n",
      "epocht 0, batch_num 400, step 401, time: 19.34028458595276 s, accu: 0.9059039354324341, loss_yt: 0.15188056230545044\n",
      "epocht 0, batch_num 600, step 601, time: 28.458898782730103 s, accu: 0.9083097577095032, loss_yt: 0.18578441441059113\n",
      "epocht 0, batch_num 800, step 801, time: 37.81488394737244 s, accu: 0.909144401550293, loss_yt: 0.17814119160175323\n",
      "epocht 0, batch_num 1000, step 1001, time: 46.93150544166565 s, accu: 0.9075777530670166, loss_yt: 0.1734616905450821\n",
      "epocht 0, batch_num 1200, step 1201, time: 55.75094819068909 s, accu: 0.9079588651657104, loss_yt: 0.17859186232089996\n",
      "epocht 0, batch_num 1400, step 1401, time: 64.86258864402771 s, accu: 0.9068582653999329, loss_yt: 0.2571984529495239\n",
      "epocht 0, batch_num 1600, step 1601, time: 74.02308559417725 s, accu: 0.9075387120246887, loss_yt: 0.2043299376964569\n",
      "epocht 0, batch_num 1800, step 1801, time: 83.13768815994263 s, accu: 0.9075245261192322, loss_yt: 0.2144034057855606\n",
      "epocht 0, batch_num 2000, step 2001, time: 92.19646549224854 s, accu: 0.908161461353302, loss_yt: 0.10482049733400345\n",
      "epocht 0, batch_num 2200, step 2201, time: 101.709059715271 s, accu: 0.907875120639801, loss_yt: 0.24236449599266052\n",
      "epocht 0, batch_num 2400, step 2401, time: 110.93136763572693 s, accu: 0.9078193306922913, loss_yt: 0.2090495228767395\n",
      "epocht 0, batch_num 2600, step 2601, time: 120.42099285125732 s, accu: 0.907462477684021, loss_yt: 0.40699538588523865\n",
      "epocht 0, batch_num 2800, step 2801, time: 129.95253705978394 s, accu: 0.9076605439186096, loss_yt: 0.2149031013250351\n",
      "epocht 0, batch_num 3000, step 3001, time: 139.27359771728516 s, accu: 0.9073561429977417, loss_yt: 0.26672133803367615\n",
      "epocht 0, batch_num 3200, step 3201, time: 148.57370972633362 s, accu: 0.9075927734375, loss_yt: 0.22081314027309418\n",
      "epocht 0, batch_num 3400, step 3401, time: 158.02247714996338 s, accu: 0.907458484172821, loss_yt: 0.16799330711364746\n",
      "epocht 0, batch_num 3600, step 3601, time: 167.48115468025208 s, accu: 0.9078643321990967, loss_yt: 0.11249079555273056\n",
      "epocht 0, batch_num 3800, step 3801, time: 176.82217454910278 s, accu: 0.9082589745521545, loss_yt: 0.1514168381690979\n",
      "epocht 0, batch_num 4000, step 4001, time: 186.24697089195251 s, accu: 0.9085561037063599, loss_yt: 0.32198089361190796\n",
      "epocht 0, batch_num 4200, step 4201, time: 195.6169159412384 s, accu: 0.9083172082901001, loss_yt: 0.1297939419746399\n",
      "epocht 0, batch_num 4400, step 4401, time: 205.27212762832642 s, accu: 0.9083707928657532, loss_yt: 0.24942906200885773\n",
      "epocht 0, batch_num 4600, step 4601, time: 213.88506722450256 s, accu: 0.9083763957023621, loss_yt: 0.27781450748443604\n",
      "epocht 0, batch_num 4800, step 4801, time: 222.75934195518494 s, accu: 0.9079292416572571, loss_yt: 0.1313197910785675\n",
      "epocht 0, batch_num 5000, step 5001, time: 231.4141960144043 s, accu: 0.9080982208251953, loss_yt: 0.26923882961273193\n",
      "epocht 0, batch_num 5200, step 5201, time: 240.0451145172119 s, accu: 0.9082391262054443, loss_yt: 0.1735856980085373\n",
      "epocht 0, batch_num 5400, step 5401, time: 248.71792936325073 s, accu: 0.9084118008613586, loss_yt: 0.14984896779060364\n",
      "epocht 0, batch_num 5600, step 5601, time: 257.41267681121826 s, accu: 0.9081670641899109, loss_yt: 0.26578009128570557\n",
      "epocht 0, batch_num 5800, step 5801, time: 266.13434982299805 s, accu: 0.9082464575767517, loss_yt: 0.28791818022727966\n",
      "epocht 0, batch_num 6000, step 6001, time: 275.0455205440521 s, accu: 0.9083042740821838, loss_yt: 0.2151268571615219\n",
      "epocht 0, batch_num 6200, step 6201, time: 283.82406091690063 s, accu: 0.9083120226860046, loss_yt: 0.07833010703325272\n",
      "epocht 0, batch_num 6400, step 6401, time: 292.55972146987915 s, accu: 0.9082145094871521, loss_yt: 0.21411274373531342\n",
      "epocht 0, batch_num 6600, step 6601, time: 301.4539325237274 s, accu: 0.9081487059593201, loss_yt: 0.6787393093109131\n",
      "epocht 0, batch_num 6800, step 6801, time: 310.14765787124634 s, accu: 0.9081383943557739, loss_yt: 0.2271317094564438\n",
      "epocht 0, batch_num 7000, step 7001, time: 318.74267506599426 s, accu: 0.9081850647926331, loss_yt: 0.27996236085891724\n",
      "epocht 0, batch_num 7200, step 7201, time: 327.53918504714966 s, accu: 0.9081217646598816, loss_yt: 0.17049898207187653\n",
      "epocht 0, batch_num 7400, step 7401, time: 336.0244619846344 s, accu: 0.90815269947052, loss_yt: 0.1619919240474701\n",
      "iter_validnum 1860\n",
      "epochv 0, step 7440, stop_n 1, time: 391.36448192596436 s, accu_va: 0.9082622593769463, loss_yv: 0.21561714184160033\n",
      "iter_trainnum 7440\n",
      "epocht 0, batch_num 0, step 7441, time: 0.7230587005615234 s, accu: 0.9081960916519165, loss_yt: 0.13707087934017181\n",
      "epocht 0, batch_num 200, step 7641, time: 9.440747022628784 s, accu: 0.908295214176178, loss_yt: 0.2571488916873932\n",
      "epocht 0, batch_num 400, step 7841, time: 18.13051152229309 s, accu: 0.90817791223526, loss_yt: 0.32823270559310913\n",
      "epocht 0, batch_num 600, step 8041, time: 26.827255249023438 s, accu: 0.9081354141235352, loss_yt: 0.12211839854717255\n",
      "epocht 0, batch_num 800, step 8241, time: 35.33450698852539 s, accu: 0.9081287384033203, loss_yt: 0.19512060284614563\n",
      "epocht 0, batch_num 1000, step 8441, time: 43.762004375457764 s, accu: 0.9082757830619812, loss_yt: 0.19577039778232574\n",
      "epocht 0, batch_num 1200, step 8641, time: 52.67314291000366 s, accu: 0.908299446105957, loss_yt: 0.1910943239927292\n",
      "epocht 0, batch_num 1400, step 8841, time: 61.2991099357605 s, accu: 0.9083866477012634, loss_yt: 0.1938827931880951\n",
      "epocht 0, batch_num 1600, step 9041, time: 69.75845646858215 s, accu: 0.908494770526886, loss_yt: 0.18477490544319153\n",
      "epocht 0, batch_num 1800, step 9241, time: 78.41929721832275 s, accu: 0.9086472988128662, loss_yt: 0.11517716199159622\n",
      "epocht 0, batch_num 2000, step 9441, time: 86.87870240211487 s, accu: 0.90871661901474, loss_yt: 0.21033526957035065\n",
      "epocht 0, batch_num 2200, step 9641, time: 95.36199378967285 s, accu: 0.9088878631591797, loss_yt: 0.1291765719652176\n",
      "epocht 0, batch_num 2400, step 9841, time: 104.1375584602356 s, accu: 0.9089592099189758, loss_yt: 0.3567366302013397\n",
      "epocht 0, batch_num 2600, step 10041, time: 112.70962929725647 s, accu: 0.9089395999908447, loss_yt: 0.28169217705726624\n",
      "epocht 0, batch_num 2800, step 10241, time: 121.51406025886536 s, accu: 0.9090244770050049, loss_yt: 0.3246525824069977\n",
      "epocht 0, batch_num 3000, step 10441, time: 130.33050107955933 s, accu: 0.9090129137039185, loss_yt: 0.27894049882888794\n",
      "epocht 0, batch_num 3200, step 10641, time: 139.07014870643616 s, accu: 0.9090819358825684, loss_yt: 0.17856083810329437\n",
      "epocht 0, batch_num 3400, step 10841, time: 147.92048406600952 s, accu: 0.9090701341629028, loss_yt: 0.242568701505661\n",
      "epocht 0, batch_num 3600, step 11041, time: 156.74189472198486 s, accu: 0.9090873003005981, loss_yt: 0.19183635711669922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 3800, step 11241, time: 165.48949456214905 s, accu: 0.9090304970741272, loss_yt: 0.14335288107395172\n",
      "epocht 0, batch_num 4000, step 11441, time: 174.05257177352905 s, accu: 0.9089558720588684, loss_yt: 0.24229009449481964\n",
      "epocht 0, batch_num 4200, step 11641, time: 182.59176898002625 s, accu: 0.9089780449867249, loss_yt: 0.27929726243019104\n",
      "epocht 0, batch_num 4400, step 11841, time: 191.20973110198975 s, accu: 0.9089504480361938, loss_yt: 0.1326301097869873\n",
      "epocht 0, batch_num 4600, step 12041, time: 200.0191388130188 s, accu: 0.9088844656944275, loss_yt: 0.24076436460018158\n",
      "epocht 0, batch_num 4800, step 12241, time: 208.50646543502808 s, accu: 0.9089377522468567, loss_yt: 0.1788983941078186\n",
      "epocht 0, batch_num 5000, step 12441, time: 216.99673867225647 s, accu: 0.9089357256889343, loss_yt: 0.5197094678878784\n",
      "epocht 0, batch_num 5200, step 12641, time: 225.6326732635498 s, accu: 0.9089720845222473, loss_yt: 0.07637245953083038\n",
      "epocht 0, batch_num 5400, step 12841, time: 234.21968364715576 s, accu: 0.9089773893356323, loss_yt: 0.44459521770477295\n",
      "epocht 0, batch_num 5600, step 13041, time: 242.80173659324646 s, accu: 0.908958375453949, loss_yt: 0.1598241925239563\n",
      "epocht 0, batch_num 5800, step 13241, time: 251.17238402366638 s, accu: 0.9090471863746643, loss_yt: 0.0825687050819397\n",
      "epocht 0, batch_num 6000, step 13441, time: 259.92597675323486 s, accu: 0.9090083837509155, loss_yt: 0.39724642038345337\n",
      "epocht 0, batch_num 6200, step 13641, time: 269.1542682647705 s, accu: 0.9089372158050537, loss_yt: 0.30423107743263245\n",
      "epocht 0, batch_num 6400, step 13841, time: 277.75028586387634 s, accu: 0.9089576005935669, loss_yt: 0.13138742744922638\n",
      "epocht 0, batch_num 6600, step 14041, time: 286.1827335357666 s, accu: 0.9089153409004211, loss_yt: 0.5317978262901306\n",
      "epocht 0, batch_num 6800, step 14241, time: 295.07096672058105 s, accu: 0.9089406728744507, loss_yt: 0.2885514199733734\n",
      "epocht 0, batch_num 7000, step 14441, time: 303.77668714523315 s, accu: 0.9089141488075256, loss_yt: 0.07954415678977966\n",
      "epocht 0, batch_num 7200, step 14641, time: 312.57220125198364 s, accu: 0.9089767932891846, loss_yt: 0.29779133200645447\n",
      "epocht 0, batch_num 7400, step 14841, time: 321.1881592273712 s, accu: 0.9089886546134949, loss_yt: 0.23511835932731628\n",
      "iter_validnum 1860\n",
      "epochv 0, step 14880, stop_n 0, time: 376.23991775512695 s, accu_va: 0.9089775839159565, loss_yv: 0.2162551606815028\n",
      "iter_trainnum 7440\n",
      "epocht 0, batch_num 0, step 14881, time: 0.655247688293457 s, accu: 0.9089998602867126, loss_yt: 0.24556516110897064\n",
      "epocht 0, batch_num 200, step 15081, time: 9.308153629302979 s, accu: 0.9090229868888855, loss_yt: 0.11059202253818512\n",
      "epocht 0, batch_num 400, step 15281, time: 17.889207124710083 s, accu: 0.9090494513511658, loss_yt: 0.08668571710586548\n",
      "epocht 0, batch_num 600, step 15481, time: 26.672696113586426 s, accu: 0.909004807472229, loss_yt: 0.22928501665592194\n",
      "epocht 0, batch_num 800, step 15681, time: 35.2686882019043 s, accu: 0.9090198874473572, loss_yt: 0.10851560533046722\n",
      "epocht 0, batch_num 1000, step 15881, time: 43.817856311798096 s, accu: 0.9090592861175537, loss_yt: 0.18675310909748077\n",
      "epocht 0, batch_num 1200, step 16081, time: 52.50462746620178 s, accu: 0.9089888334274292, loss_yt: 0.32924479246139526\n",
      "epocht 0, batch_num 1400, step 16281, time: 61.12155818939209 s, accu: 0.9089248180389404, loss_yt: 0.25018957257270813\n",
      "epocht 0, batch_num 1600, step 16481, time: 69.57295966148376 s, accu: 0.9089462757110596, loss_yt: 0.13961252570152283\n",
      "epocht 0, batch_num 1800, step 16681, time: 78.26770782470703 s, accu: 0.9089192152023315, loss_yt: 0.31796613335609436\n",
      "epocht 0, batch_num 2000, step 16881, time: 86.89368081092834 s, accu: 0.9089347124099731, loss_yt: 0.10521092265844345\n",
      "epocht 0, batch_num 2200, step 17081, time: 95.3540210723877 s, accu: 0.9089435338973999, loss_yt: 0.1773241013288498\n",
      "epocht 0, batch_num 2400, step 17281, time: 104.41183114051819 s, accu: 0.9089537262916565, loss_yt: 0.1710686981678009\n",
      "epocht 0, batch_num 2600, step 17481, time: 113.01482629776001 s, accu: 0.908995509147644, loss_yt: 0.18810585141181946\n",
      "epocht 0, batch_num 2800, step 17681, time: 121.76742267608643 s, accu: 0.909024178981781, loss_yt: 0.09763704985380173\n",
      "epocht 0, batch_num 3000, step 17881, time: 130.49305701255798 s, accu: 0.9090384840965271, loss_yt: 0.3621715307235718\n",
      "epocht 0, batch_num 3200, step 18081, time: 139.02028274536133 s, accu: 0.9090343713760376, loss_yt: 0.15752989053726196\n",
      "epocht 0, batch_num 3400, step 18281, time: 147.88255763053894 s, accu: 0.9090307354927063, loss_yt: 0.14917424321174622\n",
      "epocht 0, batch_num 3600, step 18481, time: 156.52843689918518 s, accu: 0.9090570211410522, loss_yt: 0.2037104070186615\n",
      "epocht 0, batch_num 3800, step 18681, time: 165.1633780002594 s, accu: 0.9090695977210999, loss_yt: 0.2221101075410843\n",
      "epocht 0, batch_num 4000, step 18881, time: 173.76736855506897 s, accu: 0.9090304970741272, loss_yt: 0.1420334428548813\n",
      "epocht 0, batch_num 4200, step 19081, time: 182.28356790542603 s, accu: 0.909119188785553, loss_yt: 0.205999955534935\n",
      "epocht 0, batch_num 4400, step 19281, time: 190.74494552612305 s, accu: 0.909164547920227, loss_yt: 0.27163082361221313\n",
      "epocht 0, batch_num 4600, step 19481, time: 199.2900915145874 s, accu: 0.909152626991272, loss_yt: 0.13274317979812622\n",
      "epocht 0, batch_num 4800, step 19681, time: 208.08261346817017 s, accu: 0.9091281890869141, loss_yt: 0.2415275126695633\n",
      "epocht 0, batch_num 5000, step 19881, time: 216.7045578956604 s, accu: 0.9091216325759888, loss_yt: 0.15231236815452576\n",
      "epocht 0, batch_num 5200, step 20081, time: 225.22274732589722 s, accu: 0.9091196656227112, loss_yt: 0.3912426829338074\n",
      "epocht 0, batch_num 5400, step 20281, time: 233.75892210006714 s, accu: 0.909163773059845, loss_yt: 0.26134684681892395\n",
      "epocht 0, batch_num 5600, step 20481, time: 242.31706929206848 s, accu: 0.9091105461120605, loss_yt: 0.28805413842201233\n",
      "epocht 0, batch_num 5800, step 20681, time: 251.0187680721283 s, accu: 0.9091092944145203, loss_yt: 0.18503031134605408\n",
      "epocht 0, batch_num 6000, step 20881, time: 259.7494237422943 s, accu: 0.909131646156311, loss_yt: 0.18910370767116547\n",
      "epocht 0, batch_num 6200, step 21081, time: 268.2915802001953 s, accu: 0.909110426902771, loss_yt: 0.09461995959281921\n",
      "epocht 0, batch_num 6400, step 21281, time: 276.83077931404114 s, accu: 0.9091575145721436, loss_yt: 0.15042494237422943\n",
      "epocht 0, batch_num 6600, step 21481, time: 285.2702124118805 s, accu: 0.9092069268226624, loss_yt: 0.3494373857975006\n",
      "epocht 0, batch_num 6800, step 21681, time: 293.58693957328796 s, accu: 0.9092305898666382, loss_yt: 0.12115450948476791\n",
      "epocht 0, batch_num 7000, step 21881, time: 302.1640226840973 s, accu: 0.9092201590538025, loss_yt: 0.1629972606897354\n",
      "epocht 0, batch_num 7200, step 22081, time: 310.76304388046265 s, accu: 0.9092311263084412, loss_yt: 0.2014487236738205\n",
      "epocht 0, batch_num 7400, step 22281, time: 319.2323639392853 s, accu: 0.9092596769332886, loss_yt: 0.30708909034729004\n",
      "iter_validnum 1860\n",
      "epochv 0, step 22320, stop_n 1, time: 374.46766233444214 s, accu_va: 0.9093219082522136, loss_yv: 0.21107638951350924\n",
      "iter_trainnum 7440\n",
      "epocht 0, batch_num 0, step 22321, time: 0.469745397567749 s, accu: 0.9093395471572876, loss_yt: 0.22232785820960999\n",
      "epocht 0, batch_num 200, step 22521, time: 9.237327814102173 s, accu: 0.9093432426452637, loss_yt: 0.07340173423290253\n",
      "epocht 0, batch_num 400, step 22721, time: 17.671746730804443 s, accu: 0.9093610644340515, loss_yt: 0.09744452685117722\n",
      "epocht 0, batch_num 600, step 22921, time: 25.962581157684326 s, accu: 0.9093621969223022, loss_yt: 0.1983129233121872\n",
      "epocht 0, batch_num 800, step 23121, time: 34.672285318374634 s, accu: 0.909365177154541, loss_yt: 0.19718283414840698\n",
      "epocht 0, batch_num 1000, step 23321, time: 43.534621238708496 s, accu: 0.9093306064605713, loss_yt: 0.3523711860179901\n",
      "epocht 0, batch_num 1200, step 23521, time: 52.44077205657959 s, accu: 0.9093561172485352, loss_yt: 0.18889708817005157\n",
      "epocht 0, batch_num 1400, step 23721, time: 61.13751745223999 s, accu: 0.9093578457832336, loss_yt: 0.37651872634887695\n",
      "epocht 0, batch_num 1600, step 23921, time: 69.98189306259155 s, accu: 0.9093538522720337, loss_yt: 0.24697373807430267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epocht 0, batch_num 1800, step 24121, time: 78.32156801223755 s, accu: 0.9093360900878906, loss_yt: 0.39558130502700806\n",
      "epocht 0, batch_num 2000, step 24321, time: 86.81588530540466 s, accu: 0.9093362092971802, loss_yt: 0.1599280834197998\n",
      "epocht 0, batch_num 2200, step 24521, time: 95.45874238014221 s, accu: 0.9093241691589355, loss_yt: 0.17429979145526886\n",
      "epocht 0, batch_num 2400, step 24721, time: 104.28813171386719 s, accu: 0.9093457460403442, loss_yt: 0.33818110823631287\n",
      "epocht 0, batch_num 2600, step 24921, time: 112.98687291145325 s, accu: 0.9092926979064941, loss_yt: 0.171379953622818\n",
      "epocht 0, batch_num 2800, step 25121, time: 121.86114001274109 s, accu: 0.9092838764190674, loss_yt: 0.24438674747943878\n",
      "epocht 0, batch_num 3000, step 25321, time: 130.68357491493225 s, accu: 0.9092927575111389, loss_yt: 0.13410216569900513\n",
      "epocht 0, batch_num 3200, step 25521, time: 139.18780899047852 s, accu: 0.9093096256256104, loss_yt: 0.35095706582069397\n",
      "epocht 0, batch_num 3400, step 25721, time: 147.5763771533966 s, accu: 0.9093080759048462, loss_yt: 0.13745243847370148\n",
      "epocht 0, batch_num 3600, step 25921, time: 156.0617151260376 s, accu: 0.9093387722969055, loss_yt: 0.2487402707338333\n",
      "epocht 0, batch_num 3800, step 26121, time: 164.98881578445435 s, accu: 0.9093483090400696, loss_yt: 0.24002747237682343\n",
      "epocht 0, batch_num 4000, step 26321, time: 173.69257616996765 s, accu: 0.9093853831291199, loss_yt: 0.2223479002714157\n",
      "epocht 0, batch_num 4200, step 26521, time: 182.4989948272705 s, accu: 0.9094122648239136, loss_yt: 0.11682800203561783\n",
      "epocht 0, batch_num 4400, step 26721, time: 191.30544710159302 s, accu: 0.9094209671020508, loss_yt: 0.19914424419403076\n",
      "epocht 0, batch_num 4600, step 26921, time: 199.85757756233215 s, accu: 0.9094498157501221, loss_yt: 0.18183058500289917\n",
      "epocht 0, batch_num 4800, step 27121, time: 208.45860981941223 s, accu: 0.9094589352607727, loss_yt: 0.18514211475849152\n",
      "epocht 0, batch_num 5000, step 27321, time: 218.27732157707214 s, accu: 0.909460186958313, loss_yt: 0.24609409272670746\n",
      "epocht 0, batch_num 5200, step 27521, time: 227.76794624328613 s, accu: 0.9094035625457764, loss_yt: 0.17884430289268494\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_epochs = 4096, 10000\n",
    "print(trainpd.head())\n",
    "globalstep = modelcrnn.batch_train(trainpd, labels, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "INFO:tensorflow:Restoring parameters from ..\\data\\particles\\model\\modelevery_nomul_model\\v1-1\n",
      "(1, 4086511, 1)\n",
      "(4086511, 19)\n"
     ]
    }
   ],
   "source": [
    "y_pred = modelcrnn.predict(testpd[feature])\n",
    "y_pred = np.squeeze(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#阈值大概在0.2-0.4之间 本题对召回率较敏感，可适当降低一下阈值\n",
    "thre = 0.5 \n",
    "#生成提交文件\n",
    "sub = pd.DataFrame()\n",
    "sub['hit_id']=testpd['hit_id']\n",
    "sub['flag_pred'] = y_pred\n",
    "sub['event_id'] = testpd['event_id']\n",
    "sub['flag_pred'] = sub['flag_pred'].apply(lambda x: 1 if x >= thre else 0)\n",
    "sub.to_csv(os.path.join(pathf, \"subsample.csv\").format(sub['flag_pred'].mean()),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
